{
  "hash": "b107dd6067d9cc2be047eb059520bacf",
  "result": {
    "markdown": "---\ntitle:  \"When one model is not enough: Stacking models with {stacks}.\"\ndescription: |\n  A few notes on stacking models with {stacks}. \ndate: 11/30/2020\ncategories:\n  - R\n  - Machine learning\nimage: giphy.gif  \nbibliography: ref.bib  \n---\n\n\n\nAs part of the modelling process, you might try a range of different techniques or algorithms depending on the problem and data, before ultimately picking one model to use. However, no model is perfect and there will likely be a trade off between picking one model over another. One technique that can be used to combine the strengths of different models is to create a model stack. This approach combines individual models into one \"stack\" that can _hopefully_ outperform any single model. After all, someone of wisdom once said...\n\n> *The whole is greater than the sum of its parts*\n\n...and maybe they were talking about model stacking?\n\nBesides, to get to the point of this post, I wanted to explore the `stacks` package [@Couch2020] for creating (unsurprisingly) model stacks. `stacks` a fairly recent development allowing model stacking to be achieved within the tidymodels ideology. Apart from having a great hex logo, `stacks` provides some powerful tools to create model stacks, and I've included a few notes on my first experiences using the package with _hopefully_ a motivating example..!\n\n::: {.column-margin}\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](logo.png){fig-align='center' width=80%}\n:::\n:::\n\n:::\n\n# The problem\n\nInspired by chapter 10 of Kuhn and Johnson's (2013) Applied Predictive Modelling, the problem we'll be addressing is the compressive strength of different mixture of concrete. Yep that's right, this post is going to be about concrete. But wait! I promise this isn't as dull as it sounds 🤣. \n\nFirst lets have a quick look at the data.\n\n::: {.cell}\n\n```{.r .cell-code}\n# dataset is availble in modeldata package\nlibrary(modeldata)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(stacks)\nlibrary(rules)\nlibrary(see) # for nice theme\n\ndata(\"concrete\")\nskimr::skim(concrete)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |concrete |\n|Number of rows           |1030     |\n|Number of columns        |9        |\n|_______________________  |         |\n|Column type frequency:   |         |\n|numeric                  |9        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: numeric**\n\n|skim_variable        | n_missing| complete_rate|   mean|     sd|     p0|    p25|    p50|     p75|   p100|hist  |\n|:--------------------|---------:|-------------:|------:|------:|------:|------:|------:|-------:|------:|:-----|\n|cement               |         0|             1| 281.17| 104.51| 102.00| 192.38| 272.90|  350.00|  540.0|▆▇▇▃▂ |\n|blast_furnace_slag   |         0|             1|  73.90|  86.28|   0.00|   0.00|  22.00|  142.95|  359.4|▇▂▃▁▁ |\n|fly_ash              |         0|             1|  54.19|  64.00|   0.00|   0.00|   0.00|  118.30|  200.1|▇▁▂▂▁ |\n|water                |         0|             1| 181.57|  21.35| 121.80| 164.90| 185.00|  192.00|  247.0|▁▅▇▂▁ |\n|superplasticizer     |         0|             1|   6.20|   5.97|   0.00|   0.00|   6.40|   10.20|   32.2|▇▆▁▁▁ |\n|coarse_aggregate     |         0|             1| 972.92|  77.75| 801.00| 932.00| 968.00| 1029.40| 1145.0|▃▅▇▅▂ |\n|fine_aggregate       |         0|             1| 773.58|  80.18| 594.00| 730.95| 779.50|  824.00|  992.6|▂▃▇▃▁ |\n|age                  |         0|             1|  45.66|  63.17|   1.00|   7.00|  28.00|   56.00|  365.0|▇▁▁▁▁ |\n|compressive_strength |         0|             1|  35.82|  16.71|   2.33|  23.71|  34.44|   46.14|   82.6|▅▇▇▃▁ |\n:::\n:::\n\nWe can also plot the relationship between compressive strength and each of the predictors in the data. \n\n::: {.cell}\n\n```{.r .cell-code}\nconcrete %>%\n  pivot_longer(-compressive_strength) %>%\n  ggplot(aes(x = value, y = compressive_strength)) +\n  geom_point(alpha = 0.3, size = 1) +\n  geom_smooth() +\n  facet_wrap(~name, scales = \"free_x\") +\n  labs(\n    x = \"Predictor\",\n    y = \"Compressive strength\",\n    title = \"Relationship between predictors and compressive strength\"\n  ) +\n  theme_lucid()\n```\n\n::: {.cell-output-stderr}\n```\n`geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n# Fitting the models\n\nIn chapter 10, @Kuhn2013 evaluate a range of models based on data from @yeh1998 from which I have selected three of the better performing models (which happen to be random forest, neural network and cubist). The goal is to see whether using `stacks` to create an ensemble of these models will outperform each of the individual models. Quick note, I'm going to assume some experience with using the tidymodels workflow for modelling to avoid this post become too lengthy. For an introduction to tidymodels, I have [a post](https://www.hfshr.xyz/posts/2020-05-23-tidymodel-notes/) which covers some of the basics, and you can check out some of the excellent tutorials available on the [tidymodels site](https://www.tidymodels.org/learn/).\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-10_655ad2211595f92f8e817e4de048c6c1'}\n\n```{.r .cell-code}\n# split the data\nset.seed(1)\nconcrete_split <- initial_split(concrete)\nconcrete_train <- training(concrete_split)\nconcrete_test <- testing(concrete_split)\n\n# the folds used in tuning steps\nfolds <- rsample::vfold_cv(concrete_train, v = 5)\n\n# basic recipe used in all models\nconcrete_rec <- recipe(\n  compressive_strength ~ .,\n  data = concrete_train\n)\n\n# metric for evaluation\nmetric <- metric_set(rmse, rsq)\n\n# protect your eyes!\noptions(tidymodels.dark = TRUE)\n\n# convenience function\nctrl_grid <- control_stack_grid()\n\n# Basic workflow\ncement_wf <-\n  workflow() %>%\n  add_recipe(concrete_rec)\n\n# random forest #\nrf_spec <-\n  rand_forest(\n    mtry = tune(),\n    min_n = tune(),\n    trees = 500\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"ranger\")\n\nrf_wflow <-\n  cement_wf %>%\n  add_model(rf_spec)\n\nrf_res <-\n  tune_grid(\n    object = rf_wflow,\n    resamples = folds,\n    grid = 10,\n    control = ctrl_grid\n  )\n```\n\n::: {.cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n\n```{.r .cell-code}\n# neural net #\n\nnnet_spec <-\n  mlp(\n    hidden_units = tune(),\n    penalty = tune(),\n    epochs = tune()\n  ) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"nnet\")\n\nnnet_rec <-\n  concrete_rec %>%\n  step_corr(all_predictors()) %>%\n  step_normalize(all_predictors())\n\nnnet_wflow <-\n  cement_wf %>%\n  add_model(nnet_spec) %>%\n  update_recipe(nnet_rec)\n\nnnet_res <-\n  tune_grid(\n    object = nnet_wflow,\n    resamples = folds,\n    grid = 10,\n    control = ctrl_grid\n  )\n\n# Cubist #\n\ncubist_spec <-\n  cubist_rules(\n    committees = tune(),\n    neighbors = tune(),\n    max_rules = tune()\n  )\n\ncubist_wflow <-\n  cement_wf %>%\n  add_model(cubist_spec)\n\ncubist_res <-\n  tune_grid(\n    object = cubist_wflow,\n    resamples = folds,\n    grid = 10,\n    control = ctrl_grid\n  )\n```\n:::\n\nSo at this point we have fitted 30 models, 10 models for each type (random forest, neural net and cubist). We can do a quick check of how well each of these models performed. For convenience, I've created a simple function called `finaliser()` that selects the best model, updates the workflow, fits the final model with the best parameters and pulls out the metrics.\n\n::: {.cell}\n\n```{.r .cell-code}\nfinaliser <- function(tuned, wkflow, split, model) {\n  best_mod <- tuned %>%\n    select_best(\"rmse\")\n\n  final_wf <- wkflow %>%\n    finalize_workflow(best_mod)\n\n  final_fit <-\n    final_wf %>%\n    last_fit(split)\n\n  final_fit %>%\n    collect_metrics() %>%\n    mutate(model = model)\n}\n\nbind_rows(\n  finaliser(cubist_res, cubist_wflow, concrete_split, \"cubist\"),\n  finaliser(nnet_res, nnet_wflow, concrete_split, \"nnet\"),\n  finaliser(rf_res, rf_wflow, concrete_split, \"rf\")\n) %>%\n  select(model, .metric, .estimate) %>%\n  pivot_wider(names_from = .metric, values_from = .estimate) %>%\n  arrange(rmse)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 3 × 3\n  model   rmse   rsq\n  <chr>  <dbl> <dbl>\n1 cubist  4.54 0.926\n2 rf      5.35 0.899\n3 nnet    5.44 0.894\n```\n:::\n:::\n\nWe can see that the cubist model has the best performance, closely followed by the random forest with the neural net bringing up the rear.\n\n# Time to stack\n\nNow we can start stacking! We start by initialising the stack with `stacks()` then add each candidate model with `add_candidates()`. Next we evaluate the candidate models with `blend_predictions()`, before finally training the non-zero members on the training data with `fit_members()`. \n\n::: {.cell}\n\n```{.r .cell-code}\ncement_st <-\n  # initialize the stack\n  stacks() %>%\n  # add each of the models\n  add_candidates(rf_res) %>%\n  add_candidates(nnet_res) %>%\n  add_candidates(cubist_res) %>%\n  blend_predictions() %>% # evaluate candidate models\n  fit_members() # fit non zero stacking coefficients\n```\n:::\n\nLet's have a look at our model stack\n\n::: {.cell}\n\n```{.r .cell-code}\ncement_st\n```\n\n::: {.cell-output-stderr}\n```\n── A stacked ensemble model ─────────────────────────────────────\n\nOut of 30 possible candidate members, the ensemble retained 10.\nPenalty: 0.01.\nMixture: 1.\n\nThe 10 highest weighted members are:\n```\n:::\n\n::: {.cell-output-stdout}\n```\n# A tibble: 10 × 3\n   member          type          weight\n   <chr>           <chr>          <dbl>\n 1 cubist_res_1_10 cubist_rules 0.375  \n 2 rf_res_1_04     rand_forest  0.296  \n 3 cubist_res_1_07 cubist_rules 0.181  \n 4 cubist_res_1_09 cubist_rules 0.0733 \n 5 nnet_res_1_09   mlp          0.0353 \n 6 nnet_res_1_07   mlp          0.0337 \n 7 nnet_res_1_03   mlp          0.0233 \n 8 cubist_res_1_08 cubist_rules 0.00872\n 9 nnet_res_1_06   mlp          0.00461\n10 nnet_res_1_05   mlp          0.00454\n```\n:::\n:::\n\nOut of the 30 models we initially trained, 7 models had non-zero stacking coefficients and were retained for our model stack. `stacks` provides a nice autoplot feature that allows us to quickly visualise each of the model members with their weights that are used to make predictions.\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(cement_st, type = \"weights\") +\n  theme_lucid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n# So, was it worth it?!\n\nLets first have a look at the predictions made by our stack.\n\n::: {.cell}\n\n```{.r .cell-code}\n# get predictions with stack\ncement_pred <- predict(cement_st, concrete_test) %>%\n  bind_cols(concrete_test)\n\nggplot(cement_pred, aes(x = compressive_strength, y = .pred)) +\n  geom_point(alpha = 0.4) +\n  coord_obs_pred() +\n  labs(x = \"Observed\", y = \"Predicted\") +\n  geom_abline(linetype = \"dashed\") +\n  theme_lucid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\nApart from a handful of points, our stacked model looks like it has done pretty well! We can also see how each of the members in the stack performed by using `members = TRUE` in the prediction call.\n\n::: {.cell}\n\n```{.r .cell-code}\nmember_preds <- predict(cement_st, concrete_test, members = TRUE) %>%\n  bind_cols(\n    .,\n    concrete_test %>%\n      select(compressive_strength)\n  ) %>%\n  select(compressive_strength, .pred, everything())\n```\n:::\n\nTo visualise this, I've selected the first 10 observations, and plotted the residuals. Points on the dashed line are closer to the true value. \n\n::: {.cell}\n\n```{.r .cell-code}\nplot_preds <- member_preds %>%\n  slice(1:10) %>%\n  rowid_to_column(\"obs\") %>%\n  mutate(obs = factor(obs)) %>%\n  pivot_longer(cols = c(-obs,-compressive_strength), names_to = \"model\", values_to = \"value\") %>% \n  mutate(diff = compressive_strength - value,\n         model = ifelse(model == \".pred\", \"model_stack\", model)\n         )\n\n\nplot_preds %>%\n  filter(model != \"model_stack\") %>% \n  ggplot(aes(x = obs, y = diff, colour = model)) +\n  geom_point(alpha = 0.8) +\n  geom_jitter(width = 0.20) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  viridis::scale_colour_viridis(discrete = T, option = \"C\") +\n  labs(y = \"Residual\", subtitle = paste(\"Model stack predictions marked with\", emo::ji(\"x\"))) +\n  geom_point(data = plot_preds %>% \n               filter(model == \"model_stack\"), colour = \"red\", shape = 4, size = 5) +\n  theme_lucid()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\nApart from the first observation where none of the models performed particularly well, you can see how different models resulted in different predictions with some performing better than others. \n\nNow what we're really interested in is if the model stack performed better than any of the individual models. To determine this, lets look at some metrics: \n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_metric <- metric_set(rmse, rsq)\n\nmap_dfr(\n  member_preds,\n  ~ multi_metric(\n    member_preds,\n    truth = compressive_strength,\n    estimate = .x\n  ),\n  .id = \"model\"\n) %>%\n  select(model, .metric, .estimate) %>%\n  pivot_wider(names_from = .metric, values_from = .estimate) %>%\n  filter(model != \"compressive_strength\") %>%\n  mutate(model = if_else(model == \".pred\", \"model_stack\", model)) %>%\n  arrange(rmse) %>% \n  mutate(across(where(is.numeric), round, 2))\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 11 × 3\n   model            rmse   rsq\n   <chr>           <dbl> <dbl>\n 1 model_stack      4.65  0.92\n 2 cubist_res_1_07  4.67  0.92\n 3 cubist_res_1_08  4.75  0.92\n 4 cubist_res_1_10  4.86  0.92\n 5 cubist_res_1_09  5.01  0.91\n 6 rf_res_1_04      5.41  0.9 \n 7 nnet_res_1_03    5.69  0.88\n 8 nnet_res_1_09    5.79  0.88\n 9 nnet_res_1_05    5.91  0.88\n10 nnet_res_1_07    7.52  0.8 \n11 nnet_res_1_06    9.38  0.68\n```\n:::\n:::\n\nWe can see the model stack performs better than any of the individual models for both the rmse and r-squared metrics, which is pretty cool! The cubist models are clearly the strongest but the model stack that includes the inputs from the random forest and neural nets as well as cubist edges slightly ahead in these metrics. \n\n# Summary\n\nSo that was a quick tour of the `stacks` package. I'd highly recommend checking out the [package website](https://stacks.tidymodels.org/) which has lots of good examples on which this post was heavily inspired. \n\nThanks for reading!\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}