---
title: "Experiment data 2"
output: html_notebook
---

Categorical data(=not represented by numbers) VS Numerical data (=represented by numbers)

t-test is for comparing the difference between two means

f-test is comparing the difference between many means

`Let's work with the norm experiment data some more.`

```{r}
library(tidyverse)
library(dplyr)
library(readxl)
library(descr)
library(infer)
```

**QUESTION 1**

`1. Formally test a hypothesis that any normative message is significantly different from any empirical message in its effects on cheating.`

```{r}
library(readxl)
exp21 <- read_excel("experiment_2021C(1)copy.xlsx", col_names = FALSE,
    skip = 1)
View(exp21)
```

`a) state the research hypothesis`

`b) state the null hypothesis`

[**H0**]{.ul}: I **assume** that the mean of the people who were in the normative message is [**no different**]{.ul} from the mean of the people with empirical message.

[**H1**]{.ul}: I **assume** that the outcome mean of the people with normative message is [**different**]{.ul} from the mean of the people with empirical message.

`c) state the test used and describe the procedure (what will be your procedure to decide on the answer to your question)`

We currently only have descriptive statistics. This means that while our statistics tell us about the results we have right now, they don't reveal us the chances of those results re-occurring.

Just looking at the means would not ensure that they are reliable. Therefore we need to conduct a t-test (inferential statistic) for that. In other words, the t-test would allow us to generalize our results to a whole population beyond the sample we are testing. In our case, the t-test will help us identify if the difference between the means is real or if it occurs by chance.

I take the dataset from the previous exercise and I name it "attempt1". This is for purposes of ease. In other words it is to remember the messages used to the experiment with their corresponding numbers, as well as the new columns created in the previous exercise; 'id' and 'rolnum'.

```{r}
attempt1 <- exp21 %>%
             mutate(condition = case_when(Control_Msg != "" ~ 1,
                                   Norm_Pos_Msg != "" ~ 2,
                                   Norm_Neg_Msg != "" ~ 3,
                                   Emp_Neg_Msg != "" ~ 4,
                                   Emp_Pos_Msg != "" ~ 5),
                    id = 1:133,
                    rolnum = 1:133) 
```

I use this to remove the condition that is equal to 1 (=control message) from the table so I can be left only with normative and empirical message.

```{r}
attempt5 <- attempt4 %>% filter(condition!=1) 
```

Here, I use the cleaned dataset of the previous exercise which contains all the final information and I name it "attempt2". This helps for the next step.

```{r}
attempt2 <- fin[order(fin$Age, decreasing = TRUE), ]
```

Now, I create a new dataset named "attempt3". Here, I create a column named 'cheating' where I **assume** that each person that answered correctly more than five times was cheating. The average probability of guessing correctly (20\*1/6= 3,3%) in conjunction with the fact that 95% of our data fall into a sd=2 frame, made me **assume** that the cheating threshold is everything above 5 times. Hence, we create the column cheating to indicate which people did actually cheat.

ifelse = if there are only two outcomes.

```{r}
attempt3 <- attempt2 %>% mutate(cheating = ifelse(summm > 5, "Yes", "No"))
 #if there are more statements we care about we use "case_when"
```

To have a better illustration of our data, we create a new dataset this time withouth the Outcomes each time from 1 to 20. We name the new dataset "attempt4".

```{r}
attempt4 <- attempt3 %>% select(id, condition, rolnum, cheating, summm, Gender, Risk, Age, SC0)
```

**Now, to I want to categorize my data to only normative and only empirical messages.** To do that I start by using the command 'filter' in order to remove the rows that have condition 4 and 5 which are corresponding to empirical messages.

```{r}
#only normative
attempt6 <- attempt5 %>% filter(condition!=4)
onlynorm<- attempt6 %>% filter(condition!=5)
```

I do the same to create a dataset that only has empirical messages.

```{r}
#only empirical
attempt7 <- attempt5 %>% filter(condition!=2)
onlyempi<- attempt7 %>% filter(condition!=3)
```

For later purposes I will do the same to create a dataset with only positive messages, and a dataset with only negative ones.

```{r}
#only positive
attempt8 <- attempt5 %>% filter(condition!=3)
onlyposi<- attempt8 %>% filter(condition!=4)

#only negative
attempt8 <- attempt5 %>% filter(condition!=2)
onlynega<- attempt8 %>% filter(condition!=5)
```

```{r}
crosstab(attempt5$cheating, attempt5$condition, expected = TRUE)
crosstab(onlynorm$cheating, onlynorm$condition, expected = TRUE)
crosstab(onlyempi$cheating, onlyempi$condition, expected = TRUE)
```

Now I will **assume** that the normative and empirical message did not have an effect on outcome. Thus I multiply the total number of normative condition with the total number of people that repo

```{r}
attempt9 <- attempt5 %>% mutate(cond = case_when(condition == 2 ~ "NormPos",                              condition == 3 ~ "NormNeg",
 condition == 4 ~ "EmpPos",
 condition == 5 ~ "EmpNeg"))
```

I hereby use the two new datasets I created ('onlynorm' and 'onlyempi') to identify their mean. This would help me after where I would be doing the chi square.

```{r}
onlynorm %>%  select(summm) %>% summary(answer)
onlyempi %>% select(summm) %>% summary(answer)
```

Chi square follows using the code we learned in class. **The chi-square tells us how much difference exists between our observed counts and the counts we would expect if there were no relationship at all in the population.** In our case, while the difference seems away from the two means, we need to conduct a p-value to determine whether difference exists or not.

```{r}
attempt9 %>% 
    specify(cheating ~ cond) %>% 
    hypothesize(null = "independence") %>% 
    calculate(stat = "chisq")
```

Permutation allows us to randomly redistribute people where I randomly assigned them in different boxes. We are doing that 1000 times.

Red: Is the actual difference

Black: If we randomize the sample to 1000, this is what we get.

Because the red is so far from the black ones we infer that the message actually influence weather someone will cheat or not. If the black one and the red one was interfering with each other it would mean that there is no influence of the message on weather someone will cheat or not.

```{r}
attempt9 %>% 
    specify(cheating ~ cond) %>% 
    hypothesize(null = "independence") %>% 
    generate(reps = 1000, type = "permute") %>%
    calculate(stat = "chisq") %>% 
    visualize() +
    shade_p_value(attempt9, direction = "greater")
```

`d) state the conclusion you found from your test.`

    #t.test(cheating ~ condition, data = attempt9)

We can find the p-value between the 'condition' and the 'cheating' status of people by using the command 'chisq.test'. As we can see, the t test reveals a p-value of 20.7% --- which corresponds to 79.3% confidence level. This means that we fail to reject the null hypothesis, given that we go with a 95% confidence level.

**OUTCOME = Fail to reject the null hypothesis**

```{r}
t.test((onlyempi$cheating == "Yes"), (onlynorm$cheating == "Yes"))
```

`2. Repeat Q1 to see if any positive message was significantly different from any negative message.`

To do that I go on and conduct a t-test (in the same way as before) for only positive and only negative messages. As we can see, the t test reveals a p-value of 10.45% --- which corresponds to 89.55% confidence level. This means that we fail to reject the null hypothesis, given that we go with a 95% confidence level.

**OUTCOME = Fail to reject the null hypothesis**

```{r}
t.test((onlyposi$cheating == "Yes"), (onlynega$cheating == "Yes"))
```

```{r}
#small variability and different means. In that case I clearly reject the hypothesis. f satistic is the variability accross groups devided by the variability within groups; 
```

`3. Repear Q1 to see if messages differed (across all 5 times of messages)`

Like we did in the exercise 1, when we grouped the data to create an 'onlyempi' and an 'onlynorm' datasets, we do here to create an 'onlycont' dataset. We use the attempt4 dataset where it includes all 5 times of messages and we go on to create our 'onlycont' dataset.

**OUTCOME = Fail to reject ANY of the null hypothesis**

```{r}
  
#only control message
attempt10 <- attempt4 %>% filter(condition!=2)
attempt11 <- attempt10 %>% filter(condition!=3)
attempt12 <- attempt11 %>% filter(condition!=4)
onlycont<- attempt12 %>% filter(condition!=5)

```

Then I filter to find the following \#commented conditions:

```{r}
#only positive and empirical
onlyposiempi <- attempt4 %>% filter(condition=="4")

#only negative and empirical
onlynegaempi <- attempt4 %>% filter(condition=="5")

#only positive and normative
onlyposinorm <- attempt4 %>% filter(condition=="2")

#only negative and normative
onlyneganorm <- attempt4 %>% filter(condition=="3")
```

Actually, I infer that the previous columns were not useful to conduct the pairwise test. This is because in the attempt4 dataset all 5 messages are included. In fact, I use the command 'pairwise.t.test' to do t-tests among all five messages and see where their p-value falls.

```{r}
pairwise.t.test(attempt4$cheating=="Yes", attempt4$condition)
```

`4. Compare Q3 with Q1 and Q2. Are the findings consistent? Provide an explanation for why the results are consistent or inconsistent.`

The results seems to be consistent.

In question 1 we tested weather normative and empirical messages have the same impact on the effect of cheating as the null hypothesis

In question 2 we tested whether positive and negative messages have the same impact on the effect of cheating as the null hypothesis.

In question 3 we did a pairwise testing to test/compare the mean difference among all messages and we also fail to reject any of the null hypothesis

Taking into account all three outcomes we infer that the results are consistent.

`5. List the assumptions that you relied on while answering the questions. Are the assumptions violated or not? What effect might it have on your findings?`

I assume that my results are independent. Meaning that the 133 results of the survey were not distorted. In other words, when all of the class took the test, the assumption was that we haven't talked to each other when we were completing the survey. This assumption help us generalize the sample inferences to the whole population.

However, there could be some reasons that, if true, they could have distorted our assumptions. An example could be that there is a very high probability that the person who responded in all of the answers that he/she found a correct guess done it on purpose. Another example could be that some people might actually have talked to each other and agreed to complete the test in a specific way which in the end influenced the results.

Thus, if this was the case (people actually influenced the outcome of others), the assumption is violated as our results do not represent the actual people's perceptions.

Another assumption that takes place is that the results are random. This implies that the test results occurred from a random population sample. However, this assumption is violated as the test was given to a specific group of people that study the same program.

This could have affected the results as the specific group of people that were targeted (masters students from BDS program at UPenn) probably give different results from a randomized group in society.
