{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2312c38",
   "metadata": {},
   "source": [
    "# Lecture 11. Text analysis\n",
    "\n",
    "The code below has been adapted from the following:\n",
    "    https://www.kaggle.com/code/adepvenugopal/sentiment-analysis-of-glassdoor-review/notebook\n",
    "    \n",
    "The data file can be downloaded here:\n",
    "    https://www.kaggle.com/datasets/dhirajnimbalkar/topicmodellinghoneywellglassdoorreviews\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "991b9ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np #calculations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from textblob import TextBlob #package that let us create work and create text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "780d70c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('glassdoortest1.csv',encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff4cb8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13-Apr-18</td>\n",
       "      <td>Good Company to Work For</td>\n",
       "      <td>Great Pay, Flexible Hours, Unlimited Vacation.</td>\n",
       "      <td>Health Care, 401K, nothing else really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>16-Apr-18</td>\n",
       "      <td>First Impressions</td>\n",
       "      <td>Great staff and very helpful. Fair compensatio...</td>\n",
       "      <td>As a relatively new employee I have not experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>12-Apr-18</td>\n",
       "      <td>Sr. Engineering Technologist</td>\n",
       "      <td>Great benefits, working condition and people!</td>\n",
       "      <td>Must make a 30 mile commute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11-Apr-18</td>\n",
       "      <td>Environmental Specialist</td>\n",
       "      <td>Very supportive environment for learning new t...</td>\n",
       "      <td>Corporate is slow in taking decisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12-Apr-18</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Work Life Balance, Slightly above average pay,...</td>\n",
       "      <td>High health care cost, high employee cost of c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       date                         title  \\\n",
       "0           1  13-Apr-18      Good Company to Work For   \n",
       "1           2  16-Apr-18             First Impressions   \n",
       "2           3  12-Apr-18  Sr. Engineering Technologist   \n",
       "3           4  11-Apr-18      Environmental Specialist   \n",
       "4           5  12-Apr-18                         Sales   \n",
       "\n",
       "                                                pros  \\\n",
       "0     Great Pay, Flexible Hours, Unlimited Vacation.   \n",
       "1  Great staff and very helpful. Fair compensatio...   \n",
       "2      Great benefits, working condition and people!   \n",
       "3  Very supportive environment for learning new t...   \n",
       "4  Work Life Balance, Slightly above average pay,...   \n",
       "\n",
       "                                                cons  \n",
       "0             Health Care, 401K, nothing else really  \n",
       "1  As a relatively new employee I have not experi...  \n",
       "2                       Must make a 30 mile commute.  \n",
       "3              Corporate is slow in taking decisions  \n",
       "4  High health care cost, high employee cost of c...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5)\n"
     ]
    }
   ],
   "source": [
    "display(data.head())\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b722253",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_28682/2814224401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbgcolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbgcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def wc(data,bgcolor,title):\n",
    "    plt.figure(figsize = (50,50))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 2000, random_state=42, max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168eb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = data\n",
    "\n",
    "polarity=[] #\n",
    "subjectivity=[] \n",
    "\n",
    "for i in comm['pros'].values:\n",
    "    try:\n",
    "        analysis =TextBlob(i)\n",
    "        polarity.append(analysis.sentiment.polarity)\n",
    "        subjectivity.append(analysis.sentiment.subjectivity)\n",
    "        \n",
    "    except:\n",
    "        polarity.append(0)\n",
    "        subjectivity.append(0)\n",
    "        \n",
    "        \n",
    "comm['polarity']=polarity\n",
    "comm['subjectivity']=subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm[['title','pros','polarity','subjectivity']][comm.polarity<-0.25].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555cbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm[['title','pros','polarity','subjectivity']][comm.polarity>0.8].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353dd0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc(comm['pros'][comm.polarity>0.8],'black','Common Words' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc(comm['pros'][comm.polarity<-0.4],'black','Common Words' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa028889",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm.polarity.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d140f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm.subjectivity.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm['polarity'][comm.polarity==0]= 0\n",
    "comm['polarity'][comm.polarity > 0]= 1\n",
    "comm['polarity'][comm.polarity < 0]= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8381cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm.polarity.value_counts().plot.bar()\n",
    "comm.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd59cff",
   "metadata": {},
   "source": [
    "Let's now look at more advanced ways to analyze text data\n",
    "\n",
    "The code was adapted from here: \n",
    "    https://www.kaggle.com/code/rohan74/reviews-glassdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab87e3c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autocorrect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_28682/1374715423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Spell Correction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautocorrect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpeller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autocorrect'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#Scikit-Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "#Spell Correction\n",
    "from autocorrect import Speller\n",
    "\n",
    "#Tokenization\n",
    "import wordninja\n",
    "\n",
    "\n",
    "#plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "# contractions is a library for converting words like \"I'm\" to \"I am\"\n",
    "import contractions\n",
    "\n",
    "# Necessary Libraries to find similarity\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# initializing spell checker \n",
    "spell = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bab8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordslist= ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " \"you'll\",\n",
    " \"you'd\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " \"she's\",\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " \"that'll\",\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'to',\n",
    " 'from',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "# function to extract extract noun, adjective, verb and adverbs from the text\n",
    "# and tag pos for better lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# preprocessing of text\n",
    "def text_preprocess(text):\n",
    "    if (len(text)>0):\n",
    "        # contractions is a library for converting words like \"I'm\" to \"I am\"\n",
    "        text=contractions.fix(text)\n",
    "        \n",
    "        #Removing all the special characters from the review text\n",
    "        for char in '!#$%&@?,.:;+-*/=<>\"\\'()[\\\\]X{|}~\\n\\t1234567890':\n",
    "            text = str(text).replace(char, ' ')\n",
    "            \n",
    "        #Converting all the words in review into lower case\n",
    "        text=text.lower()\n",
    "        \n",
    "        #splitting the words in a sentence.\n",
    "        word_list = wordninja.split(text)\n",
    "        \n",
    "        #removing stopwords from customzied stopwordlists \n",
    "        #and considering only word of length greater than 2\n",
    "        word_list=[spell(w) for w in word_list if w not in stopwordslist and len(w) > 2]\n",
    "        \n",
    "#       extract noun, adjective, verb and adverbs from the text and perform lemmatizton   \n",
    "        lemmatized_text=' '.join(([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_list]))\n",
    "        \n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# cleaning the pros, cons and reviewTitle\n",
    "df['cleanpros'] = df['pros'].apply(lambda text: text_preprocess(text))\n",
    "df['cleancons'] = df['cons'].apply(lambda text: text_preprocess(text))\n",
    "\n",
    "\n",
    "#Combing all the clean text information given by the user\n",
    "df['cleantext'] = df[['cleanpros', 'cleancons']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870772fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotwordcountdistribution(df,reviewtype):\n",
    "    #reviewtyepe is 'Pros_Modified_Text','Cons_Modified_Text' in our case\n",
    "    def comment_len(x):\n",
    "        if type(x) is str:\n",
    "            return len(x.split())\n",
    "        else:\n",
    "            return 0\n",
    "    df['review_len'] = df[reviewtype].apply(comment_len)\n",
    "    length_scale=[0,5,15,25,50,75,100,200,10000]#Change length scale according to your requirement\n",
    "    out = pd.cut(df['review_len'],length_scale)\n",
    "    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n",
    "    plt.title('Word Count distribution of {}'.format(reviewtype))\n",
    "    plt.show()\n",
    "    df.drop(columns=['review_len'])\n",
    "    \n",
    "plotwordcountdistribution(df,'cleanpros')\n",
    "plotwordcountdistribution(df,'cleancons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bebdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on cleantext\n",
    "\n",
    "analyzer=SentimentIntensityAnalyzer()\n",
    "def polarity_score(text):\n",
    "    if len(text)>0:\n",
    "        score=analyzer.polarity_scores(text)['compound']\n",
    "        return score\n",
    "    else:\n",
    "        return 0\n",
    "df['polarityscore'] =df['cleantext'].apply(lambda text : polarity_score(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f495fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentianamolybarplot(df):\n",
    "    polarity_scale=[0.0,0.2,0.4,0.6,0.8,1]\n",
    "    #'Review_polarity' is column name of sentiment score calculated for whole review.\n",
    "    df2=df[(df['polarityscore']>0)]\n",
    "    out = pd.cut(df2['polarityscore'],polarity_scale)\n",
    "    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n",
    "    plt.show()\n",
    "sentianamolybarplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680855e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_trigram(corpus):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68293a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_bigram(corpus):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e008f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_trigrams_pros= {}\n",
    "dict_trigrams_cons= {}\n",
    "\n",
    "dict_trigrams_pros = top_n_trigram(df['cleanpros'])\n",
    "\n",
    "dict_trigrams_cons = top_n_trigram(df['cleancons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1309828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_toptrigrams(trigrams, reviewtype = 'Pros'):\n",
    "    common_words = trigrams[:20]\n",
    "    df1 = pd.DataFrame(common_words, columns = ['word' , 'count'])\n",
    "    fig = px.bar(df1, x='word', y='count')\n",
    "    fig.update_layout(title_text= '{0} Review Tri-gram count top 20'.format(reviewtype), template=\"plotly_white\")\n",
    "    fig.show()\n",
    "\n",
    "bar_plot_toptrigrams(dict_trigrams_pros)\n",
    "bar_plot_toptrigrams(dict_trigrams_cons,'Cons') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67204a55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4213d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
