{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee52543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46f9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_tables = {}\n",
    "\n",
    "# For every table in the datasets folder...\n",
    "for table_name in os.listdir('stock_news'):\n",
    "    #this is the path to the file. Don't touch!\n",
    "    table_path = f'stock_news/{table_name}'\n",
    "    # Open as a python file in read-only mode\n",
    "    table_file = open(table_path, 'r')\n",
    "    # Read the contents of the file into 'html'\n",
    "    html = BeautifulSoup(table_file)\n",
    "    # Find 'news-table' in the Soup and load it into 'html_table'\n",
    "    html_table = html.find(id='news-table')\n",
    "    # Add the table to our dictionary\n",
    "    html_tables[table_name] = html_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a78595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File number 1:\n",
      "Billionaire investor questions Elon Musk getting 'a pass' after bombshell tweets\n",
      "Sep-21-18 09:56PM  \n",
      "File number 2:\n",
      "Broadcoms Stock Looks Like a Winner\n",
      "09:30PM  \n",
      "File number 3:\n",
      "SHAREHOLDER ALERT:  Pomerantz Law Firm Reminds Shareholders with Losses on their Investment in Tesla, Inc. of Class Action Lawsuit and Upcoming Deadline  TSLA\n",
      "05:30PM  \n",
      "File number 4:\n",
      "Tesla's People Problem and the Inscrutable Musk: 2 Things That Make You Go Hmmm\n",
      "05:30PM  \n"
     ]
    }
   ],
   "source": [
    "# Read one single day of headlines \n",
    "tsla = html_tables['tsla_22sep.html']\n",
    "# Get all the table rows tagged in HTML with <tr> into 'tesla_tr'\n",
    "tsla_tr = tsla.findAll('tr')\n",
    "\n",
    "# For each row...\n",
    "for i, table_row in enumerate(tsla_tr):\n",
    "    # Read the text of the element 'a' into 'link_text'\n",
    "    link_text = table_row.a.get_text() \n",
    "    # Read the text of the element 'td' into 'data_text'\n",
    "    data_text = table_row.td.get_text()\n",
    "    # Print the count\n",
    "    print(f'File number {i+1}:')\n",
    "    # Print the contents of 'link_text' and 'data_text' \n",
    "    print(link_text)\n",
    "    print(data_text)\n",
    "    # The following exits the loop after four rows to prevent spamming the notebook, do not touch\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf7a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold the parsed news into a list\n",
    "parsed_news = []\n",
    "# Iterate through the news\n",
    "for file_name, news_table in html_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # Read the text from the tr tag into text\n",
    "        text = x.get_text() \n",
    "        # Split the text in the td tag into a list \n",
    "        date_scrape = x.td.text.split()\n",
    "        # If the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        # If not, load 'date' as the 1st element and 'time' as the second\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "        ticker = file_name.split(\"_\")[0]\n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, x.a.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8be7e61",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/loizoskon/nltk_data'\n    - '/Users/loizoskon/opt/anaconda3/nltk_data'\n    - '/Users/loizoskon/opt/anaconda3/share/nltk_data'\n    - '/Users/loizoskon/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_55821/2936725782.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Instantiate the sentiment intensity analyzer with the existing lexicon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# Update the lexicon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mvader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/sentiment/vader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mlexicon_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     ):\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVaderConstants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/loizoskon/nltk_data'\n    - '/Users/loizoskon/opt/anaconda3/nltk_data'\n    - '/Users/loizoskon/opt/anaconda3/share/nltk_data'\n    - '/Users/loizoskon/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# NLTK VADER for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# New words and values\n",
    "new_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# Update the lexicon\n",
    "vader.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b0f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK VADER for sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# New words and values\n",
    "new_words = {\n",
    "    'crushes': 10,\n",
    "    'beats': 5,\n",
    "    'misses': -5,\n",
    "    'trouble': -10,\n",
    "    'falls': -100,\n",
    "}\n",
    "# Instantiate the sentiment intensity analyzer with the existing lexicon\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "# Update the lexicon\n",
    "vader.lexicon.update(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Use these column names\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "# Convert the list of lists into a DataFrame\n",
    "scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "# Iterate through the headlines and get the polarity scores\n",
    "scores = [vader.polarity_scores(headline) for headline in scored_news.headline]\n",
    "# Convert the list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "scored_news.columns = columns\n",
    "# Join the DataFrames\n",
    "scored_news = scored_news.join(scores_df)\n",
    "# Convert the date column from string to datetime\n",
    "scored_news['date'] = pd.to_datetime(scored_news.date).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be457071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_c = scored_news.groupby(['date', 'ticker']).mean()\n",
    "# Unstack the column ticker\n",
    "mean_c = mean_c.unstack('ticker')\n",
    "# Get the cross-section of compound in the 'columns' axis\n",
    "mean_c = mean_c.xs(\"compound\", axis=\"columns\")\n",
    "# Plot a bar chart with pandas\n",
    "mean_c.plot.bar(figsize = (10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2134cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
