{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba05d22",
   "metadata": {},
   "source": [
    "# Scraping Reddit using Python: instructions:\n",
    "\n",
    "1. Creating a Reddit App.\n",
    "\n",
    "The first thing you need to do to get access to the reddit API data is to create a reddit app. You can do that by going to this link: https://www.reddit.com/prefs/apps\n",
    "\n",
    "After you log in with your reddit account you will be able to see a button that says \"Are you a developer? create an app..\".\n",
    "\n",
    "When you press the button you should see something like this:\n",
    "\n",
    "![image](code/reddit01.png)\n",
    "\n",
    "\n",
    "You need to do the following:\n",
    "\n",
    "* Choose the name for the app (it can be anything, but make sure it's one word)\n",
    "* Make sure you choose \"script\" for the type of the app.\n",
    "* You can add a description\n",
    "* The redirect uri should be: http://localhost:8080\n",
    "\n",
    "Once the app is created, you will be able to seee three things:\n",
    "\n",
    "* client_id (right under the words personal use script)\n",
    "* secret (right night to the words secret\n",
    "* user_agent (that's the name you gave to your app)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6059bde5",
   "metadata": {},
   "source": [
    "2. Set up your python script.\n",
    "\n",
    "Install the package \"praw\" (it can be done once per lifetime of a computer)\n",
    "\n",
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6fabf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: websocket-client>=0.54.0 in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from praw) (0.58.0)\n",
      "Collecting prawcore<3,>=2.1\n",
      "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting update-checker>=0.18\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.8)\n",
      "Requirement already satisfied: six in /Users/shpenev/opt/anaconda3/lib/python3.8/site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Installing collected packages: update-checker, prawcore, praw\n",
      "Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install praw # comment this line out if you installed it before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cebf8c",
   "metadata": {},
   "source": [
    "Now load the package and set up the reddit instance. Replace the info with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='TYPE YOUR CLIENT ID HERE',\n",
    "                     client_secret='TYPE YOUR CLIENT SECRET HERE',\n",
    "                     user_agent='TYPE YOUR USER AGENT HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2e19a",
   "metadata": {},
   "source": [
    "Now you can use the API to scrape the data from Reddit.\n",
    "\n",
    "Suppose you want to see the 10 most popular posts in the subreddit \"Cats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11992ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id subreddit  \\\n",
      "0                             Chilling in the shower  10341  u1tsh7      cats   \n",
      "1  Video games are great, but sometimes it's nice...   2007  u1ypu9      cats   \n",
      "2  He‚Äôs trying to earn his keep. He‚Äôs gone to ten...   1521  u1z8nd      cats   \n",
      "3                        What kind of cat do I have?   1077  u21p6l      cats   \n",
      "4  My boyfriend died two weeks ago and my cat has...   1094  u1ziv2      cats   \n",
      "5  Spidercat, spidercat, does whatever a spiderca...    759  u20x8j      cats   \n",
      "6                                       just chillin    551  u22jf8      cats   \n",
      "7                                cutie beau and cool   8107  u1o69k      cats   \n",
      "8                My cat Bucket on his homemade couch    563  u2012d      cats   \n",
      "9                                   Catto Wanna Hide   7158  u1nrvf      cats   \n",
      "\n",
      "                                     url  num_comments body       created  \n",
      "0    https://i.redd.it/yoleio2u12t81.jpg           120       1.649751e+09  \n",
      "1        https://v.redd.it/qbrlgstek3t81            22       1.649769e+09  \n",
      "2    https://i.redd.it/greqxhpuo3t81.jpg           167       1.649770e+09  \n",
      "3    https://i.redd.it/zyft6rnz84t81.jpg           257       1.649777e+09  \n",
      "4  https://www.reddit.com/gallery/u1ziv2            50       1.649771e+09  \n",
      "5    https://i.redd.it/vel5za6y24t81.jpg             7       1.649775e+09  \n",
      "6    https://i.redd.it/fho4zxumf4t81.jpg            43       1.649779e+09  \n",
      "7  https://www.reddit.com/gallery/u1o69k           121       1.649730e+09  \n",
      "8  https://www.reddit.com/gallery/u2012d            22       1.649773e+09  \n",
      "9        https://v.redd.it/o1vwdqld80t81            42       1.649729e+09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "subreddit = reddit.subreddit('Cats')\n",
    "for post in subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85a70f",
   "metadata": {},
   "source": [
    "You can also get comments to posts using the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0122853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I know some friends and family members who live in India and  can possibly help, please can you message me privately more info\n",
      "I wish I could adopt him ...poor baby\n",
      "I wish I could adopt him ‚ù§Ô∏è.But, I can‚Äôt, I already have a few fur babies.\n",
      "Wishing you and him the best of luck. Please find him a good him!\n",
      "Thanks you üôè\n",
      "Done. I've DMed ya.\n",
      "Thanks a lot. I wish I could too. My allergies unfortunately don't afford me the luxury. It was manageable before Covid, but it's been a mess ever since.\n"
     ]
    }
   ],
   "source": [
    "submission = reddit.submission(url=\"https://www.reddit.com/r/cats/comments/u21dcf/elder_stray_needs_to_be_adopted_he_roams_around/\")\n",
    "\n",
    "submission.comments.replace_more(limit=0)\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d832ba",
   "metadata": {},
   "source": [
    "You can get inspiration for some additional commands here:\n",
    "\n",
    "https://medium.com/analytics-vidhya/scraping-reddit-using-python-reddit-api-wrapper-praw-5c275e34a8f4\n",
    "https://www.geeksforgeeks.org/scraping-reddit-using-python/\n",
    "https://gilberttanner.com/blog/scraping-redditdata\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
