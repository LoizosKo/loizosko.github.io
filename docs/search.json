[
  {
    "objectID": "posts/2022-06-09-denmark/index.html#currency",
    "href": "posts/2022-06-09-denmark/index.html#currency",
    "title": "Things to do in Copenhagen",
    "section": "CURRENCY",
    "text": "CURRENCY\nThe main currency is Danish Krone. Live currency converter. However, they accept card payments everywhere. With Revolut or Venmo you should be fine."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#transportation",
    "href": "posts/2022-06-09-denmark/index.html#transportation",
    "title": "Things to do in Copenhagen",
    "section": "TRANSPORTATION",
    "text": "TRANSPORTATION\n\nThe Metro is convenient and it takes you everywhere you want to go within the city:\n\nRejseplanen app - This is the app for the metro/train. It is designed specifically for them and it works like Google maps. Great navigation tool while you are in Denmark. There will be machines (also a person for support) at the airport where you can buy tickets. Also, Denmark is infamous for cycling with great infrastructure. You can rent bikes as well and ride across the city."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-visits",
    "href": "posts/2022-06-09-denmark/index.html#cool-visits",
    "title": "Things to do in Copenhagen",
    "section": "COOL VISITS",
    "text": "COOL VISITS\nNyhavn (The most iconic part of Copenhagen. Highly recommended. the Closest metro station: Kongens Nytorv station.)\n\n\n\nNyhavn\n\n\n\nSomething you can do is to walk through the shopping center of Copenhagen. You can go to Norreport station and walk towards Nyhavn. It is like a 1 to 1.5 mile (1.6‚Äì2.4km) walk. There are many stores and restaurants in between (along the yellow line on the map). While it is not a straightforward walk it is a pleasant one as you are passing through the shopping area of the city.\n\n\nRosenborg Castle & Botanical Garden (Nice places to walk around. Also, Rosenborg castle is a historic place that now operates as a museum ticket info. Have no opinion for the museum though as I haven‚Äôt been. Closest metro station: Norreport station)\nLittle Mermaid (You will often see it in souvenir shops as it is considered as one of the main attraction points in Copenhagen along Nyhavn. While you might not be impressed by it due to its tiny size, you may want to see it simply for its touristic outreach.) Osterport Station.\nAmalienborg (The equivalent ‚ÄòBuckingham palace‚Äô of Denmark. Cool for walking around and taking pictures with the royal guards. Have no opinion for the museum there though as I haven‚Äôt been. tick info)\nCarlsberg Factory (if you like beer this is a nice place to visit. There is a tour in the factory and some beer tasting in the end. Carlsberg Station.)\nTivoli Gardens/Park (Located in front of the Central Station. The most famous park in Copenhagen. Kobenhavn H station)\nFisketorvet (If you like Malls, this can be an option. Most famous mall in the city.)\nBlack Diamond (The largest library in Denmark. It is a modern building. A Nice place to visit. Christianshavn station.)"
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "href": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "title": "Things to do in Copenhagen",
    "section": "COOL NEIGHBOURHOODS",
    "text": "COOL NEIGHBOURHOODS\n\nFreetown Christiania (It is considered an independent town within Copenhagen where people go there to chillax. It is a renowned marijuana place since Christiania is the only part of Copenhagen where Danish law is not enforced. However, it is a safe area that is worth a visit mainly due to its particularity. Highly recommended.) Metro station: Christianshavn.\nBeautiful Parks (especially for summer walks):\n\n\nFrederiksberg Have (One of the most beautiful parks in Copenhagen) Frederiksberg station.\nSuperkilen Park (Norrebro station). Fun fact, Norrebro is an area where many immigrants stay. Diverse area.\nVestre Kirkegard (near Carlsberg factory) Metro: Carlsberg Station\n\nFOOD & COOL PLACES (min $ /max $$$$)\n\nUnion Kitchen $$ (Cool place for brunch. Heard great things from friends that have been there.)\nSticks‚Äôn‚ÄôSushi $$$ (Great sushi place. Known rooftop restaurant in the city.)\nDalle Valle $ (variety food buffet - alue for money place with plenty of food) Not fast food, but not a high-class restaurant. Almost always lots of people, decent food, a variety of options. Recommended for lunch.\nConditori La Glace $$$ (Traditional Danish pastries, it is the oldest patisserie in Denmark - founded in 1870, good for dessert)\nEspresso House $$ (coffee place, despite its Swedish origin it is considered as the ‚ÄòStarbucks‚Äô of Denmark)\nBastard cafe $$ (a place full of board games. A nice place to go with friends for coffee/snacks/beer and board games.\nTaphouse $$ (Famous bar in the city. If you like beer then this is the place for you. It has one of the largest beer selections in Europe.)\n\nYou can always map your choices and combine visits that are close to each other. Also, since it is Scandinavia, remember to check the weather while preparing your bags! You are visiting arguably the most beautiful city in Scandinavia so try to enjoy every minute of it!\nHave fun and safe travels!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Hi üëã I am Loizos! This is my personal website.\nI have a blog that is infrequently updated üìù\nSee my projects for some things I‚Äôve been working üõ†\nFeel free to get in contact below"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some things I made\n\nRStudio projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      Google Trends\n      Top 3 American Sitcoms\n   \n  \n\nPython projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      State of The Union Addresses\n      US presidents speech analysis\n   \n  \n\n\nNo matching items"
  },
  {
    "objectID": "software/sitcoms.html",
    "href": "software/sitcoms.html",
    "title": "Sitcoms",
    "section": "",
    "text": "Let's continue working with the Google Trends data you obtained for Homework 02. Homework 3 starts from the line 180 and onward.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.7\n‚úì tidyr   1.1.4     ‚úì stringr 1.4.0\n‚úì readr   2.0.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(broom)\nlibrary(ggthemes)\nlibrary(ggsci)\n\n\nCode to load the data into R and prepare it for the analysis. You need to correctly specify data types and choose concise variable names.\n\n#Import data for the US\nuscoms <- read_csv(\"uscoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (United States), South Park: (United States), The Simps...\n\n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for the world\nworldcoms <- read_csv(\"worldcoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Worldwide), South Park: (Worldwide), The Simpsons: (Wo...\n\n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for Cyprus\ncycoms <- read_csv(\"cycoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Cyprus), South Park: (Cyprus), The Simpsons: (Cyprus)\n\n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nprepare the data for analysis\n\n#NOW WE CLEAN AND PREPARE THE DATA FOR ANALYSIS\n#Clean for the US.\nuscoms_clean <- uscoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (United States)`,\n         southpark = `South Park: (United States)`,\n         simpsons = `The Simpsons: (United States)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n  mutate(day = 15, .after = month) %>%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Worldwide\nworldcoms_clean <- worldcoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (Worldwide)`,\n         southpark = `South Park: (Worldwide)`,\n         simpsons = `The Simpsons: (Worldwide)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n  mutate(day = 15, .after = month) %>%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Cyprus.\ncycoms_clean <- cycoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (Cyprus)`,\n         southpark = `South Park: (Cyprus)`,\n         simpsons = `The Simpsons: (Cyprus)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n\n  #I separate the month and  year into two columns. Then I convert the character column to a number.\n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n\n  #I create a new column that is called day, and I use 15 as it is the middle of the month.\n  mutate(day = 15, .after = month) %>%\n\n  #Here I create a date column using the ymd (=year month day)function.\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\nCode that will calculate the average popularity of the terms by year for each of the search terms in each of the geographies.\n\n#HERE WE SUMMARIZE THE AVERAGE POPULARITY OF THE TERMS BY YEAR OF EACH SEARCH\n#Summarize for the US\nuscoms_summary <- uscoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Worldwide\nworldcoms_summary <- worldcoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Cyprus\ncycoms_summary <- cycoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n\nBefore we analyse the data, we tidy them. This way, we have three rows that correspond for each month from 2004 until 2021. This makes easier for us to specify what goes where.\n\nuscoms_tidy <- uscoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworldcoms_tidy <- worldcoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\ncycoms_tidy <- cycoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\n4. Analyze the data to answer the following questions:\n\nIn what year was each term most popular? In which geography is each of the terms most popular in the year you found in Part A?\n\nuscoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\nworldcoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line()+ scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\ncycoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\nBased on the graphs the most popular year for\n‚ÄìFamily guy was: 2nd half of 2008 (USA)/ 1st half of 2009(Worldwide)/ 1st, 2nd half of 2007 (Cyprus)\n‚ÄìSimpsons was: mid-2007 (USA,Worldwide)/ 2nd half of 2008 (Cyprus)\n‚ÄìSouth Park was: 1st half of 2010 (USA, Worldwide) / first half of 2004 (Cyprus)\nCalculate the ratio of the most popular term to the least popular term. Describe how this ratio changed over time in each of the geographies by relying on yearly data.\n\n\n#I hereby summarize in three different ways the mean search score of each series.\nworldcoms_clean %>% summarize(mean(southpark), mean(simpsons), mean(famguy))\n\n# A tibble: 1 √ó 3\n  `mean(southpark)` `mean(simpsons)` `mean(famguy)`\n              <dbl>            <dbl>          <dbl>\n1              22.2             36.2           20.3\n\nworldcoms_clean %>% summarize_at(vars(southpark, simpsons, famguy), mean)\n\n# A tibble: 1 √ó 3\n  southpark simpsons famguy\n      <dbl>    <dbl>  <dbl>\n1      22.2     36.2   20.3\n\nsummary(worldcoms_clean)\n\n      year          month             day         famguy        southpark    \n Min.   :2004   Min.   : 1.000   Min.   :15   Min.   : 5.00   Min.   : 7.00  \n 1st Qu.:2008   1st Qu.: 3.000   1st Qu.:15   1st Qu.:12.00   1st Qu.:11.00  \n Median :2013   Median : 6.000   Median :15   Median :19.00   Median :21.00  \n Mean   :2013   Mean   : 6.475   Mean   :15   Mean   :20.29   Mean   :22.22  \n 3rd Qu.:2017   3rd Qu.: 9.000   3rd Qu.:15   3rd Qu.:29.00   3rd Qu.:29.00  \n Max.   :2022   Max.   :12.000   Max.   :15   Max.   :45.00   Max.   :69.00  \n    simpsons           date           \n Min.   : 13.00   Min.   :2004-01-15  \n 1st Qu.: 23.00   1st Qu.:2008-07-15  \n Median : 37.00   Median :2013-01-15  \n Mean   : 36.18   Mean   :2013-01-13  \n 3rd Qu.: 47.00   3rd Qu.:2017-07-15  \n Max.   :100.00   Max.   :2022-01-15  \n\n#Looking at the results we conclude that the simpsons on average was the most popular by far with 36.18 score. Then southpark follows with 22.22 and then family guy follows with 20.29.\n\nThen we group the averages by year so we can compare the yearly differences.\n\nworld_summary <- worldcoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\nworld_tidy <- world_summary %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() \n\n\n\n\n\nHOMEWORK 3 STARTS HERE\n\nPrepare the graph showing the trends in popularity of the search terms over time. Make sure to add a descriptive title, label the axes, and modify the look of the graph to be presentable. You can choose your own colors or use one of the palettes we talked about in class. Add a one-sentence explanation for the geometry you selected for the graph.\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\") +\n  scale_color_discrete(labels = c(\"Family Guy\", \"The Simpsons\", \"South Park\")) #rename titles\n\n\n\n\nThe same code as before is used to make the graph. The only change here is that we rename the names of the shows in the graph by using the command ‚Äúscale_color_discrete(labels = c(‚Ä¶)‚Äù.\n\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"Popularity graph\",\n       subtitle = \"Top 3 American sitcoms\"\n       ) +\n  scale_color_discrete(name = \"Shows:\",\n                       labels = c(\"South Park\", \"The Simpsons\", \"Family Guy\")) +\n  theme_wsj()  #this theme makes the graph more presentable\n\n\n\n\nThis is a more presentable version of the graph above.\n\nSmooth the data (using a graph) to eliminate random noise. Explain which smoothing method you chose and why.\n\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  #geom_line() + #I remove this so we can see the variability of each sitcom.\n  geom_smooth(method = \"loess\") + #This command creates a moving average that smoothes out all the fluctuations.\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFor this code we use LOESS command to smooth the fluctuations. LOESS (aka locally weighted smoothing), helps us see the relationship between the three variables and foresee trends. A LOESS smoother takes the data, fits a regression with the subset of data, and uses that linear regression model to get a point for the smoothed curve. The points closer to the fitted line are more impact-full.\n\nCreate a chart to show seasonality by month in your data. What does the chart tell you about the seasonality of your chosen search terms? Some examples of seasonality charts can be found here: https://www.r-graph-gallery.com/142-basic-radar-chart.html (Links to an external site.)\n\n\nworldcoms_tidy %>% \n  group_by(month, name) %>% #I group by month to see which season are popular. By name to see the shows.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score, fill = name)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  coord_polar()\n\n`summarise()` has grouped output by 'month'. You can override using the `.groups` argument.\n\n\n\n\n\nAs we can see, the Simpsons seasonality graph looks the same. That is probably we have no limits to the popularity score of the variable and every score is close to each other. i.e.¬†popularity does not vary by a lot.\n\nworldcoms_tidy %>% \n  filter(name == \"simpsons\") %>% #I filter by name \"simpsons\" so I can examine the Simpsons show by itself.\n  group_by(month) %>% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(30, 40)) + #I include score limits from 30 to 40 in order to see the exact seasons where \"The simpsons\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col).\n\n\n\n\n\n\nworldcoms_tidy %>% \n  filter(name == \"famguy\") %>% #I filter by name \"famguy\" so I can examine the \"Family Guy\" show by itself.\n  group_by(month) %>% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(15, 25)) + #I include score limits from 15 to 25 in order to see the exact seasons where \"The Family Guy\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col)."
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-of-all-presidents-words-since-truman",
    "href": "software/presidents_analysis.html#interpretation-of-all-presidents-words-since-truman",
    "title": "Loizos Konstantinou",
    "section": "Interpretation of all president‚Äôs words since Truman",
    "text": "Interpretation of all president‚Äôs words since Truman\nAmerican priorities shifted over time. As we can see, From Hoover (1929) until Nixon (1974) issues and words related to ‚Äúfreedom‚Äù, and ‚Äúpeace‚Äù were emphasized. This makes sense since during that time, WW2 and the Vietnam War were fought.\n‚ÄúEconomy‚Äù seems like a topic that is popular in almost every presidency.\nDuring Ford‚Äôs and Reagan‚Äôs presidency, ‚Äútax‚Äù, and ‚Äúgrowth‚Äù became really hot buzz words. Especially during Reagan administration, big tax reforms were introduced which they have significantly reduce taxes for businesses.\n‚Äúinflation‚Äù and ‚Äúenergy‚Äù were also popular during Nixon, Ford, and Reagan. It is important because especially during Nixon, the US economy after 14 years of economic development got in a stagflationary state; oil and gas crisis at the 70s was also a part of that.\nDuring George W Bush (2005), ‚Äúsecurity‚Äù was the most popular word used. This is because especially after 911, security became the main focus of his presidency.\nHealthcare gained importance during the Clinton Administration, and two administrations later, the Obama Administration expanded Medicaid. With the Covid-19 pandemic, healthcare again dominated the policy priorities in Biden‚Äôs 2022 address.\nAll the presidents, irrespective of their political affiliation (Democrats vs Republicans), mentioned about strengthening / growing the economy. Only presidents affiliated with the Democratic party seemed to emphasize on ‚Äúhealthcare‚Äù, whereas a common theme among the Republican presidents‚Äô addresses was ‚Äúwar / military spending / terrorism‚Äù.\nTop ten words of all Democrat Presidents\n\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\ndemocrat_speeches = pd.read_excel('democrat_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in democrat_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\ndemocrat_speeches['top ten'] = top_list\n\ndisplay(democrat_speeches)\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('industrial', 9),('work', 8),('recovery', 7)...\n    \n    \n      1\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('prosperity', 12),('production', 12),('power...\n    \n    \n      2\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('economic', 16),('development', 10),('peace'...\n    \n    \n      3\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('freedom', 12),('life', 9),('progress', 8),(...\n    \n    \n      4\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      [('inflation', 17),('economic', 14),('tax', 13...\n    \n    \n      5\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('children', 24),('work', 21),('budget', 17),...\n    \n    \n      6\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('jobs', 32),('work', 20),('energy', 18),('fa...\n    \n    \n      7\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('folks', 19),('see', 15),('families', 15),('...\n    \n  \n\n\n\n\nTop ten words of all Republican Presidents\n\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\nrepublican_speeches = pd.read_excel('republican_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in republican_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\nrepublican_speeches['top ten'] = top_list\n\ndisplay(republican_speeches)\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('public', 33),('law', 25),('service', 23),('...\n    \n    \n      1\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('free', 16),('security', 16),('economy', 12)...\n    \n    \n      2\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('peace', 27),('energy', 17),('war', 8),('pro...\n    \n    \n      3\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('energy', 25),('oil', 20),('tax', 17),('econ...\n    \n    \n      4\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('freedom', 20),('tax', 16),('growth', 14),('...\n    \n    \n      5\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('budget', 17),('work', 12),('hope', 10),('dr...\n    \n    \n      6\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('security', 29),('freedom', 20),('social', 1...\n    \n    \n      7\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('tax', 15),('last', 13),('together', 13),('w...\n    \n  \n\n\n\n\n\nCan you conduct topic analysis of LDA using the speeches to determine what things presidents talk about in state of the union speeches?\n\nLinear Discriminant Analysis (LDA) is like PCA, but it focuses on maximizing the seperatibility among known categories\n\ntype(df2.iloc[:,0])\n\npandas.core.series.Series\n\n\n\ndataset = pd.read_excel('speeches53463.xlsx')\n\ngrouped = dataset.groupby('name')\ndataset['date'] = pd.to_datetime(dataset['date'])\n\n#print(dataset.head())\ngrouped = dataset.groupby('name')\n\ndataset2 = dataset.loc[dataset.groupby('name').date.idxmin()]\n#.filter(lambda x: x['date'] == min(x['date']))\n\n\nimport re\nimport numpy as np\n    \n# Print the titles of the first rows \nprint(df2[[0]].head())\n\n# Remove punctuation\ndataset['title_processed'] = dataset['speech'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\ndataset['title_processed'] = dataset['title_processed'].map(lambda x: x.lower())\n\n# Print the processed titles of the first rows \ndataset['title_processed'].head()\n\n                                                   0\n0  Address Before a Joint Session of the Congress...\n1  Address Before a Joint Session of Congress on ...\n2  Address Before a Joint Session of Congress on ...\n3  Annual Message to the Congress on the State of...\n4  Radio Address Summarizing the State of the Uni...\n\n\n0    the presidentthank you thank you thank you goo...\n1    the presidentthank you all very very much than...\n2    thank you very much mr speaker mr vice preside...\n3    the presidentmr speaker mr vice president memb...\n4    the presidentmadam speaker mr vice president m...\nName: title_processed, dtype: object\n\n\n\n# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(dataset['title_processed'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation",
    "href": "software/presidents_analysis.html#interpretation",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nWhile the most common words do not seem to reveal something particular, when those generic words are cleaned as we saw above, we get specific topics and buzz words from each president.\n\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 5\nnumber_words = 25\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)\n\nTopics found via LDA:\n\nTopic #0:\nwar forces fighting men production japanese enemies peace peoples china enemy army german air germany world know armed nations united shall chinese nurses allies victory\n\nTopic #1:\ngovernment congress world national great people public federal law shall business economic power nations nation labor present war year country men action united legislation states\n\nTopic #2:\nnew people america year american years world congress government make nation americans time help work federal tax economy jobs let need security peace programs know\n\nTopic #3:\nanarchist anarchistic mckinley anarchists anarchy doctrines 1872 anna bocanegra dictator bridge haro reappropriation garfield apologizes malefactor recoined amazement imprison buren limb alleging preaching loses murder\n\nTopic #4:\nstates government united congress country great public year citizens time people state treaty war present foreign subject american shall act general power nations commerce necessary"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-1",
    "href": "software/presidents_analysis.html#interpretation-1",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nTopic 0: aligns with internal state laws and political stability.\nTopic 1: seems to align mostly with internal US issues. It is targeted to citizens stability (jobs) and needs.\nTopic 2: Seems to be more related with tax policy and approach to the US economy.\nTopic 3: It has to do with public policy and foreign affairs.\nTopic 4: Mainly targeted to defence department and peace status maintenance while protecting the interests of the US.\n\nCan you determine the sentiment of each state of the union using nltk‚Äôs Vader module?\n\n\nanalyzer=SentimentIntensityAnalyzer()\ndef polarity_score(text):\n    if len(text)>0:\n        score=analyzer.polarity_scores(text)['compound']\n        return score\n    else:\n        return 0\ndataset['polarityscore'] = dataset['speech'].apply(lambda text : polarity_score(text))\ndataset['polarityscore']\n\n0      0.9999\n1      0.9999\n2      1.0000\n3      0.9999\n4      0.9999\n        ...  \n254    0.9998\n255    0.9994\n256    0.9995\n257    0.9991\n258   -0.9997\nName: polarityscore, Length: 259, dtype: float64\n\n\n\ndef sentianamolybarplot(df):\n    polarity_scale=[0.9991,0.9992,0.9993,0.9994,0.9995,0.9996,0.9997,0.9998,0.9999,1]\n    #'Review_polarity' is column name of sentiment score calculated for whole review.\n    df3=df[(df['polarityscore']>0)]\n    out = pd.cut(df3['polarityscore'],polarity_scale)\n    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n    plt.show()\nsentianamolybarplot(dataset)\n\n\n\n\nInterpretation\nIn the State of Union Speeches, the presidents talk about important issues facing Americans and offers their ideas on solving the nation‚Äôs problems, including suggestions for new laws and policies. As displayed in the plot, the polarity scores for 13 speeches (barring W. Bush) is positive. This is understandable as the State of Union speeches are a PR vehicle, leveraged to display the President‚Äôs power and positive influence.\n\nDo speeches of different presidents cluster in any way that can allow you to determine their political party? How different are Biden and Trump according to this clustering?\n\n\ndef remove_noise(text, stop_words = nltk.corpus.stopwords.words('english')):\n    newsw = ['annual', 'number', 'help', 'thank', 'get', 'going', 'think', 'look', 'said', 'create',\n             'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', 'long', 'better', \n             'plan', 'national', 'ask' '10', 'much', 'good', 'great', 'best', 'cannot', 'still', \n             'know', 'years', '1', 'major', 'want', 'able', 'put', 'capacity', 'programs', 'per', \n             'percent', 'million', 'act', 'provide', 'afford', 'needed', 'may', 'possible', 'full',\n             '2', 'effort', 'meeting', 'address', 'ever', 'measures', 'ago', 'delivered', '5', \n             'program', 'past', 'future', 'need', 'needs', 'house', 'also', 'tonight', 'propose', \n             'toward', 'continue', 'society','country', 'seek', 'period', 'year', 'man', 'men', \n             'one', 'areas', 'begin', 'live', 'make', 'let', 'upon', 'well', 'office', 'meet', \n             'make' 'citizens', 'human', 'self', 'among', 'peoples', 'affairs', 'would', 'field', \n             'first', 'interest', 'today', 'recommendations', 'recomenndation', 'within', 'shall', \n             'administration', 'nation', 'nations', 'us', 'we', 'policy', 'legislation', 'time', \n             'new', 'many', 'several', 'few', 'government', 'world', 'people', 'united', 'states', \n             'system', 'every', 'people', 'must', '626','give', 'categories', '226762', '17608', \n             '24532', '430', '38','statistics', 'analyses', 'miscellaneous', 'congressional', \n             'skip', 'content', 'documents', 'attributes', 'media', 'message', 'congress', 'state',\n             'union', 'america', 'american', 'americans', 'presidency', 'president', 'project', \n             'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', 'main', \n             'take','like','yet','j','000']\n    stop_words = stop_words + newsw\n    tokens = word_tokenize(text)\n    cleaned_tokens = []\n    for token in tokens:\n        token = re.sub('[^A-Za-z0-9]+', '', token)\n        if len(token) > 1 and token.lower() not in stop_words:\n            # Get lowercase\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\n# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.8,\n                                   max_features = 50,\n                                   min_df = 0.1,\n                                   tokenizer = remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(dataset['speech'].values)\n\n\nnum_clusters = 2\n\n# Generate cluster centers through the kmeans function\ncluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n# display(cluster_centers)\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print('Cluster: {}'.format(i+1))\n    # Sort the terms and print top 3 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms [:15])\n\nCluster: 1\n['federal', 'economic', 'budget', 'work', 'tax', 'economy', 'security', 'jobs', 'nt', 'freedom', 'health', 'free', 'life', 'together', 'defense']\nCluster: 2\n['treaty', 'subject', 'commerce', 'treasury', 'general', 'relations', 'powers', 'laws', 'duty', 'session', 'interests', 'rights', 'however', 'service', 'trade']"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-2",
    "href": "software/presidents_analysis.html#interpretation-2",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nOn clustering the popular words in the speech, it seems like Cluster 1 aligns with speeches by Republican presidents and Cluster 2 with that of speeches by Democartic presidents.\n\n#Converting the datafram into list for further analysis.\nspeech_list = []\nfor i in range(len(dataset2)):\n    speech_list.append(dataset2.iloc[[i]]['speech'].item())\n    \ntitles = []\nfor i in range(len(dataset2)):\n    titles.append(dataset2.iloc[[i]]['name'].item())\n    \ntexts = [txt.split() for txt in speech_list]\n\n# Create an instance of a PorterStemmer object\nporter = PorterStemmer()\n\n# For each token of each text, we generated its stem \ntexts_stem = [[porter.stem(token) for token in text] for text in texts]\n\n# Create a dictionary from the stemmed tokens\ndictionary = corpora.Dictionary(texts_stem)\n\n# Create a bag-of-words model for each speech, using the previously generated dictionary\nbows = [dictionary.doc2bow(text) for text in texts_stem]\n\n# Generate the tf-idf model\nmodel = TfidfModel(bows)\n\n# Compute the similarity matrix (pairwise distance between all speeches)\nsims = similarities.MatrixSimilarity(model[bows])\n\n# Transform the resulting list into a DataFrame\nsim_df = pd.DataFrame(list(sims))\n\n# Add the name of the presidents as columns and index of the DataFrame\nsim_df.columns = titles\nsim_df.index = titles\n\n# Print the resulting matrix\nsim_df\n\n\n\n\n  \n    \n      \n      Abraham Lincoln\n      Andrew Jackson\n      Andrew Johnson\n      Barack Obama\n      Benjamin Harrison\n      Calvin Coolidge\n      Chester A. Arthur\n      Donald J. Trump\n      Dwight D. Eisenhower\n      Franklin D. Roosevelt\n      ...\n      Rutherford B. Hayes\n      Theodore Roosevelt\n      Thomas Jefferson\n      Ulysses S. Grant\n      Warren G. Harding\n      William Howard Taft\n      William J. Clinton\n      William McKinley\n      Woodrow Wilson\n      Zachary Taylor\n    \n  \n  \n    \n      Abraham Lincoln\n      1.000000\n      0.121246\n      0.151205\n      0.047722\n      0.110114\n      0.118578\n      0.114825\n      0.047754\n      0.075706\n      0.047841\n      ...\n      0.092012\n      0.056717\n      0.110742\n      0.124756\n      0.082261\n      0.092794\n      0.046491\n      0.113427\n      0.077418\n      0.145153\n    \n    \n      Andrew Jackson\n      0.121246\n      1.000000\n      0.121070\n      0.053532\n      0.131924\n      0.086656\n      0.106385\n      0.054705\n      0.077823\n      0.047385\n      ...\n      0.112656\n      0.070145\n      0.117074\n      0.146622\n      0.092593\n      0.113945\n      0.050358\n      0.102445\n      0.086139\n      0.155541\n    \n    \n      Andrew Johnson\n      0.151205\n      0.121070\n      1.000000\n      0.047996\n      0.097058\n      0.094947\n      0.079968\n      0.049593\n      0.073742\n      0.053389\n      ...\n      0.082619\n      0.077785\n      0.093668\n      0.127001\n      0.090346\n      0.077516\n      0.049155\n      0.080710\n      0.082443\n      0.092401\n    \n    \n      Barack Obama\n      0.047722\n      0.053532\n      0.047996\n      1.000000\n      0.046357\n      0.069860\n      0.037358\n      0.213163\n      0.103428\n      0.096561\n      ...\n      0.035344\n      0.069914\n      0.054143\n      0.047495\n      0.073191\n      0.049368\n      0.317297\n      0.050867\n      0.064416\n      0.039508\n    \n    \n      Benjamin Harrison\n      0.110114\n      0.131924\n      0.097058\n      0.046357\n      1.000000\n      0.095429\n      0.198889\n      0.043727\n      0.082837\n      0.068579\n      ...\n      0.269502\n      0.057292\n      0.080109\n      0.146124\n      0.080189\n      0.125491\n      0.049069\n      0.142834\n      0.076850\n      0.134368\n    \n    \n      Calvin Coolidge\n      0.118578\n      0.086656\n      0.094947\n      0.069860\n      0.095429\n      1.000000\n      0.085849\n      0.072170\n      0.118543\n      0.078246\n      ...\n      0.081089\n      0.087503\n      0.077970\n      0.098388\n      0.128878\n      0.092294\n      0.076209\n      0.074285\n      0.086982\n      0.092193\n    \n    \n      Chester A. Arthur\n      0.114825\n      0.106385\n      0.079968\n      0.037358\n      0.198889\n      0.085849\n      1.000000\n      0.042525\n      0.058818\n      0.042672\n      ...\n      0.144648\n      0.045146\n      0.063310\n      0.179516\n      0.066345\n      0.137828\n      0.045936\n      0.123470\n      0.059776\n      0.132799\n    \n    \n      Donald J. Trump\n      0.047754\n      0.054705\n      0.049593\n      0.213163\n      0.043727\n      0.072170\n      0.042525\n      1.000000\n      0.077175\n      0.062748\n      ...\n      0.040511\n      0.077699\n      0.046509\n      0.056097\n      0.065570\n      0.040417\n      0.183823\n      0.042885\n      0.056082\n      0.040505\n    \n    \n      Dwight D. Eisenhower\n      0.075706\n      0.077823\n      0.073742\n      0.103428\n      0.082837\n      0.118543\n      0.058818\n      0.077175\n      1.000000\n      0.108428\n      ...\n      0.059357\n      0.074973\n      0.060950\n      0.085503\n      0.107023\n      0.072974\n      0.128444\n      0.065945\n      0.104088\n      0.069061\n    \n    \n      Franklin D. Roosevelt\n      0.047841\n      0.047385\n      0.053389\n      0.096561\n      0.068579\n      0.078246\n      0.042672\n      0.062748\n      0.108428\n      1.000000\n      ...\n      0.064508\n      0.072999\n      0.052945\n      0.059127\n      0.094722\n      0.054787\n      0.086663\n      0.063699\n      0.077462\n      0.050414\n    \n    \n      Franklin Pierce\n      0.138249\n      0.149229\n      0.113608\n      0.039742\n      0.135522\n      0.096017\n      0.129588\n      0.034980\n      0.083958\n      0.055784\n      ...\n      0.096431\n      0.059912\n      0.091416\n      0.149410\n      0.087211\n      0.114362\n      0.038283\n      0.098263\n      0.081036\n      0.181408\n    \n    \n      George Bush\n      0.039750\n      0.110718\n      0.045218\n      0.223350\n      0.039387\n      0.067593\n      0.030361\n      0.166730\n      0.101130\n      0.063581\n      ...\n      0.029770\n      0.064276\n      0.043393\n      0.048644\n      0.064347\n      0.045680\n      0.236761\n      0.042725\n      0.064541\n      0.038623\n    \n    \n      George W. Bush\n      0.045682\n      0.045953\n      0.042901\n      0.244457\n      0.043902\n      0.076660\n      0.044497\n      0.170425\n      0.109930\n      0.069602\n      ...\n      0.038201\n      0.047932\n      0.043533\n      0.048213\n      0.069961\n      0.033635\n      0.258270\n      0.042687\n      0.054859\n      0.033777\n    \n    \n      George Washington\n      0.075427\n      0.080150\n      0.054498\n      0.022271\n      0.046189\n      0.048199\n      0.043466\n      0.021537\n      0.033813\n      0.023598\n      ...\n      0.056167\n      0.028022\n      0.075892\n      0.047378\n      0.038203\n      0.039933\n      0.022466\n      0.030561\n      0.040550\n      0.058378\n    \n    \n      Gerald R. Ford\n      0.040651\n      0.041807\n      0.043266\n      0.133494\n      0.045676\n      0.079921\n      0.053701\n      0.082173\n      0.135036\n      0.062518\n      ...\n      0.035063\n      0.051462\n      0.039757\n      0.051069\n      0.071961\n      0.044571\n      0.183497\n      0.041057\n      0.048935\n      0.039580\n    \n    \n      Grover Cleveland\n      0.117943\n      0.128614\n      0.102102\n      0.036857\n      0.129553\n      0.088140\n      0.141185\n      0.043138\n      0.070560\n      0.049183\n      ...\n      0.098994\n      0.069918\n      0.072758\n      0.129015\n      0.083239\n      0.154388\n      0.035827\n      0.105645\n      0.084410\n      0.169106\n    \n    \n      Harry S. Truman\n      0.060982\n      0.071213\n      0.065671\n      0.087225\n      0.070839\n      0.099180\n      0.058541\n      0.080372\n      0.165834\n      0.092160\n      ...\n      0.055451\n      0.064122\n      0.054537\n      0.079809\n      0.106270\n      0.081560\n      0.105554\n      0.059129\n      0.081950\n      0.079267\n    \n    \n      Herbert Hoover\n      0.103231\n      0.079289\n      0.081658\n      0.070459\n      0.111400\n      0.127897\n      0.128176\n      0.049934\n      0.146132\n      0.097870\n      ...\n      0.078349\n      0.062296\n      0.072092\n      0.110752\n      0.135878\n      0.098500\n      0.090548\n      0.076114\n      0.096796\n      0.091948\n    \n    \n      James Buchanan\n      0.074219\n      0.117172\n      0.085650\n      0.053138\n      0.213879\n      0.067012\n      0.108787\n      0.033511\n      0.050480\n      0.062365\n      ...\n      0.181783\n      0.050247\n      0.071056\n      0.138804\n      0.063212\n      0.083642\n      0.036889\n      0.143667\n      0.058985\n      0.117347\n    \n    \n      James K. Polk\n      0.108668\n      0.127383\n      0.080034\n      0.034267\n      0.083551\n      0.063491\n      0.086467\n      0.037409\n      0.050591\n      0.038777\n      ...\n      0.081692\n      0.041648\n      0.059918\n      0.124555\n      0.054302\n      0.098106\n      0.031113\n      0.069200\n      0.073058\n      0.204857\n    \n    \n      James Madison\n      0.071391\n      0.094473\n      0.062079\n      0.024049\n      0.066594\n      0.058868\n      0.067699\n      0.027369\n      0.047359\n      0.028658\n      ...\n      0.071821\n      0.037817\n      0.098455\n      0.088595\n      0.049361\n      0.056839\n      0.027100\n      0.063285\n      0.035300\n      0.113097\n    \n    \n      James Monroe\n      0.121900\n      0.130480\n      0.097276\n      0.034281\n      0.096656\n      0.073366\n      0.089977\n      0.034260\n      0.062603\n      0.045482\n      ...\n      0.076209\n      0.049221\n      0.112049\n      0.125313\n      0.073308\n      0.077521\n      0.033153\n      0.085300\n      0.060619\n      0.129747\n    \n    \n      Jimmy Carter\n      0.038933\n      0.046552\n      0.046946\n      0.240644\n      0.048041\n      0.088707\n      0.047153\n      0.162571\n      0.159608\n      0.088317\n      ...\n      0.040429\n      0.069212\n      0.045362\n      0.048941\n      0.091093\n      0.049687\n      0.229270\n      0.049615\n      0.075476\n      0.042085\n    \n    \n      John Adams\n      0.085961\n      0.099759\n      0.060527\n      0.025965\n      0.059714\n      0.045513\n      0.077210\n      0.026649\n      0.044619\n      0.024816\n      ...\n      0.053008\n      0.042880\n      0.083986\n      0.104180\n      0.036770\n      0.063388\n      0.029237\n      0.067327\n      0.038350\n      0.110925\n    \n    \n      John F. Kennedy\n      0.054274\n      0.064872\n      0.067020\n      0.145161\n      0.064716\n      0.087427\n      0.052211\n      0.091397\n      0.166903\n      0.082637\n      ...\n      0.060584\n      0.066535\n      0.050081\n      0.067712\n      0.091555\n      0.065329\n      0.145564\n      0.086820\n      0.069619\n      0.051925\n    \n    \n      John Quincy Adams\n      0.134975\n      0.163353\n      0.091024\n      0.045042\n      0.101502\n      0.086493\n      0.104866\n      0.043737\n      0.062438\n      0.040328\n      ...\n      0.084253\n      0.063036\n      0.126550\n      0.123556\n      0.071098\n      0.098951\n      0.040523\n      0.082057\n      0.072336\n      0.149257\n    \n    \n      John Tyler\n      0.131702\n      0.152333\n      0.117307\n      0.045652\n      0.123190\n      0.103976\n      0.117884\n      0.047375\n      0.062916\n      0.067584\n      ...\n      0.141787\n      0.081356\n      0.119774\n      0.153659\n      0.101667\n      0.120820\n      0.052103\n      0.113313\n      0.072481\n      0.170983\n    \n    \n      Joseph R. Biden\n      0.030122\n      0.035307\n      0.032910\n      0.345184\n      0.028887\n      0.054890\n      0.031472\n      0.220390\n      0.068947\n      0.053099\n      ...\n      0.021369\n      0.050805\n      0.034194\n      0.039431\n      0.052046\n      0.028445\n      0.306361\n      0.027510\n      0.054481\n      0.033634\n    \n    \n      Lyndon B. Johnson\n      0.045479\n      0.043621\n      0.046318\n      0.128715\n      0.035904\n      0.065005\n      0.038097\n      0.106125\n      0.100885\n      0.056544\n      ...\n      0.033196\n      0.058649\n      0.040729\n      0.053333\n      0.065493\n      0.042870\n      0.168042\n      0.054452\n      0.047279\n      0.036023\n    \n    \n      Martin van Buren\n      0.123016\n      0.189609\n      0.095427\n      0.050744\n      0.148065\n      0.078894\n      0.110916\n      0.030465\n      0.075507\n      0.051770\n      ...\n      0.114139\n      0.069005\n      0.121863\n      0.127558\n      0.084042\n      0.114678\n      0.043701\n      0.112850\n      0.086940\n      0.182542\n    \n    \n      Millard Fillmore\n      0.128602\n      0.168584\n      0.111184\n      0.040514\n      0.116195\n      0.083215\n      0.112446\n      0.046339\n      0.078221\n      0.067761\n      ...\n      0.102227\n      0.080306\n      0.119655\n      0.143147\n      0.097297\n      0.102903\n      0.044584\n      0.100449\n      0.100466\n      0.200081\n    \n    \n      Richard Nixon\n      0.039255\n      0.049510\n      0.062639\n      0.139949\n      0.042007\n      0.068414\n      0.038798\n      0.105487\n      0.136241\n      0.066019\n      ...\n      0.036937\n      0.061997\n      0.043634\n      0.054097\n      0.077172\n      0.050246\n      0.174663\n      0.044486\n      0.073935\n      0.037279\n    \n    \n      Ronald Reagan\n      0.042896\n      0.039901\n      0.044701\n      0.231009\n      0.048165\n      0.076235\n      0.040150\n      0.140081\n      0.140061\n      0.081909\n      ...\n      0.034992\n      0.052896\n      0.045076\n      0.049203\n      0.080074\n      0.039373\n      0.300796\n      0.046391\n      0.057221\n      0.038241\n    \n    \n      Rutherford B. Hayes\n      0.092012\n      0.112656\n      0.082619\n      0.035344\n      0.269502\n      0.081089\n      0.144648\n      0.040511\n      0.059357\n      0.064508\n      ...\n      1.000000\n      0.058833\n      0.074037\n      0.131257\n      0.078165\n      0.077687\n      0.046635\n      0.139910\n      0.066988\n      0.097658\n    \n    \n      Theodore Roosevelt\n      0.056717\n      0.070145\n      0.077785\n      0.069914\n      0.057292\n      0.087503\n      0.045146\n      0.077699\n      0.074973\n      0.072999\n      ...\n      0.058833\n      1.000000\n      0.050832\n      0.071917\n      0.103435\n      0.060783\n      0.066718\n      0.069393\n      0.067994\n      0.059469\n    \n    \n      Thomas Jefferson\n      0.110742\n      0.117074\n      0.093668\n      0.054143\n      0.080109\n      0.077970\n      0.063310\n      0.046509\n      0.060950\n      0.052945\n      ...\n      0.074037\n      0.050832\n      1.000000\n      0.093839\n      0.069619\n      0.051927\n      0.047307\n      0.067414\n      0.075135\n      0.109134\n    \n    \n      Ulysses S. Grant\n      0.124756\n      0.146622\n      0.127001\n      0.047495\n      0.146124\n      0.098388\n      0.179516\n      0.056097\n      0.085503\n      0.059127\n      ...\n      0.131257\n      0.071917\n      0.093839\n      1.000000\n      0.091914\n      0.121181\n      0.060122\n      0.167308\n      0.080233\n      0.137702\n    \n    \n      Warren G. Harding\n      0.082261\n      0.092593\n      0.090346\n      0.073191\n      0.080189\n      0.128878\n      0.066345\n      0.065570\n      0.107023\n      0.094722\n      ...\n      0.078165\n      0.103435\n      0.069619\n      0.091914\n      1.000000\n      0.081010\n      0.092189\n      0.096931\n      0.096031\n      0.070389\n    \n    \n      William Howard Taft\n      0.092794\n      0.113945\n      0.077516\n      0.049368\n      0.125491\n      0.092294\n      0.137828\n      0.040417\n      0.072974\n      0.054787\n      ...\n      0.077687\n      0.060783\n      0.051927\n      0.121181\n      0.081010\n      1.000000\n      0.045340\n      0.082594\n      0.087100\n      0.124097\n    \n    \n      William J. Clinton\n      0.046491\n      0.050358\n      0.049155\n      0.317297\n      0.049069\n      0.076209\n      0.045936\n      0.183823\n      0.128444\n      0.086663\n      ...\n      0.046635\n      0.066718\n      0.047307\n      0.060122\n      0.092189\n      0.045340\n      1.000000\n      0.051195\n      0.075808\n      0.044786\n    \n    \n      William McKinley\n      0.113427\n      0.102445\n      0.080710\n      0.050867\n      0.142834\n      0.074285\n      0.123470\n      0.042885\n      0.065945\n      0.063699\n      ...\n      0.139910\n      0.069393\n      0.067414\n      0.167308\n      0.096931\n      0.082594\n      0.051195\n      1.000000\n      0.067011\n      0.103954\n    \n    \n      Woodrow Wilson\n      0.077418\n      0.086139\n      0.082443\n      0.064416\n      0.076850\n      0.086982\n      0.059776\n      0.056082\n      0.104088\n      0.077462\n      ...\n      0.066988\n      0.067994\n      0.075135\n      0.080233\n      0.096031\n      0.087100\n      0.075808\n      0.067011\n      1.000000\n      0.081701\n    \n    \n      Zachary Taylor\n      0.145153\n      0.155541\n      0.092401\n      0.039508\n      0.134368\n      0.092193\n      0.132799\n      0.040505\n      0.069061\n      0.050414\n      ...\n      0.097658\n      0.059469\n      0.109134\n      0.137702\n      0.070389\n      0.124097\n      0.044786\n      0.103954\n      0.081701\n      1.000000\n    \n  \n\n43 rows √ó 43 columns\n\n\n\nHere we can see the degree of similarity of each president‚Äôs speech with each other. There is no speech with similarity more than 40%.\n\n# Compute the clusters from the similarity matrix,\n# using the Ward variance minimization algorithm\nZ = hierarchy.linkage(sim_df, 'ward')\nplt.rcParams['figure.figsize'] = [10,20]\n# Display this result as a horizontal dendrogram\na = hierarchy.dendrogram(Z,  leaf_font_size=20, labels=sim_df.index,  orientation=\"left\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-3",
    "href": "software/presidents_analysis.html#interpretation-3",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nIn the dendrogram above, the presidents seem to be clustered based on the era that they served. Two big clusters are distinct ‚Äì the green one which includes mostly recent presidents 20th and 21st century, and ‚Äì the orange one which includes presidents of the 18th and 19th century.\n\nWho was the president whose speech was the most similar to the speech of Biden in 2022?\n\n\nv = sim_df[['Joseph R. Biden']]\nv\n\n\n\n\n  \n    \n      \n      Joseph R. Biden\n    \n  \n  \n    \n      Abraham Lincoln\n      0.030122\n    \n    \n      Andrew Jackson\n      0.035307\n    \n    \n      Andrew Johnson\n      0.032910\n    \n    \n      Barack Obama\n      0.345184\n    \n    \n      Benjamin Harrison\n      0.028887\n    \n    \n      Calvin Coolidge\n      0.054890\n    \n    \n      Chester A. Arthur\n      0.031472\n    \n    \n      Donald J. Trump\n      0.220390\n    \n    \n      Dwight D. Eisenhower\n      0.068947\n    \n    \n      Franklin D. Roosevelt\n      0.053099\n    \n    \n      Franklin Pierce\n      0.030974\n    \n    \n      George Bush\n      0.227037\n    \n    \n      George W. Bush\n      0.205001\n    \n    \n      George Washington\n      0.012446\n    \n    \n      Gerald R. Ford\n      0.094785\n    \n    \n      Grover Cleveland\n      0.025062\n    \n    \n      Harry S. Truman\n      0.052615\n    \n    \n      Herbert Hoover\n      0.045206\n    \n    \n      James Buchanan\n      0.024993\n    \n    \n      James K. Polk\n      0.038190\n    \n    \n      James Madison\n      0.016895\n    \n    \n      James Monroe\n      0.019880\n    \n    \n      Jimmy Carter\n      0.198534\n    \n    \n      John Adams\n      0.016608\n    \n    \n      John F. Kennedy\n      0.094466\n    \n    \n      John Quincy Adams\n      0.024704\n    \n    \n      John Tyler\n      0.035437\n    \n    \n      Joseph R. Biden\n      1.000000\n    \n    \n      Lyndon B. Johnson\n      0.107517\n    \n    \n      Martin van Buren\n      0.030007\n    \n    \n      Millard Fillmore\n      0.024063\n    \n    \n      Richard Nixon\n      0.113265\n    \n    \n      Ronald Reagan\n      0.223184\n    \n    \n      Rutherford B. Hayes\n      0.021369\n    \n    \n      Theodore Roosevelt\n      0.050805\n    \n    \n      Thomas Jefferson\n      0.034194\n    \n    \n      Ulysses S. Grant\n      0.039431\n    \n    \n      Warren G. Harding\n      0.052046\n    \n    \n      William Howard Taft\n      0.028445\n    \n    \n      William J. Clinton\n      0.306361\n    \n    \n      William McKinley\n      0.027510\n    \n    \n      Woodrow Wilson\n      0.054481\n    \n    \n      Zachary Taylor\n      0.033634\n    \n  \n\n\n\n\n\n# This is needed to display plots in a notebook\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10,10]\n\n# Select the column corresponding to Biden's address and \nv = sim_df['Joseph R. Biden']\n\n# Sort by ascending scores\nv_sorted = v.sort_values(ascending=True)\n\n# Plot this data has a horizontal bar plot\nv_sorted.plot.barh(x='lab', y='val', rot=0).plot()\n\n# Modify the axes labels and plot title for better readability\nplt.xlabel(\"Cosine distance\")\nplt.ylabel(\"\")\nplt.title(\"Most similar to Biden's\")\n\nText(0.5, 1.0, \"Most similar to Biden's\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-4",
    "href": "software/presidents_analysis.html#interpretation-4",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nBiden‚Äôs address is most similar to that of Obama‚Äôs, follwed by Clinton and George Bush. As we can see, George Washington, John Adams and James Madison are the least similar to Biden. This can be explained by the different eras that each president lived. Speeches in 18th century are different than speeches of today. Biden‚Äôs speech similarity with Obama and Clinton makes sense also because they are all recently elected democrats.\n\nBonus points: (5 points): Develop and algorithm that can allow you to determine if the speech was given by a Democrat or by a republican.\n\nPS2: I will go over this homework on Thursday to help you think through how to solve it. You will be able to recycle a lot of code discussed.\n\nspeeches\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('public', 33),('law', 25),('service', 23),('...\n    \n    \n      1\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('industrial', 9),('work', 8),('recovery', 7)...\n    \n    \n      2\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('prosperity', 12),('production', 12),('power...\n    \n    \n      3\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('free', 16),('security', 16),('economy', 12)...\n    \n    \n      4\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('economic', 16),('development', 10),('peace'...\n    \n    \n      5\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('freedom', 12),('life', 9),('progress', 8),(...\n    \n    \n      6\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('peace', 27),('energy', 17),('war', 8),('pro...\n    \n    \n      7\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('energy', 25),('oil', 20),('tax', 17),('econ...\n    \n    \n      8\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      [('inflation', 17),('economic', 14),('tax', 13...\n    \n    \n      9\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('freedom', 20),('tax', 16),('growth', 14),('...\n    \n    \n      10\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('budget', 17),('work', 12),('hope', 10),('dr...\n    \n    \n      11\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('children', 24),('work', 21),('budget', 17),...\n    \n    \n      12\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('security', 29),('freedom', 20),('social', 1...\n    \n    \n      13\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('jobs', 32),('work', 20),('energy', 18),('fa...\n    \n    \n      14\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('tax', 15),('last', 13),('together', 13),('w...\n    \n    \n      15\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('folks', 19),('see', 15),('families', 15),('...\n    \n  \n\n\n\n\n\ndf2[[0]]\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      Address Before a Joint Session of the Congress...\n    \n    \n      1\n      Address Before a Joint Session of Congress on ...\n    \n    \n      2\n      Address Before a Joint Session of Congress on ...\n    \n    \n      3\n      Annual Message to the Congress on the State of...\n    \n    \n      4\n      Radio Address Summarizing the State of the Uni...\n    \n    \n      5\n      Fifth Annual Message | The American Presidency...\n    \n    \n      6\n      First Annual Message | The American Presidency...\n    \n    \n      7\n      First Annual Message | The American Presidency...\n    \n    \n      8\n      First Annual Message | The American Presidency...\n    \n    \n      9\n      Fifth Annual Message | The American Presidency...\n    \n    \n      10\n      Fifth Annual Message | The American Presidency...\n    \n    \n      11\n      State of the Union Message to the Congress: Ov...\n    \n    \n      12\n      State of the Union Message to the Congress on ...\n    \n    \n      13\n      Address Before a Joint Session of the Congress...\n    \n  \n\n\n\n\n\nstate_speeches = pd.read_excel('state_speeches.xlsx')\nstate_speeches\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      address\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      The Constitution requires that the President \"...\n    \n    \n      1\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      I COME before you at the opening of the Regula...\n    \n    \n      2\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      I am happy to report to this 81st Congress tha...\n    \n    \n      3\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      I appear before the Congress today to report o...\n    \n    \n      4\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      It is a pleasure to return from whence I came....\n    \n    \n      5\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      On this Hill which was my home, I am stirred b...\n    \n    \n      6\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      We meet here tonight at a time of great challe...\n    \n    \n      7\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      Twenty-six years ago, a freshman Congressman, ...\n    \n    \n      8\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      Two years ago today we had the first caucus in...\n    \n    \n      9\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      I come before you to report on the state of ou...\n    \n    \n      10\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      Mr. Speaker, Mr. President, and distinguished ...\n    \n    \n      11\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      Mr. Speaker, Mr. Vice President, Members of th...\n    \n    \n      12\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      As a new Congress gathers, all of us in the el...\n    \n    \n      13\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      Please, everybody, have a seat. Mr. Speaker, M...\n    \n    \n      14\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      The President. Mr. Speaker, Mr. Vice President...\n    \n    \n      15\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      The President. Thank you all very, very much. ...\n    \n  \n\n\n\n\n\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(state_speeches['address'], state_speeches['party'], test_size=0.3, \n                 random_state=53)\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n# Create count train and test variables\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ntfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train, y_train)\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\ncount_nb = MultinomialNB()\ncount_nb.fit(count_train, y_train)\ncount_nb_pred = count_nb.predict(count_test)\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.2\nNaiveBayes Count Score:  0.2\n\n\n\n%matplotlib inline\nfrom sklearn.metrics import plot_confusion_matrix\n\n\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred, labels=['republican', 'democrat'])\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred, labels=['republican', 'democrat'])\n\n# plot_confusion_matrix(tfidf_nb_cm, classes=['republican', 'democrat'], title=\"TF-IDF NB Confusion Matrix\")\n\n# plot_confusion_matrix(count_nb_cm, classes=['republican', 'democrat'], title=\"Count NB Confusion Matrix\", figure=1)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-5",
    "href": "software/presidents_analysis.html#interpretation-5",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nIt looks like that algorithm‚Äôs power to identify whether the speech comes from a democrat or a republican is only 20%. In this case, for the algorithm to get stronger, more speeches are necessary from both sides, and maybe more text cleaning.\n\npip install jupyterthemes\n\nRequirement already satisfied: jupyterthemes in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (0.20.0)\nRequirement already satisfied: notebook>=5.6.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (6.4.5)\nRequirement already satisfied: ipython>=5.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (7.29.0)\nRequirement already satisfied: lesscpy>=0.11.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (0.15.0)\nRequirement already satisfied: jupyter-core in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (4.8.1)\nRequirement already satisfied: matplotlib>=1.4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (3.4.3)\nRequirement already satisfied: matplotlib-inline in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\nRequirement already satisfied: backcall in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\nRequirement already satisfied: pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (2.10.0)\nRequirement already satisfied: appnope in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.20)\nRequirement already satisfied: jedi>=0.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.18.0)\nRequirement already satisfied: pexpect>4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (4.8.0)\nRequirement already satisfied: setuptools>=18.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (58.0.4)\nRequirement already satisfied: traitlets>=4.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.0)\nRequirement already satisfied: pickleshare in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\nRequirement already satisfied: decorator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=5.4.1->jupyterthemes) (0.8.2)\nRequirement already satisfied: ply in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\nRequirement already satisfied: six in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.16.0)\nRequirement already satisfied: pillow>=6.2.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (8.4.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (3.0.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\nRequirement already satisfied: numpy>=1.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.20.3)\nRequirement already satisfied: nbconvert in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.0)\nRequirement already satisfied: pyzmq>=17 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (22.2.1)\nRequirement already satisfied: ipython-genutils in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\nRequirement already satisfied: argon2-cffi in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (20.1.0)\nRequirement already satisfied: terminado>=0.8.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.9.4)\nRequirement already satisfied: tornado>=6.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1)\nRequirement already satisfied: jupyter-client>=5.3.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.12)\nRequirement already satisfied: prometheus-client in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.11.0)\nRequirement already satisfied: jinja2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.3)\nRequirement already satisfied: ipykernel in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.4.1)\nRequirement already satisfied: nbformat in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.3)\nRequirement already satisfied: Send2Trash>=1.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (1.8.0)\nRequirement already satisfied: ptyprocess>=0.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=5.4.1->jupyterthemes) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\nRequirement already satisfied: cffi>=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.14.6)\nRequirement already satisfied: pycparser in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.20)\nRequirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipykernel->notebook>=5.6.0->jupyterthemes) (1.4.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.1)\nRequirement already satisfied: testpath in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.3)\nRequirement already satisfied: jupyterlab-pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\nRequirement already satisfied: defusedxml in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.7.1)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.3)\nRequirement already satisfied: entrypoints>=0.2.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\nRequirement already satisfied: bleach in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (4.0.0)\nRequirement already satisfied: mistune<2,>=0.8.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\nRequirement already satisfied: nest-asyncio in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.5.1)\nRequirement already satisfied: async-generator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.10)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.18.0)\nRequirement already satisfied: attrs>=17.4.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (21.2.0)\n\n\nRequirement already satisfied: packaging in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (21.0)\nRequirement already satisfied: webencodings in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n!jt -l\n\nAvailable Themes: \n   chesterish\n   grade3\n   gruvboxd\n   gruvboxl\n   monokai\n   oceans16\n   onedork\n   solarizedd\n   solarizedl"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume",
    "section": "",
    "text": "Here is my up-to-date Resume\nI am currently pursuing a Master of Behavioral and Decision Sciences from the University of Pennsylvania, graduating in December 2022. Through my graduate coursework I am doing concentration in consumer analytics.\nAt UPenn, I am taking a challenging coursework including ‚ÄòStatistical Reasoning for Behavioral Science‚Äô, ‚ÄòData Science and Quantitative Modeling‚Äô, ‚ÄòJudgments and Decisions‚Äô, ‚ÄòPublic policy and Public Finance‚Äô. I was also elected as a student representative for GAPSA (Graduate and Professional Student Assembly) where I advocated for graduate student needs, and helped to organize activities that enhance the ties of the school‚Äôs graduate community.\nMy passion lies at the intersection of consumer behavior, marketing, technology, and data analysis. I believe behavioral science can inform marketing decisions to improve the consumer experience, satisfaction, and welfare. In my experience at previous positions, I developed analytical and problem-solving skills, an understanding of quantitative and qualitative methodologies, as well as strong interpersonal skills. While in school, I‚Äôve worked for non-governmental organizations and represented Cyprus in educational programs in Europe and the US.\nI am always seeking new ways to improve marketing and consumer relations using behavioral science and analytics. During school, I‚Äôve gained experience in Python, R, MS office, and SPSS. Looking forward to pursuing my interests and gaining experience in the field of consumer experience and market research."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some words I wrote\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\n\n\nThings to do in Copenhagen\n\n\nJun 9, 2022\n\n\n\n\n\nNo matching items"
  }
]