[
  {
    "objectID": "posts/2022-06-09-denmark/index.html#currency",
    "href": "posts/2022-06-09-denmark/index.html#currency",
    "title": "Things to do in Copenhagen",
    "section": "CURRENCY",
    "text": "CURRENCY\nThe main currency is Danish Krone. Live currency converter. However, they accept card payments everywhere. With Revolut or Venmo you should be fine."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#transportation",
    "href": "posts/2022-06-09-denmark/index.html#transportation",
    "title": "Things to do in Copenhagen",
    "section": "TRANSPORTATION",
    "text": "TRANSPORTATION\n\nThe Metro is convenient and it takes you everywhere you want to go within the city:\n\nRejseplanen app - This is the app for the metro/train. It is designed specifically for them and it works like Google maps. Great navigation tool while you are in Denmark. There will be machines (also a person for support) at the airport where you can buy tickets. Also, Denmark is infamous for cycling with great infrastructure. You can rent bikes as well and ride across the city."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-visits",
    "href": "posts/2022-06-09-denmark/index.html#cool-visits",
    "title": "Things to do in Copenhagen",
    "section": "COOL VISITS",
    "text": "COOL VISITS\nNyhavn (The most iconic part of Copenhagen. Highly recommended. the Closest metro station: Kongens Nytorv station.)\n\n\n\nNyhavn\n\n\n\nSomething you can do is to walk through the shopping center of Copenhagen. You can go to Norreport station and walk towards Nyhavn. It is like a 1 to 1.5 mile (1.6‚Äì2.4km) walk. There are many stores and restaurants in between (along the yellow line on the map). While it is not a straightforward walk it is a pleasant one as you are passing through the shopping area of the city.\n\n\nRosenborg Castle & Botanical Garden (Nice places to walk around. Also, Rosenborg castle is a historic place that now operates as a museum ticket info. Have no opinion for the museum though as I haven‚Äôt been. Closest metro station: Norreport station)\nLittle Mermaid (You will often see it in souvenir shops as it is considered as one of the main attraction points in Copenhagen along Nyhavn. While you might not be impressed by it due to its tiny size, you may want to see it simply for its touristic outreach.) Osterport Station.\nAmalienborg (The equivalent ‚ÄòBuckingham palace‚Äô of Denmark. Cool for walking around and taking pictures with the royal guards. Have no opinion for the museum there though as I haven‚Äôt been. tick info)\nCarlsberg Factory (if you like beer this is a nice place to visit. There is a tour in the factory and some beer tasting in the end. Carlsberg Station.)\nTivoli Gardens/Park (Located in front of the Central Station. The most famous park in Copenhagen. Kobenhavn H station)\nFisketorvet (If you like Malls, this can be an option. Most famous mall in the city.)\nBlack Diamond (The largest library in Denmark. It is a modern building. A Nice place to visit. Christianshavn station.)"
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "href": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "title": "Things to do in Copenhagen",
    "section": "COOL NEIGHBOURHOODS",
    "text": "COOL NEIGHBOURHOODS\n\nFreetown Christiania (It is considered an independent town within Copenhagen where people go there to chillax. It is a renowned marijuana place since Christiania is the only part of Copenhagen where Danish law is not enforced. However, it is a safe area that is worth a visit mainly due to its particularity. Highly recommended.) Metro station: Christianshavn.\nBeautiful Parks (especially for summer walks):\n\n\nFrederiksberg Have (One of the most beautiful parks in Copenhagen) Frederiksberg station.\nSuperkilen Park (Norrebro station). Fun fact, Norrebro is an area where many immigrants stay. Diverse area.\nVestre Kirkegard (near Carlsberg factory) Metro: Carlsberg Station\n\nFOOD & COOL PLACES (min $ /max $$$$)\n\nUnion Kitchen $$ (Cool place for brunch. Heard great things from friends that have been there.)\nSticks‚Äôn‚ÄôSushi $$$ (Great sushi place. Known rooftop restaurant in the city.)\nDalle Valle $ (variety food buffet - alue for money place with plenty of food) Not fast food, but not a high-class restaurant. Almost always lots of people, decent food, a variety of options. Recommended for lunch.\nConditori La Glace $$$ (Traditional Danish pastries, it is the oldest patisserie in Denmark - founded in 1870, good for dessert)\nEspresso House $$ (coffee place, despite its Swedish origin it is considered as the ‚ÄòStarbucks‚Äô of Denmark)\nBastard cafe $$ (a place full of board games. A nice place to go with friends for coffee/snacks/beer and board games.\nTaphouse $$ (Famous bar in the city. If you like beer then this is the place for you. It has one of the largest beer selections in Europe.)\n\nYou can always map your choices and combine visits that are close to each other. Also, since it is Scandinavia, remember to check the weather while preparing your bags! You are visiting arguably the most beautiful city in Scandinavia so try to enjoy every minute of it!\nHave fun and safe travels!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Hi üëã I am Loizos! This is my personal website.\nI have a blog that is infrequently updated üìù\nSee my projects for some things I‚Äôve been working üõ†\nFeel free to get in contact below"
  },
  {
    "objectID": "software/credcardpred.html",
    "href": "software/credcardpred.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "#1. Properly load the data into Jupyter Notebooks\n\nimport matplotlib.pyplot as plt #matplotlib generates graphs.\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC #i.e. Support Vector Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.read_csv(\"crx.data\", header=None)\nprint(data.columns)\n\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')\n\n\nChange the column names to correspond to the ‚Äúreal‚Äù labels from the second link.\n\ndata.rename(columns={1 : \"age\", \n                     2 : \"debt\",\n                     3 : \"married\",\n                     4 : \"bankcustomer\", \n                     5 : \"educationlevel\", \n                     6 : \"ethnicity\",\n                     7 : \"yearsemployed\", \n                     8 : \"priordefault\",\n                     9 : \"employed\",\n                     10 : \"creditscore\",\n                     11 : \"driverslicense\",\n                     12 : \"citizen\",\n                     13 : \"zipcode\",\n                     14 : \"income\",\n                     15 : \"approved\"}, inplace=True)\nprint(data.head())\n\n   0    age   debt married bankcustomer educationlevel ethnicity  \\\n0  b  30.83  0.000       u            g              w         v   \n1  a  58.67  4.460       u            g              q         h   \n2  a  24.50  0.500       u            g              q         h   \n3  b  27.83  1.540       u            g              w         v   \n4  b  20.17  5.625       u            g              w         v   \n\n   yearsemployed priordefault employed  creditscore driverslicense citizen  \\\n0           1.25            t        t            1              f       g   \n1           3.04            t        t            6              f       g   \n2           1.50            t        f            0              f       g   \n3           3.75            t        t            5              t       g   \n4           1.71            t        f            0              f       s   \n\n  zipcode  income approved  \n0   00202       0        +  \n1   00043     560        +  \n2   00280     824        +  \n3   00100       3        +  \n4   00120       0        +  \n\n\nRemove all question marks from value ‚Äòage‚Äô, and convert it to numerical.\n\ndata.loc[83,'age'] = \"\"\ndata.loc[86,'age'] = \"\"\ndata.loc[92,'age'] = \"\"\ndata.loc[97,'age'] = \"\"\ndata.loc[254,'age'] = \"\"\ndata.loc[286,'age'] = \"\"\ndata.loc[329,'age'] = \"\"\ndata.loc[445,'age'] = \"\"\ndata.loc[450,'age'] = \"\"\ndata.loc[500,'age'] = \"\"\ndata.loc[515,'age'] = \"\"\ndata.loc[608,'age'] = \"\"\n\n\n\ndata['age'] = pd.to_numeric(data['age'])\n\n#2. Summarize the data.Frequency tables for categorical variables and histograms for continuous variables.\n\n%matplotlib inline\nz = data.hist(column=['debt', 'age', 'yearsemployed', 'income', 'creditscore'],bins=5, figsize=(12,7))\nprint(z) #is not adding them i.e. it creates a longer list.\n\nsummary_data = data.describe()\nprint(summary_data)\n\n#Categorical variables:\n#             \"married\", \n#             \"bankcustomer\", \n#             'educationlevel', \n#             'ethnicity',  \n#             'priordefault', \n#             'employed', \n#             'driverslicense',\n#             'citizen',\n#             'approved'\n\nmarried = data.loc[:,\"married\"].value_counts()\nprint(married)\n\nbankcustomer = data.loc[:,\"bankcustomer\"].value_counts()\nprint(bankcustomer)\n\neducationlevel = data.loc[:,\"educationlevel\"].value_counts()\nprint(educationlevel)\n\nethnicity = data.loc[:,\"ethnicity\"].value_counts()\nprint(ethnicity)\n\npriordefault = data.loc[:,\"priordefault\"].value_counts()\nprint(priordefault)\n\nemployed = data.loc[:,\"employed\"].value_counts()\nprint(employed)\n\ndriverslicense = data.loc[:,\"driverslicense\"].value_counts()\nprint(driverslicense)\n\napproved = data.loc[:,\"approved\"].value_counts()\nprint(approved)\n\ncitizen = data.loc[:,\"citizen\"].value_counts()\nprint(citizen)\n\n\n[[<AxesSubplot:title={'center':'debt'}>\n  <AxesSubplot:title={'center':'age'}>]\n [<AxesSubplot:title={'center':'yearsemployed'}>\n  <AxesSubplot:title={'center':'income'}>]\n [<AxesSubplot:title={'center':'creditscore'}> <AxesSubplot:>]]\n              age        debt  yearsemployed  creditscore         income\ncount  678.000000  690.000000     690.000000    690.00000     690.000000\nmean    31.568171    4.758725       2.223406      2.40000    1017.385507\nstd     11.957862    4.978163       3.346513      4.86294    5210.102598\nmin     13.750000    0.000000       0.000000      0.00000       0.000000\n25%     22.602500    1.000000       0.165000      0.00000       0.000000\n50%     28.460000    2.750000       1.000000      0.00000       5.000000\n75%     38.230000    7.207500       2.625000      3.00000     395.500000\nmax     80.250000   28.000000      28.500000     67.00000  100000.000000\nu    519\ny    163\n?      6\nl      2\nName: married, dtype: int64\ng     519\np     163\n?       6\ngg      2\nName: bankcustomer, dtype: int64\nc     137\nq      78\nw      64\ni      59\naa     54\nff     53\nk      51\ncc     41\nm      38\nx      38\nd      30\ne      25\nj      10\n?       9\nr       3\nName: educationlevel, dtype: int64\nv     399\nh     138\nbb     59\nff     57\n?       9\nj       8\nz       8\ndd      6\nn       4\no       2\nName: ethnicity, dtype: int64\nt    361\nf    329\nName: priordefault, dtype: int64\nf    395\nt    295\nName: employed, dtype: int64\nf    374\nt    316\nName: driverslicense, dtype: int64\n-    383\n+    307\nName: approved, dtype: int64\ng    625\ns     57\np      8\nName: citizen, dtype: int64\n\n\n\n\n\nOur data is skewed towards the left of the distribution because there are some outliers in our data, such as customers with a high income and credit score relative to the overall dataset. However, we are working with a limited number of data and it is still in the initial stage of development, we can leave it as-is for now. We can try to remove the outliers when we are tuning our model or when we have more data to work with.\n#3. Split the sample into a test set and a train set with 20% of data being in the test dataset. Your random seed should be 808.\n\n#split dataset into train and test, and set the random_state so it maintain the same result\ndata_features = data[['debt', \"age\", \"married\",\"bankcustomer\", \"educationlevel\", \"ethnicity\", \"yearsemployed\", \"priordefault\",\"employed\",\"creditscore\",\n\"driverslicense\",\"citizen\",\"income\"]]\ndata_target = data['approved']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data_features,\n                                                    data_target,\n                                                    test_size = 0.2,\n                                                    random_state = 808)\n\n#4. Try the following algorithms and choose the one that generates the best accuracy:\n\n#First, we convert categorical variables to numerical.\ndata1 = pd.get_dummies(data).dropna()\nprint(data1)\n\n\nprint(list(data1.columns))\n\n       age    debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n0    30.83   0.000           1.25            1       0    0    0    1   \n1    58.67   4.460           3.04            6     560    0    1    0   \n2    24.50   0.500           1.50            0     824    0    1    0   \n3    27.83   1.540           3.75            5       3    0    0    1   \n4    20.17   5.625           1.71            0       0    0    0    1   \n..     ...     ...            ...          ...     ...  ...  ...  ...   \n685  21.08  10.085           1.25            0       0    0    0    1   \n686  22.67   0.750           2.00            2     394    0    1    0   \n687  25.25  13.500           2.00            1       1    0    1    0   \n688  17.92   0.205           0.04            0     750    0    0    1   \n689  35.00   3.375           8.29            0       0    0    0    1   \n\n     married_?  married_l  ...  zipcode_00720  zipcode_00760  zipcode_00840  \\\n0            0          0  ...              0              0              0   \n1            0          0  ...              0              0              0   \n2            0          0  ...              0              0              0   \n3            0          0  ...              0              0              0   \n4            0          0  ...              0              0              0   \n..         ...        ...  ...            ...            ...            ...   \n685          0          0  ...              0              0              0   \n686          0          0  ...              0              0              0   \n687          0          0  ...              0              0              0   \n688          0          0  ...              0              0              0   \n689          0          0  ...              0              0              0   \n\n     zipcode_00928  zipcode_00980  zipcode_01160  zipcode_02000  zipcode_?  \\\n0                0              0              0              0          0   \n1                0              0              0              0          0   \n2                0              0              0              0          0   \n3                0              0              0              0          0   \n4                0              0              0              0          0   \n..             ...            ...            ...            ...        ...   \n685              0              0              0              0          0   \n686              0              0              0              0          0   \n687              0              0              0              0          0   \n688              0              0              0              0          0   \n689              0              0              0              0          0   \n\n     approved_+  approved_-  \n0             1           0  \n1             1           0  \n2             1           0  \n3             1           0  \n4             1           0  \n..          ...         ...  \n685           0           1  \n686           0           1  \n687           0           1  \n688           0           1  \n689           0           1  \n\n[678 rows x 223 columns]\n['age', 'debt', 'yearsemployed', 'creditscore', 'income', '0_?', '0_a', '0_b', 'married_?', 'married_l', 'married_u', 'married_y', 'bankcustomer_?', 'bankcustomer_g', 'bankcustomer_gg', 'bankcustomer_p', 'educationlevel_?', 'educationlevel_aa', 'educationlevel_c', 'educationlevel_cc', 'educationlevel_d', 'educationlevel_e', 'educationlevel_ff', 'educationlevel_i', 'educationlevel_j', 'educationlevel_k', 'educationlevel_m', 'educationlevel_q', 'educationlevel_r', 'educationlevel_w', 'educationlevel_x', 'ethnicity_?', 'ethnicity_bb', 'ethnicity_dd', 'ethnicity_ff', 'ethnicity_h', 'ethnicity_j', 'ethnicity_n', 'ethnicity_o', 'ethnicity_v', 'ethnicity_z', 'priordefault_f', 'priordefault_t', 'employed_f', 'employed_t', 'driverslicense_f', 'driverslicense_t', 'citizen_g', 'citizen_p', 'citizen_s', 'zipcode_00000', 'zipcode_00017', 'zipcode_00020', 'zipcode_00021', 'zipcode_00022', 'zipcode_00024', 'zipcode_00028', 'zipcode_00029', 'zipcode_00030', 'zipcode_00032', 'zipcode_00040', 'zipcode_00043', 'zipcode_00045', 'zipcode_00049', 'zipcode_00050', 'zipcode_00052', 'zipcode_00056', 'zipcode_00060', 'zipcode_00062', 'zipcode_00070', 'zipcode_00073', 'zipcode_00075', 'zipcode_00076', 'zipcode_00080', 'zipcode_00086', 'zipcode_00088', 'zipcode_00092', 'zipcode_00093', 'zipcode_00094', 'zipcode_00096', 'zipcode_00099', 'zipcode_00100', 'zipcode_00102', 'zipcode_00108', 'zipcode_00110', 'zipcode_00112', 'zipcode_00117', 'zipcode_00120', 'zipcode_00121', 'zipcode_00128', 'zipcode_00129', 'zipcode_00130', 'zipcode_00132', 'zipcode_00136', 'zipcode_00140', 'zipcode_00141', 'zipcode_00144', 'zipcode_00145', 'zipcode_00150', 'zipcode_00152', 'zipcode_00154', 'zipcode_00156', 'zipcode_00160', 'zipcode_00163', 'zipcode_00164', 'zipcode_00167', 'zipcode_00168', 'zipcode_00170', 'zipcode_00171', 'zipcode_00174', 'zipcode_00176', 'zipcode_00178', 'zipcode_00180', 'zipcode_00181', 'zipcode_00186', 'zipcode_00188', 'zipcode_00195', 'zipcode_00200', 'zipcode_00202', 'zipcode_00204', 'zipcode_00208', 'zipcode_00210', 'zipcode_00211', 'zipcode_00212', 'zipcode_00216', 'zipcode_00220', 'zipcode_00221', 'zipcode_00224', 'zipcode_00225', 'zipcode_00228', 'zipcode_00230', 'zipcode_00231', 'zipcode_00232', 'zipcode_00239', 'zipcode_00240', 'zipcode_00250', 'zipcode_00252', 'zipcode_00253', 'zipcode_00254', 'zipcode_00256', 'zipcode_00260', 'zipcode_00263', 'zipcode_00268', 'zipcode_00272', 'zipcode_00274', 'zipcode_00276', 'zipcode_00280', 'zipcode_00288', 'zipcode_00290', 'zipcode_00292', 'zipcode_00300', 'zipcode_00303', 'zipcode_00309', 'zipcode_00311', 'zipcode_00312', 'zipcode_00320', 'zipcode_00329', 'zipcode_00330', 'zipcode_00333', 'zipcode_00340', 'zipcode_00348', 'zipcode_00349', 'zipcode_00350', 'zipcode_00352', 'zipcode_00356', 'zipcode_00360', 'zipcode_00368', 'zipcode_00369', 'zipcode_00370', 'zipcode_00371', 'zipcode_00372', 'zipcode_00375', 'zipcode_00380', 'zipcode_00381', 'zipcode_00383', 'zipcode_00393', 'zipcode_00395', 'zipcode_00396', 'zipcode_00399', 'zipcode_00400', 'zipcode_00408', 'zipcode_00410', 'zipcode_00411', 'zipcode_00416', 'zipcode_00420', 'zipcode_00422', 'zipcode_00431', 'zipcode_00432', 'zipcode_00434', 'zipcode_00440', 'zipcode_00443', 'zipcode_00450', 'zipcode_00454', 'zipcode_00455', 'zipcode_00460', 'zipcode_00465', 'zipcode_00470', 'zipcode_00480', 'zipcode_00487', 'zipcode_00491', 'zipcode_00500', 'zipcode_00510', 'zipcode_00515', 'zipcode_00519', 'zipcode_00520', 'zipcode_00523', 'zipcode_00550', 'zipcode_00560', 'zipcode_00583', 'zipcode_00600', 'zipcode_00640', 'zipcode_00680', 'zipcode_00711', 'zipcode_00720', 'zipcode_00760', 'zipcode_00840', 'zipcode_00928', 'zipcode_00980', 'zipcode_01160', 'zipcode_02000', 'zipcode_?', 'approved_+', 'approved_-']\n\n\nAs before, we split the sample into a test set and a train set with 20% of data being in the test dataset. Your random seed should be 808. This needs to be done after the ‚Äúget_dummies()‚Äù command for decision tree to work.\n\ndata_features = data1.loc[:,\"age\":\"citizen_s\"]\ndata_target = data1['approved_+']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data_features,\n                                                    data_target,\n                                                    test_size = 0.2,\n                                                    random_state = 808)\n\n\nprint(x_train, x_test, y_train, y_test)\n\n       age    debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n27   56.58  18.500         15.000           17       0    0    0    1   \n100  37.50   1.750          0.250            0     400    0    0    1   \n132  47.42   8.000          6.500            6   51100    0    1    0   \n404  34.00   5.085          1.085            0       0    0    0    1   \n401  28.92   0.375          0.290            0     140    0    0    1   \n..     ...     ...            ...          ...     ...  ...  ...  ...   \n480  16.92   0.500          0.165            6      35    0    1    0   \n384  22.08  11.460          1.585            0    1212    0    0    1   \n300  57.58   2.000          6.500            1      10    0    1    0   \n249  21.83  11.000          0.290            6       0    0    0    1   \n471  21.08   4.125          0.040            0     100    0    0    1   \n\n     married_?  married_l  ...  ethnicity_z  priordefault_f  priordefault_t  \\\n27           0          0  ...            0               0               1   \n100          0          0  ...            0               0               1   \n132          0          0  ...            0               0               1   \n404          0          0  ...            0               1               0   \n401          0          0  ...            0               1               0   \n..         ...        ...  ...          ...             ...             ...   \n480          0          0  ...            0               1               0   \n384          0          0  ...            0               1               0   \n300          0          0  ...            0               1               0   \n249          0          0  ...            0               0               1   \n471          0          0  ...            0               1               0   \n\n     employed_f  employed_t  driverslicense_f  driverslicense_t  citizen_g  \\\n27            0           1                 0                 1          1   \n100           1           0                 0                 1          1   \n132           0           1                 1                 0          1   \n404           1           0                 0                 1          1   \n401           1           0                 1                 0          1   \n..          ...         ...               ...               ...        ...   \n480           0           1                 0                 1          1   \n384           1           0                 0                 1          1   \n300           0           1                 1                 0          1   \n249           0           1                 1                 0          1   \n471           1           0                 1                 0          1   \n\n     citizen_p  citizen_s  \n27           0          0  \n100          0          0  \n132          0          0  \n404          0          0  \n401          0          0  \n..         ...        ...  \n480          0          0  \n384          0          0  \n300          0          0  \n249          0          0  \n471          0          0  \n\n[542 rows x 50 columns]        age   debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n207  28.67  9.335          5.665            6     168    0    0    1   \n406  40.33  8.125          0.165            2      18    0    1    0   \n231  47.42  3.000         13.875            2    1704    0    1    0   \n452  36.50  4.250          3.500            0      50    0    0    1   \n567  25.17  2.875          0.875            0       0    0    1    0   \n..     ...    ...            ...          ...     ...  ...  ...  ...   \n457  29.67  0.750          0.040            0       0    0    0    1   \n544  30.08  1.040          0.500           10      28    0    0    1   \n145  32.83  2.500          2.750            6    2072    0    0    1   \n342  26.92  2.250          0.500            0    4000    0    0    1   \n323  48.58  0.205          0.250           11    2732    0    0    1   \n\n     married_?  married_l  ...  ethnicity_z  priordefault_f  priordefault_t  \\\n207          0          0  ...            0               0               1   \n406          0          0  ...            0               1               0   \n231          0          0  ...            0               0               1   \n452          0          0  ...            0               1               0   \n567          0          0  ...            0               0               1   \n..         ...        ...  ...          ...             ...             ...   \n457          0          0  ...            0               1               0   \n544          0          0  ...            0               0               1   \n145          0          0  ...            0               0               1   \n342          0          0  ...            0               1               0   \n323          0          0  ...            0               0               1   \n\n     employed_f  employed_t  driverslicense_f  driverslicense_t  citizen_g  \\\n207           0           1                 1                 0          1   \n406           0           1                 1                 0          1   \n231           0           1                 0                 1          1   \n452           1           0                 1                 0          1   \n567           1           0                 1                 0          1   \n..          ...         ...               ...               ...        ...   \n457           1           0                 1                 0          1   \n544           0           1                 0                 1          1   \n145           0           1                 1                 0          1   \n342           1           0                 0                 1          1   \n323           0           1                 1                 0          1   \n\n     citizen_p  citizen_s  \n207          0          0  \n406          0          0  \n231          0          0  \n452          0          0  \n567          0          0  \n..         ...        ...  \n457          0          0  \n544          0          0  \n145          0          0  \n342          0          0  \n323          0          0  \n\n[136 rows x 50 columns] 27     1\n100    0\n132    1\n404    0\n401    0\n      ..\n480    0\n384    0\n300    0\n249    1\n471    0\nName: approved_+, Length: 542, dtype: uint8 207    1\n406    0\n231    1\n452    0\n567    1\n      ..\n457    0\n544    0\n145    1\n342    0\n323    1\nName: approved_+, Length: 136, dtype: uint8\n\n\n\ndata_features.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 678 entries, 0 to 689\nData columns (total 50 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   age                678 non-null    float64\n 1   debt               678 non-null    float64\n 2   yearsemployed      678 non-null    float64\n 3   creditscore        678 non-null    int64  \n 4   income             678 non-null    int64  \n 5   0_?                678 non-null    uint8  \n 6   0_a                678 non-null    uint8  \n 7   0_b                678 non-null    uint8  \n 8   married_?          678 non-null    uint8  \n 9   married_l          678 non-null    uint8  \n 10  married_u          678 non-null    uint8  \n 11  married_y          678 non-null    uint8  \n 12  bankcustomer_?     678 non-null    uint8  \n 13  bankcustomer_g     678 non-null    uint8  \n 14  bankcustomer_gg    678 non-null    uint8  \n 15  bankcustomer_p     678 non-null    uint8  \n 16  educationlevel_?   678 non-null    uint8  \n 17  educationlevel_aa  678 non-null    uint8  \n 18  educationlevel_c   678 non-null    uint8  \n 19  educationlevel_cc  678 non-null    uint8  \n 20  educationlevel_d   678 non-null    uint8  \n 21  educationlevel_e   678 non-null    uint8  \n 22  educationlevel_ff  678 non-null    uint8  \n 23  educationlevel_i   678 non-null    uint8  \n 24  educationlevel_j   678 non-null    uint8  \n 25  educationlevel_k   678 non-null    uint8  \n 26  educationlevel_m   678 non-null    uint8  \n 27  educationlevel_q   678 non-null    uint8  \n 28  educationlevel_r   678 non-null    uint8  \n 29  educationlevel_w   678 non-null    uint8  \n 30  educationlevel_x   678 non-null    uint8  \n 31  ethnicity_?        678 non-null    uint8  \n 32  ethnicity_bb       678 non-null    uint8  \n 33  ethnicity_dd       678 non-null    uint8  \n 34  ethnicity_ff       678 non-null    uint8  \n 35  ethnicity_h        678 non-null    uint8  \n 36  ethnicity_j        678 non-null    uint8  \n 37  ethnicity_n        678 non-null    uint8  \n 38  ethnicity_o        678 non-null    uint8  \n 39  ethnicity_v        678 non-null    uint8  \n 40  ethnicity_z        678 non-null    uint8  \n 41  priordefault_f     678 non-null    uint8  \n 42  priordefault_t     678 non-null    uint8  \n 43  employed_f         678 non-null    uint8  \n 44  employed_t         678 non-null    uint8  \n 45  driverslicense_f   678 non-null    uint8  \n 46  driverslicense_t   678 non-null    uint8  \n 47  citizen_g          678 non-null    uint8  \n 48  citizen_p          678 non-null    uint8  \n 49  citizen_s          678 non-null    uint8  \ndtypes: float64(3), int64(2), uint8(45)\nmemory usage: 61.6 KB\n\n\n\ndata1.head()\n\n\n\n\n  \n    \n      \n      age\n      debt\n      yearsemployed\n      creditscore\n      income\n      0_?\n      0_a\n      0_b\n      married_?\n      married_l\n      ...\n      zipcode_00720\n      zipcode_00760\n      zipcode_00840\n      zipcode_00928\n      zipcode_00980\n      zipcode_01160\n      zipcode_02000\n      zipcode_?\n      approved_+\n      approved_-\n    \n  \n  \n    \n      0\n      30.83\n      0.000\n      1.25\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      58.67\n      4.460\n      3.04\n      6\n      560\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      24.50\n      0.500\n      1.50\n      0\n      824\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      3\n      27.83\n      1.540\n      3.75\n      5\n      3\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      4\n      20.17\n      5.625\n      1.71\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n5 rows √ó 223 columns\n\n\n\n#a. Decision Trees\n\nfrom sklearn.tree import DecisionTreeClassifier\ninit_model = DecisionTreeClassifier ()\nfitted_model = init_model.fit(x_train ,y_train)\ntest_predictions = fitted_model.predict(x_test)\naccuracy_score = fitted_model.score(x_test,y_test)\nprint(accuracy_score)\n\n#overall how many of the predictions are correct\n\n\n#overall out of those defaults how many of those 0.17 are correct and not correct?\n\n0.8455882352941176\n\n\n#b. Logistic Regression\n\n#Logistic regression\nmodel_lr = LogisticRegression()\nfitted_model_lr = model_lr.fit(x_train, y_train)\ntest_predictions_lr = fitted_model_lr.predict(x_test)\naccuracy_lr = fitted_model_lr.score(x_test,y_test)\nprint(fitted_model_lr.coef_)\nprint(accuracy_lr)\n\n#overall how many of the predictions are correct\n\n[[ 4.84069824e-03 -2.46678805e-02  1.20267584e-01  1.64279116e-01\n   5.13339863e-04 -7.02796069e-03 -1.76746656e-01 -2.76489739e-02\n   1.08863388e-01  3.10703871e-02 -8.26650419e-02 -2.68692324e-01\n   1.08863388e-01 -8.26650419e-02  3.10703871e-02 -2.68692324e-01\n   1.08863388e-01 -9.05194758e-02 -9.94068100e-02  1.87520056e-01\n  -2.93554456e-02  3.49157435e-02 -3.61569082e-01 -2.52229191e-01\n  -9.00501443e-04 -2.10370142e-01 -8.76721486e-02  1.76003560e-01\n  -5.84801526e-03  1.76604192e-01  2.42540281e-01  1.08863388e-01\n  -1.29883594e-01 -2.29978369e-02 -3.51268838e-01  1.80746065e-01\n   5.79890564e-02  1.89933137e-02 -1.73749301e-08 -9.58931264e-02\n   2.20279985e-02 -1.72796178e+00  1.51653819e+00 -3.04359173e-01\n   9.29355818e-02 -1.62785055e-03 -2.09795740e-01 -2.90778257e-01\n   9.97707085e-02 -2.04160423e-02]]\n0.8602941176470589\n\n\n/Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n#c.¬†Bagging\n\n#bagging, need to explore the concept and what each parameter does\nfrom sklearn.ensemble import BaggingClassifier\n\n#n_estimators is a number of randomly sampled datasets, similar to cv\nmodel_bag = BaggingClassifier(\n    base_estimator = tree.DecisionTreeClassifier(),\n    n_estimators = 400,\n    max_samples = 0.8,\n    oob_score = True,\n    random_state = 808)\n\nfitted_model_bag = model_bag.fit(x_train,y_train)\ntest_predictions_bag = fitted_model_bag.predict(x_test)\naccuracy_bag = fitted_model_bag.score(x_test,y_test)#validation accuracy, because I want to see if the model works generalize in new data\nprint(accuracy_bag)\nprint(model_bag.oob_score_)\n\n0.8823529411764706\n0.8616236162361623\n\n\n#d.¬†Boosting\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\nmodel_boost = GradientBoostingClassifier()\n\nfitted_model_boost = model_boost.fit(x_train,y_train)\ntest_predictions_boost = fitted_model_boost.predict(x_test)\naccuracy_boost = fitted_model_boost.score(x_test,y_test)\nprint(accuracy_boost)\nprint(model_boost)\n\n0.8529411764705882\nGradientBoostingClassifier()\n\n\n#e. Random Forest\n\nrandomforest = RandomForestClassifier (random_state = 808)\nmodel_randforest = RandomForestClassifier()\n\nfitted_model_randforest = model_randforest.fit(x_train,y_train)\ntest_predictions_randforest = fitted_model_randforest.predict(x_test)\naccuracy_randforest = fitted_model_randforest.score(x_test,y_test)\nprint(accuracy_randforest)\n\n0.8382352941176471\n\n\n#f.¬†SVM\n\nfrom sklearn.svm import SVC #i.e. Support Vector Classifier\nsvm=svm.SVC(random_state = 808)\n\nprint(svm.fit(x_train, y_train).score(x_test,y_test))\n\n0.6985294117647058\n\n\n#g. Passive Aggressive Classifier (Links to an external site.)\n\npa=PassiveAggressiveClassifier(random_state = 808)\n\nprint(pa.fit(x_train, y_train).score(x_test,y_test))\n\n0.7573529411764706\n\n\n#h. Radius Neighbors Classifier (Links to an external site.)\n\nmodel_rn = RadiusNeighborsClassifier(radius=11700)\nfitted_model_rn = model_rn.fit(x_train,y_train)\ntest_predictions_rn = fitted_model_rn.predict(x_test)\naccuracy_rn = fitted_model_rn.score(x_test,y_test)\nprint(accuracy_rn)\n\n0.6470588235294118\n\n\n#5. After completing Step 4, explain: # Which algorithm you recommend? What accuracy it has? Why you measured accuracy the way you did? The accuracy of the algorithms above indicate the percentage of predictions that are actually correct. I would reccommend the bagging algorithm because it has the highest accuracy percentage (88,23%).\nThe confusion matrix below indicates that Bagging is the most suitable approach in reaching to the accuracy of the algorithms. In particular, the confusion matrix above shows us that the Bagging models have a higher number of true positives and true negatives than rest accuracy models. In this example above, we can see that bagging is compared with logistic regression and random forest classifiers, the 2nd and the 3rd highest accuracy models.\n\nfrom sklearn.metrics import confusion_matrix\n\nprint('Confusion matrix: Bagging Classifiers')\nprint(confusion_matrix(y_test, test_predictions_bag))\n\nprint('Confusion matrix: Logistic Regression')\nprint(confusion_matrix(y_test, test_predictions_lr))\n\nprint('Confusion matrix: Random Forest Classifiers')\nprint(confusion_matrix(y_test, test_predictions_randforest))\n\nConfusion matrix: Bagging Classifiers\n[[75 12]\n [ 4 45]]\nConfusion matrix: Logistic Regression\n[[70 17]\n [ 2 47]]\nConfusion matrix: Random Forest Classifiers\n[[70 17]\n [ 5 44]]\n\n\n\n6. Brief overview of the last two classifiers (Passive Agressive & Radius Neighbors).\nPassive Agressive classifier is an online learning predictor best suited for systems that receive data in a continuous stream. The calculation passively corrects for the classifications and penalizes ‚Äòaggressive‚Äô for any miscalculation. While the model can perfectly predict all data, it will not change the algorithm; hence it is called passive. The term aggressive referst to the fact that when the model fails to predict the outcome variable just in the slightest, it will change the algorithm to compensate for the failed prediction for every set of a new sample of data.\nRadius Neighbors classifer is similar to the KNN (k-nearest-neighbours) concept and makes predictions based on the data within a radius. Instead of locating the k-neighbors, the Radius Neighbors Classifier locates all examples in the dataset that are within a given radius of the new example. The radius neighbors are then used to make a prediction for the new example. The radius is defined in the feature space and generally assumes that the input variables are numeric and scaled to the range 0-1, e.g.¬†normalized. The radius-based approach to locating neighbors is appropriate for those datasets where it is desirable for the contribution of neighbors to be proportional to the density of examples in the feature space."
  },
  {
    "objectID": "software/titanic.html",
    "href": "software/titanic.html",
    "title": "Titanic Dataset",
    "section": "",
    "text": "Let's work with the titanic dataset.\n1. Construct a table showing the distribution of passengers by class and survival.\n\ntitanic %$% table(survived, pclass)\n\n        pclass\nsurvived   1   2   3\n       0 123 158 528\n       1 200 119 181\n\n# magrittr does this --> table(titanic$survived, titanic$pclass)\n\n2. Construct a logistic regression model that links survival to the passenger class. Write out the equation first without Running it in R. HINT: Class is a factor variable\n\\[\nlog(odds(y)) = Œ≤0 + Œ≤1*pclass2 + Œ≤2*plcass3\n\\]\n\\[\nodds(y) = e^{Œ≤_0} * e^{Œ≤_1 * pclass2} * e^{Œ≤_2 * pclass3}\n\\]\nŒ≤0 = intercept\ny = survival\n3. Using hand-calculations, determine the coefficients in the model and interpret them (HINT: all you need to do is to use the table, calculate odds for the default category and the odds-ratios for the other categories versus the default)\ne.g.¬†not survival is the default exercise (i.e.¬†=0). prob of survival of:\nfirst class => 200/(200+123) = 62% of people survived\nsecond class => 119/(119+158) = 43% of people survived\nthird class => 181/(181+528) = 26% of people survived\nProbabilities vary between zero and one. Instead, for the purpose of the logistical regression we can calculate odds of survival of:\nfirst class =>200/123=1.62 => for each first class person who died, 1.62 first class people survived.\nsecond class =>119/158 = 0.75 => for each second class person who died, 0.75 second class person survived.\nthird class =>181/528 = 0.34 => for each third class person who died, 0.34 third class person survived.\nWhen we take the log of odds, the result varies from - infinity to + infinity. log of survival of:\nfirst class people: log(200/123)=0.486\nsecond class people: log(119/158)=-0.28\nthird class people: log(181/528)=-1.07\n4. Now Run the model in R. Confirm that you got the same results as in part c). Interpret the results and talk about significance (both statistical and substantive).\n\ntitanic %$% summary(glm(survived ~ factor(pclass), family = \"binomial\"))\n\n\nCall:\nglm(formula = survived ~ factor(pclass), family = \"binomial\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3896  -0.7678  -0.7678   0.9791   1.6525  \n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       0.4861     0.1146   4.242 2.21e-05 ***\nfactor(pclass)2  -0.7696     0.1669  -4.611 4.02e-06 ***\nfactor(pclass)3  -1.5567     0.1433 -10.860  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1741.0  on 1308  degrees of freedom\nResidual deviance: 1613.3  on 1306  degrees of freedom\nAIC: 1619.3\n\nNumber of Fisher Scoring iterations: 4\n\nquestion2 <- titanic %$% glm(survived ~ factor(pclass), family = \"binomial\")\nexp(question2$coefficients)\n\n    (Intercept) factor(pclass)2 factor(pclass)3 \n      1.6260163       0.4631962       0.2108239 \n\n\nThis code leaves us with the following equation of log odds:\nSurvived = 0.486 -0.77class2 -1.56class3\nThe significant p-values are telling us that the intercept is not zero. It means that the beta one (Œ≤1) is different from zero. This implies that there is a difference between classes and survival. We do not know yet what is the magnitude of the difference because this is a difference in log odds.\nSince is negative it means that classes 2 and 3 have lower survival chances from the first class. The 3rd class has the lowest chances of survival since its Œ≤1 coefficient has the highest negative number.\n5. What's the probability of survival for each class of passengers?\nFirst class had the odds of survival of 1.62. We can calculate the probability of survival as (1.62)/(1 + 1.62) = 0.6183206 or about 61.8%.\nSecond class had the odds of survival of 0.75. We can calculate the probability of survival as (0.75)/(1 + 0.75) = 0.4285714 or about 42.8%\nThird class had the odds of survival of 0.34. We can calculate the probability of survival as (0.34)/(1 + 0.34) = 0.2537313 or about 25.3%\n6. Construct a model that interacts class of passenger and his/her gender. Interpret the results the same way you did before.\nFirst we conduct the model just like as before but this time we add gender (aka ‚Äòsex‚Äô) as a factor variable. This time though we either add or multiply the one variable with the other. This model would help us make predictions for a passenger.\nBecause here the model assumes interaction between class and gender we construct the model with a ‚Äò*‚Äô sign. If we add a ‚Äò*‚Äô sign we assume that there are interactions; meaning that the effect of class depends on gender OR the effect of gender depends on class.\n‚ÄîThe alternative was the ‚Äò+‚Äô sign. If we add a ‚Äò+‚Äô sign we assume that there are no interactions; meaning that the effect of class does not depend on gender. This was our assumption when we were doing linear models.‚Äî\n\nsummary(glm(survived~factor(pclass) * factor(sex), family = \"binomial\", data = titanic))\n\n\nCall:\nglm(formula = survived ~ factor(pclass) * factor(sex), family = \"binomial\", \n    data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5924  -0.5745  -0.5745   0.4902   1.9610  \n\nCoefficients:\n                                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                       3.3250     0.4549   7.309 2.68e-13 ***\nfactor(pclass)2                  -1.2666     0.5485  -2.309   0.0209 *  \nfactor(pclass)3                  -3.3621     0.4748  -7.081 1.43e-12 ***\nfactor(sex)male                  -3.9848     0.4815  -8.277  < 2e-16 ***\nfactor(pclass)2:factor(sex)male   0.1617     0.6104   0.265   0.7911    \nfactor(pclass)3:factor(sex)male   2.3039     0.5158   4.467 7.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1741  on 1308  degrees of freedom\nResidual deviance: 1210  on 1303  degrees of freedom\nAIC: 1222\n\nNumber of Fisher Scoring iterations: 5\n\n\nLet‚Äôs make an assumption for a male sitting in first class.\nlog odds of survival => 3.325 - 3.9848 = -0.6598\nodds of survival => exp(-0.6598) = 0.5169547\nprobability of survival => 0.5169547/(1+0.5169547) = 0.3407845 or 34%\nLet‚Äôs make an assumption for a male sitting in second class.\nlog odds of survival => 3.325 -1.2666 - 3.9848+0.1617 = -1.7647\nodds of survival => exp(-1.7647) = 0.1712382\nprobability of survival => 0.1712382/(1+0.1712382) = 0.1462027 or 14.6%\nLet‚Äôs make an assumption for a male sitting in third class.\nlog odds of survival => 3.325 -3.3621 - 3.9848 +2.3039 = -1.718\nodds of survival => exp(-1.718) = 0.1794246\nprobability of survival => 0.1794246/(1+0.1794246) = 0.1521289 or 15.21%\nLet‚Äôs make an assumption for a female sitting in first class.\nlog odds of survival => 3.325\nodds of survival => exp(3.325) = 27.799\nprobability of survival => 27.799/(1+27.799) = 0.9652766 or 96.5%\nLet‚Äôs make an assumption for a female sitting in second class.\nlog odds of survival => 3.325 -1.2666 = 2.0584\nodds of survival => exp(2.0584) = 7.833426\nprobability of survival => 7.833426/(1+7.833426) = 0.8867936 or 88.6%\nLet‚Äôs make an assumption for a female sitting in third class.\nlog odds of survival => 3.325 -3.3621 = -0.0371\nodds of survival => exp(-0.0371) = 0.9635798\nprobability of survival => 0.9635798/(1+0.9635798) = 0.4907261 or 49%\nInterpretation of results\nIt looks like the closer someone is in the first class, the better the chance to survive. However, there is an interesting result between men of third and second category, where the men in third category had slightly higher chances of survival than the men in second class. In general, females had higher chances of survival. Even the females in the third class had 15% higher chance of survival than the males in the first class (49% vs 34%). Around 9 out of 10 women survived in classes 1 and 2. One out of two women survived out of the third class. Only one out of three men in the first class survived while in classes 2 and 3 men had the lowest chances of survival (14.6% and 15.2%). This slight difference though can also be due to random noise.\nHow we arrived to those results:\nAfter running the logistical regression model we come up with the intercept and coefficient number odds which where in log form; hence we had to convert those in actual odds. Before converting them to odds, we sum up the related variables that we want to find in order to also have the log odds of survival (e.g.¬†for female in second class we used the intercept 3.325 and the pclass2 value -1.2666).\nThen, we exponentiate the log odds of survival to find the actual odds of survival (e.g.¬†female in second class = 7.83 ‚Äî which means each female who died, 7.83 survived).\nBecause human brain is not well designed enough to perceive odds as a measurement, we converted those to probability using the formula:\n\\[\nprobability = odds/1+odds\n\\]\nWe did that for each one of the six outcomes\n\nmale 1st class = 34%\nmale 2nd class = 14.6%\nmale 3rd class = 15.2%\nfemale 1st class = 96.5%\nfemale 2nd class = 88.6%\nfemale 3rd class = 49%"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some things I made ü™Ñ\n\nRStudio projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      Google Trends\n      Top 3 American Animated Sitcoms\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Class Die Experiment\n      Correct guesses Vs cheats ~ summary statistics\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Titanic Dataset\n      Probabilities of surviving in terms of class and gender\n   \n  \n\nPython projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      State of The Union Addresses\n      US presidents speech, keyword similarity analysis\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Credit Card Application\n      Predicting a  customer's credit card application outcome\n   \n  \n\n\nNo matching items"
  },
  {
    "objectID": "software/expoftruth.html",
    "href": "software/expoftruth.html",
    "title": "Class Die Experiment",
    "section": "",
    "text": "library(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.7\n‚úì tidyr   1.1.4     ‚úì stringr 1.4.0\n‚úì readr   2.0.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(descr)\nlibrary(infer)\n\nHere we will have to work with the experiment data we collected for this class.\n\nWe are interested in whether some people were more likely to make correct guesses. What are the summary statistics for all draws? Plot a histogram. Are there any outliers in the data (i.e. individuals who guess 0 times correctly and individuals who guessed all 20 times correctly?)\nwe draw a histogram based on the summary statistics. To find\n\nlibrary(readxl)\nexp21 <- read_excel(\"experiment_2021C(1)copy.xlsx\", col_names = FALSE,\n    skip = 2)\n\nNew names:\n* `` -> ...1\n* `` -> ...2\n* `` -> ...3\n* `` -> ...4\n* `` -> ...5\n* ...\n\n\nI find all the outcome columns and I name them after the variable names of our dataset.\n\nvar_names <- read_excel(\"experiment_2021C(1)copy.xlsx\", n_max = 1) \n\nNew names:\n* `Q171_First Click` -> `Q171_First Click...46`\n* `Q171_Last Click` -> `Q171_Last Click...47`\n* `Q171_Page Submit` -> `Q171_Page Submit...48`\n* `Q171_Click Count` -> `Q171_Click Count...49`\n* `Q171_First Click` -> `Q171_First Click...82`\n* ...\n\ncolnames(exp21) <- colnames(var_names)\n\nrm(var_names) \n\ncolumnss <- exp21 %>% \n  select(starts_with(\"Outcome\"), \"Age\", \"Risk\", \"Gender\", \"SC0\") \n\nNow we pivot longer to have all the outcomes in line. This helps to name the correct answers as ‚Äú1‚Äù and the wrong answers as ‚Äú0‚Äù.\n\ncountcol <- columnss %>% \n  pivot_longer(cols = starts_with(\"Outcome\"), names_to= \"draw\", values_to = \"answer\") %>%\n  mutate(count= case_when(answer == \"Yes (bonus payment of $0.10)\" ~ 1,\n         answer == \"No (no bonus payment)\" ~ 0))\n\nAfter we pivot wider back in its first format. This helps us to have the data\n\npilpi <- countcol %>% select(-answer) %>% mutate(id = rep(1:133, each = 20)) %>%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender), names_from = draw, values_from = count)\n\nWe use pivot longer once more in order to have all the observations in one column. Having the observations in one column, is easier for us to group by the id column we created before. So we groupby id. This helps us identify what each person done. In addition, we add a column using mutate in order to have the clear result of each person.\n\npi <- pilpi %>% pivot_longer(cols = starts_with(\"Outcome\"), names_to= \"draw\", values_to = \"answer\") %>% group_by(id) \n\nti<- pi %>% mutate(summm=sum(answer))\n\ntii<- pi %>% mutate(summm=sum(answer)) %>% separate(draw, into = c(\"rolsep\", \"rolnum\"), sep = \"e\") %>% group_by(id)\n\n# boxplot.stats(data$score)$out\n# \n# rr <- ti %>% separate(draw, into = c(\"rolsep\", \"rolnum\"), sep = \"e\") %>% group_by(id, rolnum)\n\nNow we pivot wider once more so the data can be presented in the form that shows how people are doing from the 1st one until the last (133rd).\n\nfinal <- ti %>%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender, summm), names_from = draw, values_from = answer)\n\n# finally <- tii %>%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender, summm), names_from = rolnum, values_from = answer)\n# \n#   fin <- final %>% inner_join(ex22, by = \"id\")\n\nWe use the code below to do our Histogram.\n\nggplot(final, aes(summm)) + geom_histogram(bins = 21) + geom_rug()\n\n\n\n\nAnalyze the data by gender, the draw number, and by experimental condition. Are there differences between the genders in terms of the number of wins versus losses? Are there differences between the experimental conditions? Are people more or less likely to win towards the last draws as opposed to the beginning of the experiment?\n‚ÄòFinal‚Äô is the column that already has everything needed for the analysis. The only missing column is conditions. To do that, I used mutate to create a new column named as condition where each real condition is named after a number from one to five. I also create an id column in the ex2 dataset so I can then inner_join it with our main column.\n\nex2 <- exp21 %>%\n         mutate(condition = case_when(Control_Msg != \"\" ~ 1,\n                               Norm_Pos_Msg != \"\" ~ 2,\n                               Norm_Neg_Msg != \"\" ~ 3,\n                               Emp_Neg_Msg != \"\" ~ 4,\n                               Emp_Pos_Msg != \"\" ~ 5),\n                id = 1:133,\n                rolnum = 1:133) \n\nAt this point, I create a new dataset with ‚Äòid‚Äô and ‚Äòcondition‚Äô columns so it is easier to inner_join them. Then I inner_join them so I have everything in one dataset named ‚Äòfin‚Äô.\n\nex22 <- ex2 %>% select(id, condition, rolnum)\n\nfin <- final %>% inner_join(ex22, by = \"id\")\n\nThen we group by Gender and summarise by mean. This allows us to see the mean score that each gender got correct. As we can see, females reported a higher correct score than males on average.\n\nfin %>% group_by(Gender) %>% summarise(mean(summm))\n\n# A tibble: 2 √ó 2\n  Gender `mean(summm)`\n  <chr>          <dbl>\n1 Female          4.75\n2 Male            3.68\n\n# mutate(rrr, coun)\n# fin %>% group_by(condition) %>% summarise(mean(summm))\n\nAnalyzing the data regarding the draw number we use the tii dataset. The reason is because this dataset was created in question 1 with a function named seperate. This function separated the column ‚Äòdraw‚Äô into two others (rolsep, rolsum). The first stands for the title of the draw column while the second one stands for the values that a correct answer was reported.\nTherefore, we here use the filter column(rolnum) equal with the number of observation we want piped with summary(answer). This helps us find the stats behind the correct answers in each round. Also, we named a new dataset ‚Äòi‚Äô where it excludes some irrelevant columns for the purpose of analyzing the data based on draws.\nRegarding the draws we can see that the mean for outcome number 1 is almost 20% (0.1955). That means that one out of five people reported a correct answer in that case. As we can see the mean of correct responses ranges from 0.16 to 0.27. This means that the standard deviation is relatively low. In other words all correct response rates are clustered around the mean.\n\ni <- tii %>% select(-Gender, -Age, -SC0, -id, -Risk, -rolsep, -summm, -id)\n\nAdding missing grouping variables: `id`\n\ni %>% filter(rolnum==1) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==2) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==3) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==4) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==5) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2331  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==6) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==7) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==8) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==9) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==10) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2105  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==11) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==12) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==13) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==14) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2707  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==15) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==16) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==17) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2256  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==18) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2105  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==19) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.218  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==20) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2632  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\n\nRegarding the experimental condition of the data, we use the groupby function to call it in the fin dataset, and then use summarise to find the mean. Alongside, as we set above, each number corresponds in one condition. As we can see, the most popular experimental condition is normative positive message (2) while the least popular is empirical positive message (5).\n\nfin %>% group_by(condition) %>% summarise(mean(summm))\n\n# A tibble: 5 √ó 2\n  condition `mean(summm)`\n      <dbl>         <dbl>\n1         1          5.52\n2         2          6.08\n3         3          4.20\n4         4          3.64\n5         5          3.36\n\n\nRedo part 2 after excluding outliers. Do you still see the different in the experimental groups after outliers are eliminated? Do you see differences between the first and the last rolls? Are there any differences by gender?\n\n# trial <- fin %>% select(-Gender, -Age, -SC0, -Risk, -Outcome1, -Outcome2, -Outcome3, -Outcome4, -Outcome5, -Outcome6, -Outcome7, -Outcome8, -Outcome9, -Outcome10, -Outcome11, -Outcome12, -Outcome13, -Outcome14, -Outcome15, -Outcome16, -Outcome17, -Outcome18, -Outcome19, -Outcome20, -rolnum) %>% \n#   boxplot(trial)\n# \n# boxplot.stats(trial$summm)$out \n\n\nboxplot(fin$summm,\n  ylab = \"summm\")\n\n\n\nboxplot.stats(fin$summm)$out\n\n[1] 10 17 20 11 13 10 11 13\n\nout <- boxplot.stats(fin$summm)$out\nout_ind <- which(fin$summm %in% c(out))\nout_ind\n\n[1]  15  25  48  57  89  98 121 130\n\n# ggplot(fin) +\n#   aes(x = summm) +\n#   geom_histogram(bins = 21, fill = \"#0c4c8a\") +\n#   theme_minimal()\n\nAfter, wecreate a new dataset called ‚Äòdesperate‚Äô where we remove the indicative rows with the outliers. This will help us redo exercise 2\n\ndesperate <- fin[-c(15, 25,48,57,89,98,121,130), ]\n\nRedoing exercise 2 with the new dataframe ‚Äòdesperate‚Äô.\nIn comparison with exercise 2, we can see that the mean for both male and female has fallen. This was a natural outcome to follow since the outliers were all above the mean. However, the outcome remained the same in matters of which gender reported the most correct answers. On average females report 4 correct answers out of twenty, as opposed to men (3/20).\n\ndesperate %>% group_by(Gender) %>% summarise(mean(summm))\n\n# A tibble: 2 √ó 2\n  Gender `mean(summm)`\n  <chr>          <dbl>\n1 Female          4.19\n2 Male            3.14\n\n\nFor the draws, we are creating a new column to make the analysis named ‚Äòabcdefg‚Äô. This is to exclude the outlier rows from the the ‚Äòtii‚Äô dataset we‚Äôve used at exercise 2. After doing that though, we can see that the mean correct response reporting ratio remained at the same levels ranging from 16.54% to 27.07%. This indicates that the outliers did not affect the variability of the data. No difference between the first and last rows.\n\nabcdefg <- tii[-c(15, 25,48,57,89,98,121,130), ]\n\ni <- abcdefg %>% select(-Gender, -Age, -SC0, -id, -Risk, -summm, -rolsep, -id)\n\nAdding missing grouping variables: `id`\n\ni %>% filter(rolnum==1) %>% summary(answer)\n\n       id            rolnum              answer     \n Min.   :  1.00   Length:132         Min.   :0.000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.000  \n Median : 67.50   Mode  :character   Median :0.000  \n Mean   : 67.45                      Mean   :0.197  \n 3rd Qu.:100.25                      3rd Qu.:0.000  \n Max.   :133.00                      Max.   :1.000  \n\ni %>% filter(rolnum==2) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==3) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==4) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==5) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.49                      Mean   :0.2348  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==6) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==7) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==8) %>% summary(answer)\n\n       id            rolnum              answer     \n Min.   :  1.00   Length:132         Min.   :0.000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.000  \n Median : 67.50   Mode  :character   Median :0.000  \n Mean   : 67.48                      Mean   :0.197  \n 3rd Qu.:100.25                      3rd Qu.:0.000  \n Max.   :133.00                      Max.   :1.000  \n\ni %>% filter(rolnum==9) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.47                      Mean   :0.2348  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==10) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.45                      Mean   :0.2121  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==11) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==12) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==13) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==14) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2707  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==15) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  2.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.50                      Mean   :0.2576  \n 3rd Qu.:100.25                      3rd Qu.:1.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==16) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==17) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.48                      Mean   :0.2273  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==18) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.47                      Mean   :0.2045  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==19) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.218  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==20) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2632  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\n\nIn the case of experimental conditions we can see that there are some significant differences. The range between them became smaller (3.36 to 4.69). In other words the data became more disperse. While there is no difference in the lower end, there is a difference in the higher end of the range. This indicates that the previous means were inflated due to the high outliers. For insance, the most popular experimental condition, normative positive message (2), was 6.08, whereas now is just 4.33.\n\ndesperate %>% group_by(condition) %>% summarise(mean(summm))\n\n# A tibble: 5 √ó 2\n  condition `mean(summm)`\n      <dbl>         <dbl>\n1         1          4.70\n2         2          4.33\n3         3          3.74\n4         4          3.43\n5         5          3.36\n\n\nSplit your sample into the \"younger\" half and the \"older half. Are there differences between the two age groups?\nFirst we sort the date from highest to lowest reported age using the order function.\n\n\nordered<- fin[order(fin$Age, decreasing = TRUE), ]\n\nThen we find the Age median from fin dataset. As we can see, the median is 24 (in the column 67 since is exactly the middle of 133).\n\nmedian(fin$Age)\n\n[1] 24\n\n\nSince I have the ordered dataframe (from the highest to lowest Age), I use it to create two other dataframes, one for the younger Age group ‚Äúorderedyoung‚Äù, and one for the older Age group ‚Äúorderedold‚Äù.\n\norderedyoung <- ordered[-c(1:66), ]\norderedold <- ordered[-c(68:133), ]\n\norderedold%>% ggplot(aes(x= Age, y= summm)) + geom_point()\n\nboxplot(orderedold$Age,\n  ylab = \"Age\")\n\n\n\nboxplot.stats(orderedold$Age)$out\n\n[1] 56 51 46 35\n\nout <- boxplot.stats(orderedold$Age)$out\nout_ind <- which(orderedold$Age %in% c(out))\nout_ind\n\n[1] 1 2 3 4\n\n\n\nboxplot(orderedyoung$Age,\n  ylab = \"Age\")\n\n\n\nboxplot.stats(orderedyoung$Age)$out\n\n[1] 20\n\nout <- boxplot.stats(orderedyoung$Age)$out\nout_ind <- which(orderedyoung$Age %in% c(out))\nout_ind\n\n[1] 67\n\n\n\noo <- orderedold[-c(1,2,3,4), ]\n\noy <- orderedyoung[-c(67), ]\n\nI use the ggplot function to create a graph that indicates the Age of the participants in the x axes, and the number of correct guesses (summm) in the y axes. As we can see, while the Age column now indicates only the oldest people, the majority is still under 30 ‚Äì closer to the median. From the age 24 until 30s, the amount of correct responses seems to decline. We use the median and the mean code in order to see if there is a difference with them. Median indicates that there is no difference among the data as the reported correct outcome. However, if we look at the mean of the two, the younger group tends to report more correct answers on average than the older group. This difference is small though (4.62-3.92)\n\noo%>% ggplot(aes(x= Age, y= summm)) + geom_point()\n\n\n\nmedian(oo$summm)\n\n[1] 4\n\nmean(oo$summm)\n\n[1] 3.920635\n\noy%>% ggplot(aes(x= Age, y= summm)) + geom_point()\n\n\n\nmedian(oy$summm)\n\n[1] 4\n\nmean(oy$summm)\n\n[1] 4.621212\n\n\n\nPART TWO\nLet's study how the number of wins depends on certain factors:\n\nlibrary(readxl)\nexp21 <- read_excel(\"experiment_2021C(1)copy.xlsx\", col_names = FALSE,\n    skip = 2)\n\nNew names:\n* `` -> ...1\n* `` -> ...2\n* `` -> ...3\n* `` -> ...4\n* `` -> ...5\n* ...\n\n    var_names <- read_excel(\"experiment_2021C(1)copy.xlsx\", n_max = 1) \n\nNew names:\n* `Q171_First Click` -> `Q171_First Click...46`\n* `Q171_Last Click` -> `Q171_Last Click...47`\n* `Q171_Page Submit` -> `Q171_Page Submit...48`\n* `Q171_Click Count` -> `Q171_Click Count...49`\n* `Q171_First Click` -> `Q171_First Click...82`\n* ...\n\n    colnames(exp21) <- colnames(var_names)\n\n    rm(var_names) \n\n    columnss <- exp21 %>% \n      select(starts_with(\"Outcome\"), \"Age\", \"Risk\", \"Gender\", \"SC0\") \n\nnormal numbers, the more * the more signif\n\ndatac <- columnss %>% mutate(score = SC0 *10)\n\ndatacool <- datac %>% select(-Outcome1, -Outcome2, -Outcome3, -Outcome4, -Outcome5, -Outcome6, -Outcome7, -Outcome8, -Outcome9, -Outcome10, -Outcome11, -Outcome12, -Outcome13, -Outcome14, -Outcome15, -Outcome16, -Outcome17, -Outcome18, -Outcome19, -Outcome20, -SC0)\n\ndataex1<- datacool %>% select(-Risk, - Age)\n\n\noptions(scipen = 100)\n\n1. Examine the relationship between gender and the number of wins using regression analysis.\nindependent variable x = the cause explanatory, dependent variable y = outcome\nNumber of wins is referred to as the Score in our data, and it is a continuous dependent variable(=y).\nGender is a categorical variable and is the independent variable(=x).\nOn the first chunk I create one new column name ‚ÄòGendering‚Äô where I assign values 0 and 1 to female and male correspondingly.\nOn the second and third chunks, I do the linear regression between Gender(x) and Score - aka number of wins - (y).\n\ndatacool1 <- datacool %>% mutate(Gendering = case_when(Gender == \"Male\" ~ \"1\",\n                                        Gender== \"Female\" ~ \"0\"))\n\ndataex1 %>% summary(lm(score ~ factor(Gendering), data = datacool1))\n\n    Gender              score       \n Length:133         Min.   : 0.000  \n Class :character   1st Qu.: 3.000  \n Mode  :character   Median : 4.000  \n                    Mean   : 4.398  \n                    3rd Qu.: 5.000  \n                    Max.   :20.000  \n\nmodel<- lm(formula= score~Gendering, data= datacool1)\n(summary(model))\n\n\nCall:\nlm(formula = score ~ Gendering, data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7528 -1.6818 -0.7528  0.3182 16.3182 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)   4.7528     0.3088  15.389 <0.0000000000000002 ***\nGendering1   -1.0710     0.5370  -1.995              0.0482 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.914 on 131 degrees of freedom\nMultiple R-squared:  0.02947,   Adjusted R-squared:  0.02206 \nF-statistic: 3.978 on 1 and 131 DF,  p-value: 0.04817\n\n\nLooking at the results from the intercept and the Gendering column (1 = given that is a male), we can see that the regression equation is the following:\ny=-1.07x+4.75\na) What is the effect (the slope) of gender?\nTo see the effect we look to the slope of x. It indicates that on average males win one time less than females.\nb) How strong is the predictive power of gender?\nLooking at the multiple R square we can see that the effect of gender explains 2,947% of the variability of the score.\nc) What are the predicted outcomes for men and women?\nFor this I substitute the values for male and female on the regression equation. For male x=1, for female x=0. After doing that, we will have a result that demonstrates the predicted score for females and males given that our initial assumption holds true.\n\n#for male\nmalepred <- -1.07*1+4.75\n\n#for female\nfemalepred <- -1.07*0+4.75\n\nThe predicted score for male on average is 3.68, while the equivalent predicted score for female is 4.75.\n2. Examine the relationship between age and the number of wins using regression analysis.\n\nmodel1<- lm(formula = score~Age, data= datacool1)\nsummary(model1)\n\n\nCall:\nlm(formula = score ~ Age, data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4383 -1.4232 -0.4080  0.6524 15.4559 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  4.01507    1.36266   2.946  0.00381 **\nAge          0.01511    0.05276   0.287  0.77495   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.957 on 131 degrees of freedom\nMultiple R-squared:  0.0006262, Adjusted R-squared:  -0.007003 \nF-statistic: 0.08208 on 1 and 131 DF,  p-value: 0.7749\n\n\nAfter following the same procedure as we followed in exercise 1, we have the equivalent regression results for Age as independent variable x, and score as dependent variable y. The equation is as follows:\ny=0.015x+4.01\nmultiple R-squared, how much of the score variability is explained assuming that there is an effect of age.\na) What is the effect of age?\nThe effect (aka slope) of age is 0.015. This implies that there is an observed higher score of 0.015 with people that are older. Put differerently, the age factor has a negligible result to the score as is almost zero.\nb) How strong is the predictive power of age?\nLooking at the multiple R square we can see that the effect of age has almost zero explanation for the variability of the score. In other words, the predictive power of age on the time of victories is very weak.\nc) What's the predicted outcome for a 20 year old person? For a 40 year old person?\n\n#for 20 year old\ntwentypred <- 0.015*20+4.01\n\n#for 40 year old\nfourtypred <- 0.015*40+4.01\n\nFor this I substitute the values for 20year old and 40year old on the regression equation. For 20year x=20, for 40year old x=40. After doing that, we will have a result that demonstrates the predicted score for 20year old and 40year old given that our initial assumption holds true.\nThe predicted score for 20year old on average is 4.31, while the equivalent predicted score for 40year old is 4.61.\n3. Examine the relationship between age and gender combined.\nwhy the numbers are changing slightly (=men and women have different ages. when u plug both coefficients together it tries to)\na) What are the effects now? How do they compare with the results in 1 and 2? Explain the differences\n\nmodel1<- lm(formula = score~Age+factor(Gendering), data= datacool1)\nsummary(model1)\n\n\nCall:\nlm(formula = score ~ Age + factor(Gendering), data = datacool1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.859 -1.673 -0.673  0.401 16.082 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)   \n(Intercept)         4.08770    1.34714   3.034  0.00291 **\nAge                 0.02660    0.05244   0.507  0.61280   \nfactor(Gendering)1 -1.10062    0.54165  -2.032  0.04419 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.922 on 130 degrees of freedom\nMultiple R-squared:  0.03139,   Adjusted R-squared:  0.01649 \nF-statistic: 2.107 on 2 and 130 DF,  p-value: 0.1258\n\n\nNow the equation becomes:\ny=0.02x1-1.1x2+4.08\nwith x1 being the age, and x2 being the gender given is a male (if that one is zero then is a female)\nAs we can see here, while the age effect is still close to zero, now it is a bit higher (0.026 as opposed to 0.015); however it still remains a weak factor. With respect to Gender, now is also higher ton the other end than its corresponding previous result (-1.10 as opposed to -1.07).\nConsequently, there is a very slight inter correlation between the two factors as the model tends to become slightly stronger. How ever the differences are minimal. This is indicated also in the adjusted R squared (=0.016) which is even lower than R squared. This means that at least one of the two variables does not explain the dependent variable score. As we can see from the exercises 1 and 2, this variable is Age because its adjusted R squared is negative when we have Age as the only independent variable (see exercise 2).\nThen we proceed to an interaction coefficient just to observe the cross sections between our independent variables. Despite that the numbers are changing a bit though interaction coefficient is not necessary for the purpose of this exercise.\n\ncool3 <- lm(score~ Age * factor(Gendering), data = datacool1)\nsummary(cool3)\n\n\nCall:\nlm(formula = score ~ Age * factor(Gendering), data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8912 -1.6606 -0.6606  0.6527 15.5740 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)   \n(Intercept)             5.90581    1.99661   2.958  0.00369 **\nAge                    -0.04612    0.07890  -0.585  0.55989   \nfactor(Gendering)1     -4.41097    2.74144  -1.609  0.11006   \nAge:factor(Gendering)1  0.12987    0.10544   1.232  0.22030   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.916 on 129 degrees of freedom\nMultiple R-squared:  0.04265,   Adjusted R-squared:  0.02039 \nF-statistic: 1.916 on 3 and 129 DF,  p-value: 0.1303\n\n\nWe observe that the estimations between age and gender are not significant.\nb) How strong is the predictive power of this model?\nLooking at the adjusted R square we can see that the effect of age combined with the effect of gender explains 1,6% the variability of the score - when doing multiple regression, adjusted R squared is a better indication to look at for the predictive power of the models. And in conclusion, this one does not have a strong predictive power.\nc) What is the predicted outcome for a 20 year old man? 20 year old woman? 40 year old man? 40 year old woman?\nFollowing similar procedure as the exercise 1 and 2 c, we substitute the age and gender factors in the equation for the equivalent results we need in the four different instances:\n\n#for 20year old man\ntwentyman <- 0.02*20-1.1*1+4.08\n\n#for 20year old woman\ntwentywoman <- 0.02*20-1.1*0+4.08\n\n#for 40year old man\nfourtyman <- 0.02*40-1.1*1+4.08\n\n#for 40year old woman\nfourtywoman <- 0.02*40-1.1*0+4.08\n\nAs we can see, the predicted scores are the following:\nfor 20year old man = 3.38\nfor 20year old woman = 4.48\nfor 40year old man = 3.78\nfor 40year old woman = 4.88"
  },
  {
    "objectID": "software/sitcoms.html",
    "href": "software/sitcoms.html",
    "title": "Sitcoms",
    "section": "",
    "text": "Let's continue working with the Google Trends data you obtained for Homework 02. Homework 3 starts from the line 180 and onward.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úì ggplot2 3.3.5     ‚úì purrr   0.3.4\n‚úì tibble  3.1.6     ‚úì dplyr   1.0.7\n‚úì tidyr   1.1.4     ‚úì stringr 1.4.0\n‚úì readr   2.0.1     ‚úì forcats 0.5.1\n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(broom)\nlibrary(ggthemes)\nlibrary(ggsci)\n\n\nCode to load the data into R and prepare it for the analysis. You need to correctly specify data types and choose concise variable names.\n\n#Import data for the US\nuscoms <- read_csv(\"uscoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (United States), South Park: (United States), The Simps...\n\n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for the world\nworldcoms <- read_csv(\"worldcoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Worldwide), South Park: (Worldwide), The Simpsons: (Wo...\n\n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for Cyprus\ncycoms <- read_csv(\"cycoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n\n\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Cyprus), South Park: (Cyprus), The Simpsons: (Cyprus)\n\n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nprepare the data for analysis\n\n#NOW WE CLEAN AND PREPARE THE DATA FOR ANALYSIS\n#Clean for the US.\nuscoms_clean <- uscoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (United States)`,\n         southpark = `South Park: (United States)`,\n         simpsons = `The Simpsons: (United States)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n  mutate(day = 15, .after = month) %>%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Worldwide\nworldcoms_clean <- worldcoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (Worldwide)`,\n         southpark = `South Park: (Worldwide)`,\n         simpsons = `The Simpsons: (Worldwide)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n  mutate(day = 15, .after = month) %>%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Cyprus.\ncycoms_clean <- cycoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (Cyprus)`,\n         southpark = `South Park: (Cyprus)`,\n         simpsons = `The Simpsons: (Cyprus)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n\n  #I separate the month and  year into two columns. Then I convert the character column to a number.\n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n\n  #I create a new column that is called day, and I use 15 as it is the middle of the month.\n  mutate(day = 15, .after = month) %>%\n\n  #Here I create a date column using the ymd (=year month day)function.\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\nCode that will calculate the average popularity of the terms by year for each of the search terms in each of the geographies.\n\n#HERE WE SUMMARIZE THE AVERAGE POPULARITY OF THE TERMS BY YEAR OF EACH SEARCH\n#Summarize for the US\nuscoms_summary <- uscoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Worldwide\nworldcoms_summary <- worldcoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Cyprus\ncycoms_summary <- cycoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n\nBefore we analyse the data, we tidy them. This way, we have three rows that correspond for each month from 2004 until 2021. This makes easier for us to specify what goes where.\n\nuscoms_tidy <- uscoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworldcoms_tidy <- worldcoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\ncycoms_tidy <- cycoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\n4. Analyze the data to answer the following questions:\n\nIn what year was each term most popular? In which geography is each of the terms most popular in the year you found in Part A?\n\nuscoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\nworldcoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line()+ scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\ncycoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\nBased on the graphs the most popular year for\n‚ÄìFamily guy was: 2nd half of 2008 (USA)/ 1st half of 2009(Worldwide)/ 1st, 2nd half of 2007 (Cyprus)\n‚ÄìSimpsons was: mid-2007 (USA,Worldwide)/ 2nd half of 2008 (Cyprus)\n‚ÄìSouth Park was: 1st half of 2010 (USA, Worldwide) / first half of 2004 (Cyprus)\nCalculate the ratio of the most popular term to the least popular term. Describe how this ratio changed over time in each of the geographies by relying on yearly data.\n\n\n#I hereby summarize in three different ways the mean search score of each series.\nworldcoms_clean %>% summarize(mean(southpark), mean(simpsons), mean(famguy))\n\n# A tibble: 1 √ó 3\n  `mean(southpark)` `mean(simpsons)` `mean(famguy)`\n              <dbl>            <dbl>          <dbl>\n1              22.2             36.2           20.3\n\nworldcoms_clean %>% summarize_at(vars(southpark, simpsons, famguy), mean)\n\n# A tibble: 1 √ó 3\n  southpark simpsons famguy\n      <dbl>    <dbl>  <dbl>\n1      22.2     36.2   20.3\n\nsummary(worldcoms_clean)\n\n      year          month             day         famguy        southpark    \n Min.   :2004   Min.   : 1.000   Min.   :15   Min.   : 5.00   Min.   : 7.00  \n 1st Qu.:2008   1st Qu.: 3.000   1st Qu.:15   1st Qu.:12.00   1st Qu.:11.00  \n Median :2013   Median : 6.000   Median :15   Median :19.00   Median :21.00  \n Mean   :2013   Mean   : 6.475   Mean   :15   Mean   :20.29   Mean   :22.22  \n 3rd Qu.:2017   3rd Qu.: 9.000   3rd Qu.:15   3rd Qu.:29.00   3rd Qu.:29.00  \n Max.   :2022   Max.   :12.000   Max.   :15   Max.   :45.00   Max.   :69.00  \n    simpsons           date           \n Min.   : 13.00   Min.   :2004-01-15  \n 1st Qu.: 23.00   1st Qu.:2008-07-15  \n Median : 37.00   Median :2013-01-15  \n Mean   : 36.18   Mean   :2013-01-13  \n 3rd Qu.: 47.00   3rd Qu.:2017-07-15  \n Max.   :100.00   Max.   :2022-01-15  \n\n#Looking at the results we conclude that the simpsons on average was the most popular by far with 36.18 score. Then southpark follows with 22.22 and then family guy follows with 20.29.\n\nThen we group the averages by year so we can compare the yearly differences.\n\nworld_summary <- worldcoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\nworld_tidy <- world_summary %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() \n\n\n\n\n\nHOMEWORK 3 STARTS HERE\n\nPrepare the graph showing the trends in popularity of the search terms over time. Make sure to add a descriptive title, label the axes, and modify the look of the graph to be presentable. You can choose your own colors or use one of the palettes we talked about in class. Add a one-sentence explanation for the geometry you selected for the graph.\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\") +\n  scale_color_discrete(labels = c(\"Family Guy\", \"The Simpsons\", \"South Park\")) #rename titles\n\n\n\n\nThe same code as before is used to make the graph. The only change here is that we rename the names of the shows in the graph by using the command ‚Äúscale_color_discrete(labels = c(‚Ä¶)‚Äù.\n\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"Popularity graph\",\n       subtitle = \"Top 3 American sitcoms\"\n       ) +\n  scale_color_discrete(name = \"Shows:\",\n                       labels = c(\"South Park\", \"The Simpsons\", \"Family Guy\")) +\n  theme_wsj()  #this theme makes the graph more presentable\n\n\n\n\nThis is a more presentable version of the graph above.\n\nSmooth the data (using a graph) to eliminate random noise. Explain which smoothing method you chose and why.\n\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  #geom_line() + #I remove this so we can see the variability of each sitcom.\n  geom_smooth(method = \"loess\") + #This command creates a moving average that smoothes out all the fluctuations.\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFor this code we use LOESS command to smooth the fluctuations. LOESS (aka locally weighted smoothing), helps us see the relationship between the three variables and foresee trends. A LOESS smoother takes the data, fits a regression with the subset of data, and uses that linear regression model to get a point for the smoothed curve. The points closer to the fitted line are more impact-full.\n\nCreate a chart to show seasonality by month in your data. What does the chart tell you about the seasonality of your chosen search terms? Some examples of seasonality charts can be found here: https://www.r-graph-gallery.com/142-basic-radar-chart.html (Links to an external site.)\n\n\nworldcoms_tidy %>% \n  group_by(month, name) %>% #I group by month to see which season are popular. By name to see the shows.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score, fill = name)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  coord_polar()\n\n`summarise()` has grouped output by 'month'. You can override using the `.groups` argument.\n\n\n\n\n\nAs we can see, the Simpsons seasonality graph looks the same. That is probably we have no limits to the popularity score of the variable and every score is close to each other. i.e.¬†popularity does not vary by a lot.\n\nworldcoms_tidy %>% \n  filter(name == \"simpsons\") %>% #I filter by name \"simpsons\" so I can examine the Simpsons show by itself.\n  group_by(month) %>% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(30, 40)) + #I include score limits from 30 to 40 in order to see the exact seasons where \"The simpsons\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col).\n\n\n\n\n\n\nworldcoms_tidy %>% \n  filter(name == \"famguy\") %>% #I filter by name \"famguy\" so I can examine the \"Family Guy\" show by itself.\n  group_by(month) %>% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(15, 25)) + #I include score limits from 15 to 25 in order to see the exact seasons where \"The Family Guy\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col)."
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-of-all-presidents-words-since-truman",
    "href": "software/presidents_analysis.html#interpretation-of-all-presidents-words-since-truman",
    "title": "Loizos Konstantinou",
    "section": "Interpretation of all president‚Äôs words since Truman",
    "text": "Interpretation of all president‚Äôs words since Truman\nAmerican priorities shifted over time. As we can see, From Hoover (1929) until Nixon (1974) issues and words related to ‚Äúfreedom‚Äù, and ‚Äúpeace‚Äù were emphasized. This makes sense since during that time, WW2 and the Vietnam War were fought.\n‚ÄúEconomy‚Äù seems like a topic that is popular in almost every presidency.\nDuring Ford‚Äôs and Reagan‚Äôs presidency, ‚Äútax‚Äù, and ‚Äúgrowth‚Äù became really hot buzz words. Especially during Reagan administration, big tax reforms were introduced which they have significantly reduce taxes for businesses.\n‚Äúinflation‚Äù and ‚Äúenergy‚Äù were also popular during Nixon, Ford, and Reagan. It is important because especially during Nixon, the US economy after 14 years of economic development got in a stagflationary state; oil and gas crisis at the 70s was also a part of that.\nDuring George W Bush (2005), ‚Äúsecurity‚Äù was the most popular word used. This is because especially after 911, security became the main focus of his presidency.\nHealthcare gained importance during the Clinton Administration, and two administrations later, the Obama Administration expanded Medicaid. With the Covid-19 pandemic, healthcare again dominated the policy priorities in Biden‚Äôs 2022 address.\nAll the presidents, irrespective of their political affiliation (Democrats vs Republicans), mentioned about strengthening / growing the economy. Only presidents affiliated with the Democratic party seemed to emphasize on ‚Äúhealthcare‚Äù, whereas a common theme among the Republican presidents‚Äô addresses was ‚Äúwar / military spending / terrorism‚Äù.\nTop ten words of all Democrat Presidents\n\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\ndemocrat_speeches = pd.read_excel('democrat_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in democrat_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\ndemocrat_speeches['top ten'] = top_list\n\ndisplay(democrat_speeches)\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('industrial', 9),('work', 8),('recovery', 7)...\n    \n    \n      1\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('prosperity', 12),('production', 12),('power...\n    \n    \n      2\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('economic', 16),('development', 10),('peace'...\n    \n    \n      3\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('freedom', 12),('life', 9),('progress', 8),(...\n    \n    \n      4\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      [('inflation', 17),('economic', 14),('tax', 13...\n    \n    \n      5\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('children', 24),('work', 21),('budget', 17),...\n    \n    \n      6\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('jobs', 32),('work', 20),('energy', 18),('fa...\n    \n    \n      7\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('folks', 19),('see', 15),('families', 15),('...\n    \n  \n\n\n\n\nTop ten words of all Republican Presidents\n\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\nrepublican_speeches = pd.read_excel('republican_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in republican_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\nrepublican_speeches['top ten'] = top_list\n\ndisplay(republican_speeches)\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('public', 33),('law', 25),('service', 23),('...\n    \n    \n      1\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('free', 16),('security', 16),('economy', 12)...\n    \n    \n      2\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('peace', 27),('energy', 17),('war', 8),('pro...\n    \n    \n      3\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('energy', 25),('oil', 20),('tax', 17),('econ...\n    \n    \n      4\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('freedom', 20),('tax', 16),('growth', 14),('...\n    \n    \n      5\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('budget', 17),('work', 12),('hope', 10),('dr...\n    \n    \n      6\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('security', 29),('freedom', 20),('social', 1...\n    \n    \n      7\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('tax', 15),('last', 13),('together', 13),('w...\n    \n  \n\n\n\n\n\nCan you conduct topic analysis of LDA using the speeches to determine what things presidents talk about in state of the union speeches?\n\nLinear Discriminant Analysis (LDA) is like PCA, but it focuses on maximizing the seperatibility among known categories\n\ntype(df2.iloc[:,0])\n\npandas.core.series.Series\n\n\n\ndataset = pd.read_excel('speeches53463.xlsx')\n\ngrouped = dataset.groupby('name')\ndataset['date'] = pd.to_datetime(dataset['date'])\n\n#print(dataset.head())\ngrouped = dataset.groupby('name')\n\ndataset2 = dataset.loc[dataset.groupby('name').date.idxmin()]\n#.filter(lambda x: x['date'] == min(x['date']))\n\n\nimport re\nimport numpy as np\n    \n# Print the titles of the first rows \nprint(df2[[0]].head())\n\n# Remove punctuation\ndataset['title_processed'] = dataset['speech'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\ndataset['title_processed'] = dataset['title_processed'].map(lambda x: x.lower())\n\n# Print the processed titles of the first rows \ndataset['title_processed'].head()\n\n                                                   0\n0  Address Before a Joint Session of the Congress...\n1  Address Before a Joint Session of Congress on ...\n2  Address Before a Joint Session of Congress on ...\n3  Annual Message to the Congress on the State of...\n4  Radio Address Summarizing the State of the Uni...\n\n\n0    the presidentthank you thank you thank you goo...\n1    the presidentthank you all very very much than...\n2    thank you very much mr speaker mr vice preside...\n3    the presidentmr speaker mr vice president memb...\n4    the presidentmadam speaker mr vice president m...\nName: title_processed, dtype: object\n\n\n\n# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(dataset['title_processed'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation",
    "href": "software/presidents_analysis.html#interpretation",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nWhile the most common words do not seem to reveal something particular, when those generic words are cleaned as we saw above, we get specific topics and buzz words from each president.\n\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 5\nnumber_words = 25\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)\n\nTopics found via LDA:\n\nTopic #0:\nwar forces fighting men production japanese enemies peace peoples china enemy army german air germany world know armed nations united shall chinese nurses allies victory\n\nTopic #1:\ngovernment congress world national great people public federal law shall business economic power nations nation labor present war year country men action united legislation states\n\nTopic #2:\nnew people america year american years world congress government make nation americans time help work federal tax economy jobs let need security peace programs know\n\nTopic #3:\nanarchist anarchistic mckinley anarchists anarchy doctrines 1872 anna bocanegra dictator bridge haro reappropriation garfield apologizes malefactor recoined amazement imprison buren limb alleging preaching loses murder\n\nTopic #4:\nstates government united congress country great public year citizens time people state treaty war present foreign subject american shall act general power nations commerce necessary"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-1",
    "href": "software/presidents_analysis.html#interpretation-1",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nTopic 0: aligns with internal state laws and political stability.\nTopic 1: seems to align mostly with internal US issues. It is targeted to citizens stability (jobs) and needs.\nTopic 2: Seems to be more related with tax policy and approach to the US economy.\nTopic 3: It has to do with public policy and foreign affairs.\nTopic 4: Mainly targeted to defence department and peace status maintenance while protecting the interests of the US.\n\nCan you determine the sentiment of each state of the union using nltk‚Äôs Vader module?\n\n\nanalyzer=SentimentIntensityAnalyzer()\ndef polarity_score(text):\n    if len(text)>0:\n        score=analyzer.polarity_scores(text)['compound']\n        return score\n    else:\n        return 0\ndataset['polarityscore'] = dataset['speech'].apply(lambda text : polarity_score(text))\ndataset['polarityscore']\n\n0      0.9999\n1      0.9999\n2      1.0000\n3      0.9999\n4      0.9999\n        ...  \n254    0.9998\n255    0.9994\n256    0.9995\n257    0.9991\n258   -0.9997\nName: polarityscore, Length: 259, dtype: float64\n\n\n\ndef sentianamolybarplot(df):\n    polarity_scale=[0.9991,0.9992,0.9993,0.9994,0.9995,0.9996,0.9997,0.9998,0.9999,1]\n    #'Review_polarity' is column name of sentiment score calculated for whole review.\n    df3=df[(df['polarityscore']>0)]\n    out = pd.cut(df3['polarityscore'],polarity_scale)\n    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n    plt.show()\nsentianamolybarplot(dataset)\n\n\n\n\nInterpretation\nIn the State of Union Speeches, the presidents talk about important issues facing Americans and offers their ideas on solving the nation‚Äôs problems, including suggestions for new laws and policies. As displayed in the plot, the polarity scores for 13 speeches (barring W. Bush) is positive. This is understandable as the State of Union speeches are a PR vehicle, leveraged to display the President‚Äôs power and positive influence.\n\nDo speeches of different presidents cluster in any way that can allow you to determine their political party? How different are Biden and Trump according to this clustering?\n\n\ndef remove_noise(text, stop_words = nltk.corpus.stopwords.words('english')):\n    newsw = ['annual', 'number', 'help', 'thank', 'get', 'going', 'think', 'look', 'said', 'create',\n             'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', 'long', 'better', \n             'plan', 'national', 'ask' '10', 'much', 'good', 'great', 'best', 'cannot', 'still', \n             'know', 'years', '1', 'major', 'want', 'able', 'put', 'capacity', 'programs', 'per', \n             'percent', 'million', 'act', 'provide', 'afford', 'needed', 'may', 'possible', 'full',\n             '2', 'effort', 'meeting', 'address', 'ever', 'measures', 'ago', 'delivered', '5', \n             'program', 'past', 'future', 'need', 'needs', 'house', 'also', 'tonight', 'propose', \n             'toward', 'continue', 'society','country', 'seek', 'period', 'year', 'man', 'men', \n             'one', 'areas', 'begin', 'live', 'make', 'let', 'upon', 'well', 'office', 'meet', \n             'make' 'citizens', 'human', 'self', 'among', 'peoples', 'affairs', 'would', 'field', \n             'first', 'interest', 'today', 'recommendations', 'recomenndation', 'within', 'shall', \n             'administration', 'nation', 'nations', 'us', 'we', 'policy', 'legislation', 'time', \n             'new', 'many', 'several', 'few', 'government', 'world', 'people', 'united', 'states', \n             'system', 'every', 'people', 'must', '626','give', 'categories', '226762', '17608', \n             '24532', '430', '38','statistics', 'analyses', 'miscellaneous', 'congressional', \n             'skip', 'content', 'documents', 'attributes', 'media', 'message', 'congress', 'state',\n             'union', 'america', 'american', 'americans', 'presidency', 'president', 'project', \n             'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', 'main', \n             'take','like','yet','j','000']\n    stop_words = stop_words + newsw\n    tokens = word_tokenize(text)\n    cleaned_tokens = []\n    for token in tokens:\n        token = re.sub('[^A-Za-z0-9]+', '', token)\n        if len(token) > 1 and token.lower() not in stop_words:\n            # Get lowercase\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\n# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.8,\n                                   max_features = 50,\n                                   min_df = 0.1,\n                                   tokenizer = remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(dataset['speech'].values)\n\n\nnum_clusters = 2\n\n# Generate cluster centers through the kmeans function\ncluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n# display(cluster_centers)\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print('Cluster: {}'.format(i+1))\n    # Sort the terms and print top 3 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms [:15])\n\nCluster: 1\n['federal', 'economic', 'budget', 'work', 'tax', 'economy', 'security', 'jobs', 'nt', 'freedom', 'health', 'free', 'life', 'together', 'defense']\nCluster: 2\n['treaty', 'subject', 'commerce', 'treasury', 'general', 'relations', 'powers', 'laws', 'duty', 'session', 'interests', 'rights', 'however', 'service', 'trade']"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-2",
    "href": "software/presidents_analysis.html#interpretation-2",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nOn clustering the popular words in the speech, it seems like Cluster 1 aligns with speeches by Republican presidents and Cluster 2 with that of speeches by Democartic presidents.\n\n#Converting the datafram into list for further analysis.\nspeech_list = []\nfor i in range(len(dataset2)):\n    speech_list.append(dataset2.iloc[[i]]['speech'].item())\n    \ntitles = []\nfor i in range(len(dataset2)):\n    titles.append(dataset2.iloc[[i]]['name'].item())\n    \ntexts = [txt.split() for txt in speech_list]\n\n# Create an instance of a PorterStemmer object\nporter = PorterStemmer()\n\n# For each token of each text, we generated its stem \ntexts_stem = [[porter.stem(token) for token in text] for text in texts]\n\n# Create a dictionary from the stemmed tokens\ndictionary = corpora.Dictionary(texts_stem)\n\n# Create a bag-of-words model for each speech, using the previously generated dictionary\nbows = [dictionary.doc2bow(text) for text in texts_stem]\n\n# Generate the tf-idf model\nmodel = TfidfModel(bows)\n\n# Compute the similarity matrix (pairwise distance between all speeches)\nsims = similarities.MatrixSimilarity(model[bows])\n\n# Transform the resulting list into a DataFrame\nsim_df = pd.DataFrame(list(sims))\n\n# Add the name of the presidents as columns and index of the DataFrame\nsim_df.columns = titles\nsim_df.index = titles\n\n# Print the resulting matrix\nsim_df\n\n\n\n\n  \n    \n      \n      Abraham Lincoln\n      Andrew Jackson\n      Andrew Johnson\n      Barack Obama\n      Benjamin Harrison\n      Calvin Coolidge\n      Chester A. Arthur\n      Donald J. Trump\n      Dwight D. Eisenhower\n      Franklin D. Roosevelt\n      ...\n      Rutherford B. Hayes\n      Theodore Roosevelt\n      Thomas Jefferson\n      Ulysses S. Grant\n      Warren G. Harding\n      William Howard Taft\n      William J. Clinton\n      William McKinley\n      Woodrow Wilson\n      Zachary Taylor\n    \n  \n  \n    \n      Abraham Lincoln\n      1.000000\n      0.121246\n      0.151205\n      0.047722\n      0.110114\n      0.118578\n      0.114825\n      0.047754\n      0.075706\n      0.047841\n      ...\n      0.092012\n      0.056717\n      0.110742\n      0.124756\n      0.082261\n      0.092794\n      0.046491\n      0.113427\n      0.077418\n      0.145153\n    \n    \n      Andrew Jackson\n      0.121246\n      1.000000\n      0.121070\n      0.053532\n      0.131924\n      0.086656\n      0.106385\n      0.054705\n      0.077823\n      0.047385\n      ...\n      0.112656\n      0.070145\n      0.117074\n      0.146622\n      0.092593\n      0.113945\n      0.050358\n      0.102445\n      0.086139\n      0.155541\n    \n    \n      Andrew Johnson\n      0.151205\n      0.121070\n      1.000000\n      0.047996\n      0.097058\n      0.094947\n      0.079968\n      0.049593\n      0.073742\n      0.053389\n      ...\n      0.082619\n      0.077785\n      0.093668\n      0.127001\n      0.090346\n      0.077516\n      0.049155\n      0.080710\n      0.082443\n      0.092401\n    \n    \n      Barack Obama\n      0.047722\n      0.053532\n      0.047996\n      1.000000\n      0.046357\n      0.069860\n      0.037358\n      0.213163\n      0.103428\n      0.096561\n      ...\n      0.035344\n      0.069914\n      0.054143\n      0.047495\n      0.073191\n      0.049368\n      0.317297\n      0.050867\n      0.064416\n      0.039508\n    \n    \n      Benjamin Harrison\n      0.110114\n      0.131924\n      0.097058\n      0.046357\n      1.000000\n      0.095429\n      0.198889\n      0.043727\n      0.082837\n      0.068579\n      ...\n      0.269502\n      0.057292\n      0.080109\n      0.146124\n      0.080189\n      0.125491\n      0.049069\n      0.142834\n      0.076850\n      0.134368\n    \n    \n      Calvin Coolidge\n      0.118578\n      0.086656\n      0.094947\n      0.069860\n      0.095429\n      1.000000\n      0.085849\n      0.072170\n      0.118543\n      0.078246\n      ...\n      0.081089\n      0.087503\n      0.077970\n      0.098388\n      0.128878\n      0.092294\n      0.076209\n      0.074285\n      0.086982\n      0.092193\n    \n    \n      Chester A. Arthur\n      0.114825\n      0.106385\n      0.079968\n      0.037358\n      0.198889\n      0.085849\n      1.000000\n      0.042525\n      0.058818\n      0.042672\n      ...\n      0.144648\n      0.045146\n      0.063310\n      0.179516\n      0.066345\n      0.137828\n      0.045936\n      0.123470\n      0.059776\n      0.132799\n    \n    \n      Donald J. Trump\n      0.047754\n      0.054705\n      0.049593\n      0.213163\n      0.043727\n      0.072170\n      0.042525\n      1.000000\n      0.077175\n      0.062748\n      ...\n      0.040511\n      0.077699\n      0.046509\n      0.056097\n      0.065570\n      0.040417\n      0.183823\n      0.042885\n      0.056082\n      0.040505\n    \n    \n      Dwight D. Eisenhower\n      0.075706\n      0.077823\n      0.073742\n      0.103428\n      0.082837\n      0.118543\n      0.058818\n      0.077175\n      1.000000\n      0.108428\n      ...\n      0.059357\n      0.074973\n      0.060950\n      0.085503\n      0.107023\n      0.072974\n      0.128444\n      0.065945\n      0.104088\n      0.069061\n    \n    \n      Franklin D. Roosevelt\n      0.047841\n      0.047385\n      0.053389\n      0.096561\n      0.068579\n      0.078246\n      0.042672\n      0.062748\n      0.108428\n      1.000000\n      ...\n      0.064508\n      0.072999\n      0.052945\n      0.059127\n      0.094722\n      0.054787\n      0.086663\n      0.063699\n      0.077462\n      0.050414\n    \n    \n      Franklin Pierce\n      0.138249\n      0.149229\n      0.113608\n      0.039742\n      0.135522\n      0.096017\n      0.129588\n      0.034980\n      0.083958\n      0.055784\n      ...\n      0.096431\n      0.059912\n      0.091416\n      0.149410\n      0.087211\n      0.114362\n      0.038283\n      0.098263\n      0.081036\n      0.181408\n    \n    \n      George Bush\n      0.039750\n      0.110718\n      0.045218\n      0.223350\n      0.039387\n      0.067593\n      0.030361\n      0.166730\n      0.101130\n      0.063581\n      ...\n      0.029770\n      0.064276\n      0.043393\n      0.048644\n      0.064347\n      0.045680\n      0.236761\n      0.042725\n      0.064541\n      0.038623\n    \n    \n      George W. Bush\n      0.045682\n      0.045953\n      0.042901\n      0.244457\n      0.043902\n      0.076660\n      0.044497\n      0.170425\n      0.109930\n      0.069602\n      ...\n      0.038201\n      0.047932\n      0.043533\n      0.048213\n      0.069961\n      0.033635\n      0.258270\n      0.042687\n      0.054859\n      0.033777\n    \n    \n      George Washington\n      0.075427\n      0.080150\n      0.054498\n      0.022271\n      0.046189\n      0.048199\n      0.043466\n      0.021537\n      0.033813\n      0.023598\n      ...\n      0.056167\n      0.028022\n      0.075892\n      0.047378\n      0.038203\n      0.039933\n      0.022466\n      0.030561\n      0.040550\n      0.058378\n    \n    \n      Gerald R. Ford\n      0.040651\n      0.041807\n      0.043266\n      0.133494\n      0.045676\n      0.079921\n      0.053701\n      0.082173\n      0.135036\n      0.062518\n      ...\n      0.035063\n      0.051462\n      0.039757\n      0.051069\n      0.071961\n      0.044571\n      0.183497\n      0.041057\n      0.048935\n      0.039580\n    \n    \n      Grover Cleveland\n      0.117943\n      0.128614\n      0.102102\n      0.036857\n      0.129553\n      0.088140\n      0.141185\n      0.043138\n      0.070560\n      0.049183\n      ...\n      0.098994\n      0.069918\n      0.072758\n      0.129015\n      0.083239\n      0.154388\n      0.035827\n      0.105645\n      0.084410\n      0.169106\n    \n    \n      Harry S. Truman\n      0.060982\n      0.071213\n      0.065671\n      0.087225\n      0.070839\n      0.099180\n      0.058541\n      0.080372\n      0.165834\n      0.092160\n      ...\n      0.055451\n      0.064122\n      0.054537\n      0.079809\n      0.106270\n      0.081560\n      0.105554\n      0.059129\n      0.081950\n      0.079267\n    \n    \n      Herbert Hoover\n      0.103231\n      0.079289\n      0.081658\n      0.070459\n      0.111400\n      0.127897\n      0.128176\n      0.049934\n      0.146132\n      0.097870\n      ...\n      0.078349\n      0.062296\n      0.072092\n      0.110752\n      0.135878\n      0.098500\n      0.090548\n      0.076114\n      0.096796\n      0.091948\n    \n    \n      James Buchanan\n      0.074219\n      0.117172\n      0.085650\n      0.053138\n      0.213879\n      0.067012\n      0.108787\n      0.033511\n      0.050480\n      0.062365\n      ...\n      0.181783\n      0.050247\n      0.071056\n      0.138804\n      0.063212\n      0.083642\n      0.036889\n      0.143667\n      0.058985\n      0.117347\n    \n    \n      James K. Polk\n      0.108668\n      0.127383\n      0.080034\n      0.034267\n      0.083551\n      0.063491\n      0.086467\n      0.037409\n      0.050591\n      0.038777\n      ...\n      0.081692\n      0.041648\n      0.059918\n      0.124555\n      0.054302\n      0.098106\n      0.031113\n      0.069200\n      0.073058\n      0.204857\n    \n    \n      James Madison\n      0.071391\n      0.094473\n      0.062079\n      0.024049\n      0.066594\n      0.058868\n      0.067699\n      0.027369\n      0.047359\n      0.028658\n      ...\n      0.071821\n      0.037817\n      0.098455\n      0.088595\n      0.049361\n      0.056839\n      0.027100\n      0.063285\n      0.035300\n      0.113097\n    \n    \n      James Monroe\n      0.121900\n      0.130480\n      0.097276\n      0.034281\n      0.096656\n      0.073366\n      0.089977\n      0.034260\n      0.062603\n      0.045482\n      ...\n      0.076209\n      0.049221\n      0.112049\n      0.125313\n      0.073308\n      0.077521\n      0.033153\n      0.085300\n      0.060619\n      0.129747\n    \n    \n      Jimmy Carter\n      0.038933\n      0.046552\n      0.046946\n      0.240644\n      0.048041\n      0.088707\n      0.047153\n      0.162571\n      0.159608\n      0.088317\n      ...\n      0.040429\n      0.069212\n      0.045362\n      0.048941\n      0.091093\n      0.049687\n      0.229270\n      0.049615\n      0.075476\n      0.042085\n    \n    \n      John Adams\n      0.085961\n      0.099759\n      0.060527\n      0.025965\n      0.059714\n      0.045513\n      0.077210\n      0.026649\n      0.044619\n      0.024816\n      ...\n      0.053008\n      0.042880\n      0.083986\n      0.104180\n      0.036770\n      0.063388\n      0.029237\n      0.067327\n      0.038350\n      0.110925\n    \n    \n      John F. Kennedy\n      0.054274\n      0.064872\n      0.067020\n      0.145161\n      0.064716\n      0.087427\n      0.052211\n      0.091397\n      0.166903\n      0.082637\n      ...\n      0.060584\n      0.066535\n      0.050081\n      0.067712\n      0.091555\n      0.065329\n      0.145564\n      0.086820\n      0.069619\n      0.051925\n    \n    \n      John Quincy Adams\n      0.134975\n      0.163353\n      0.091024\n      0.045042\n      0.101502\n      0.086493\n      0.104866\n      0.043737\n      0.062438\n      0.040328\n      ...\n      0.084253\n      0.063036\n      0.126550\n      0.123556\n      0.071098\n      0.098951\n      0.040523\n      0.082057\n      0.072336\n      0.149257\n    \n    \n      John Tyler\n      0.131702\n      0.152333\n      0.117307\n      0.045652\n      0.123190\n      0.103976\n      0.117884\n      0.047375\n      0.062916\n      0.067584\n      ...\n      0.141787\n      0.081356\n      0.119774\n      0.153659\n      0.101667\n      0.120820\n      0.052103\n      0.113313\n      0.072481\n      0.170983\n    \n    \n      Joseph R. Biden\n      0.030122\n      0.035307\n      0.032910\n      0.345184\n      0.028887\n      0.054890\n      0.031472\n      0.220390\n      0.068947\n      0.053099\n      ...\n      0.021369\n      0.050805\n      0.034194\n      0.039431\n      0.052046\n      0.028445\n      0.306361\n      0.027510\n      0.054481\n      0.033634\n    \n    \n      Lyndon B. Johnson\n      0.045479\n      0.043621\n      0.046318\n      0.128715\n      0.035904\n      0.065005\n      0.038097\n      0.106125\n      0.100885\n      0.056544\n      ...\n      0.033196\n      0.058649\n      0.040729\n      0.053333\n      0.065493\n      0.042870\n      0.168042\n      0.054452\n      0.047279\n      0.036023\n    \n    \n      Martin van Buren\n      0.123016\n      0.189609\n      0.095427\n      0.050744\n      0.148065\n      0.078894\n      0.110916\n      0.030465\n      0.075507\n      0.051770\n      ...\n      0.114139\n      0.069005\n      0.121863\n      0.127558\n      0.084042\n      0.114678\n      0.043701\n      0.112850\n      0.086940\n      0.182542\n    \n    \n      Millard Fillmore\n      0.128602\n      0.168584\n      0.111184\n      0.040514\n      0.116195\n      0.083215\n      0.112446\n      0.046339\n      0.078221\n      0.067761\n      ...\n      0.102227\n      0.080306\n      0.119655\n      0.143147\n      0.097297\n      0.102903\n      0.044584\n      0.100449\n      0.100466\n      0.200081\n    \n    \n      Richard Nixon\n      0.039255\n      0.049510\n      0.062639\n      0.139949\n      0.042007\n      0.068414\n      0.038798\n      0.105487\n      0.136241\n      0.066019\n      ...\n      0.036937\n      0.061997\n      0.043634\n      0.054097\n      0.077172\n      0.050246\n      0.174663\n      0.044486\n      0.073935\n      0.037279\n    \n    \n      Ronald Reagan\n      0.042896\n      0.039901\n      0.044701\n      0.231009\n      0.048165\n      0.076235\n      0.040150\n      0.140081\n      0.140061\n      0.081909\n      ...\n      0.034992\n      0.052896\n      0.045076\n      0.049203\n      0.080074\n      0.039373\n      0.300796\n      0.046391\n      0.057221\n      0.038241\n    \n    \n      Rutherford B. Hayes\n      0.092012\n      0.112656\n      0.082619\n      0.035344\n      0.269502\n      0.081089\n      0.144648\n      0.040511\n      0.059357\n      0.064508\n      ...\n      1.000000\n      0.058833\n      0.074037\n      0.131257\n      0.078165\n      0.077687\n      0.046635\n      0.139910\n      0.066988\n      0.097658\n    \n    \n      Theodore Roosevelt\n      0.056717\n      0.070145\n      0.077785\n      0.069914\n      0.057292\n      0.087503\n      0.045146\n      0.077699\n      0.074973\n      0.072999\n      ...\n      0.058833\n      1.000000\n      0.050832\n      0.071917\n      0.103435\n      0.060783\n      0.066718\n      0.069393\n      0.067994\n      0.059469\n    \n    \n      Thomas Jefferson\n      0.110742\n      0.117074\n      0.093668\n      0.054143\n      0.080109\n      0.077970\n      0.063310\n      0.046509\n      0.060950\n      0.052945\n      ...\n      0.074037\n      0.050832\n      1.000000\n      0.093839\n      0.069619\n      0.051927\n      0.047307\n      0.067414\n      0.075135\n      0.109134\n    \n    \n      Ulysses S. Grant\n      0.124756\n      0.146622\n      0.127001\n      0.047495\n      0.146124\n      0.098388\n      0.179516\n      0.056097\n      0.085503\n      0.059127\n      ...\n      0.131257\n      0.071917\n      0.093839\n      1.000000\n      0.091914\n      0.121181\n      0.060122\n      0.167308\n      0.080233\n      0.137702\n    \n    \n      Warren G. Harding\n      0.082261\n      0.092593\n      0.090346\n      0.073191\n      0.080189\n      0.128878\n      0.066345\n      0.065570\n      0.107023\n      0.094722\n      ...\n      0.078165\n      0.103435\n      0.069619\n      0.091914\n      1.000000\n      0.081010\n      0.092189\n      0.096931\n      0.096031\n      0.070389\n    \n    \n      William Howard Taft\n      0.092794\n      0.113945\n      0.077516\n      0.049368\n      0.125491\n      0.092294\n      0.137828\n      0.040417\n      0.072974\n      0.054787\n      ...\n      0.077687\n      0.060783\n      0.051927\n      0.121181\n      0.081010\n      1.000000\n      0.045340\n      0.082594\n      0.087100\n      0.124097\n    \n    \n      William J. Clinton\n      0.046491\n      0.050358\n      0.049155\n      0.317297\n      0.049069\n      0.076209\n      0.045936\n      0.183823\n      0.128444\n      0.086663\n      ...\n      0.046635\n      0.066718\n      0.047307\n      0.060122\n      0.092189\n      0.045340\n      1.000000\n      0.051195\n      0.075808\n      0.044786\n    \n    \n      William McKinley\n      0.113427\n      0.102445\n      0.080710\n      0.050867\n      0.142834\n      0.074285\n      0.123470\n      0.042885\n      0.065945\n      0.063699\n      ...\n      0.139910\n      0.069393\n      0.067414\n      0.167308\n      0.096931\n      0.082594\n      0.051195\n      1.000000\n      0.067011\n      0.103954\n    \n    \n      Woodrow Wilson\n      0.077418\n      0.086139\n      0.082443\n      0.064416\n      0.076850\n      0.086982\n      0.059776\n      0.056082\n      0.104088\n      0.077462\n      ...\n      0.066988\n      0.067994\n      0.075135\n      0.080233\n      0.096031\n      0.087100\n      0.075808\n      0.067011\n      1.000000\n      0.081701\n    \n    \n      Zachary Taylor\n      0.145153\n      0.155541\n      0.092401\n      0.039508\n      0.134368\n      0.092193\n      0.132799\n      0.040505\n      0.069061\n      0.050414\n      ...\n      0.097658\n      0.059469\n      0.109134\n      0.137702\n      0.070389\n      0.124097\n      0.044786\n      0.103954\n      0.081701\n      1.000000\n    \n  \n\n43 rows √ó 43 columns\n\n\n\nHere we can see the degree of similarity of each president‚Äôs speech with each other. There is no speech with similarity more than 40%.\n\n# Compute the clusters from the similarity matrix,\n# using the Ward variance minimization algorithm\nZ = hierarchy.linkage(sim_df, 'ward')\nplt.rcParams['figure.figsize'] = [10,20]\n# Display this result as a horizontal dendrogram\na = hierarchy.dendrogram(Z,  leaf_font_size=20, labels=sim_df.index,  orientation=\"left\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-3",
    "href": "software/presidents_analysis.html#interpretation-3",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nIn the dendrogram above, the presidents seem to be clustered based on the era that they served. Two big clusters are distinct ‚Äì the green one which includes mostly recent presidents 20th and 21st century, and ‚Äì the orange one which includes presidents of the 18th and 19th century.\n\nWho was the president whose speech was the most similar to the speech of Biden in 2022?\n\n\nv = sim_df[['Joseph R. Biden']]\nv\n\n\n\n\n  \n    \n      \n      Joseph R. Biden\n    \n  \n  \n    \n      Abraham Lincoln\n      0.030122\n    \n    \n      Andrew Jackson\n      0.035307\n    \n    \n      Andrew Johnson\n      0.032910\n    \n    \n      Barack Obama\n      0.345184\n    \n    \n      Benjamin Harrison\n      0.028887\n    \n    \n      Calvin Coolidge\n      0.054890\n    \n    \n      Chester A. Arthur\n      0.031472\n    \n    \n      Donald J. Trump\n      0.220390\n    \n    \n      Dwight D. Eisenhower\n      0.068947\n    \n    \n      Franklin D. Roosevelt\n      0.053099\n    \n    \n      Franklin Pierce\n      0.030974\n    \n    \n      George Bush\n      0.227037\n    \n    \n      George W. Bush\n      0.205001\n    \n    \n      George Washington\n      0.012446\n    \n    \n      Gerald R. Ford\n      0.094785\n    \n    \n      Grover Cleveland\n      0.025062\n    \n    \n      Harry S. Truman\n      0.052615\n    \n    \n      Herbert Hoover\n      0.045206\n    \n    \n      James Buchanan\n      0.024993\n    \n    \n      James K. Polk\n      0.038190\n    \n    \n      James Madison\n      0.016895\n    \n    \n      James Monroe\n      0.019880\n    \n    \n      Jimmy Carter\n      0.198534\n    \n    \n      John Adams\n      0.016608\n    \n    \n      John F. Kennedy\n      0.094466\n    \n    \n      John Quincy Adams\n      0.024704\n    \n    \n      John Tyler\n      0.035437\n    \n    \n      Joseph R. Biden\n      1.000000\n    \n    \n      Lyndon B. Johnson\n      0.107517\n    \n    \n      Martin van Buren\n      0.030007\n    \n    \n      Millard Fillmore\n      0.024063\n    \n    \n      Richard Nixon\n      0.113265\n    \n    \n      Ronald Reagan\n      0.223184\n    \n    \n      Rutherford B. Hayes\n      0.021369\n    \n    \n      Theodore Roosevelt\n      0.050805\n    \n    \n      Thomas Jefferson\n      0.034194\n    \n    \n      Ulysses S. Grant\n      0.039431\n    \n    \n      Warren G. Harding\n      0.052046\n    \n    \n      William Howard Taft\n      0.028445\n    \n    \n      William J. Clinton\n      0.306361\n    \n    \n      William McKinley\n      0.027510\n    \n    \n      Woodrow Wilson\n      0.054481\n    \n    \n      Zachary Taylor\n      0.033634\n    \n  \n\n\n\n\n\n# This is needed to display plots in a notebook\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10,10]\n\n# Select the column corresponding to Biden's address and \nv = sim_df['Joseph R. Biden']\n\n# Sort by ascending scores\nv_sorted = v.sort_values(ascending=True)\n\n# Plot this data has a horizontal bar plot\nv_sorted.plot.barh(x='lab', y='val', rot=0).plot()\n\n# Modify the axes labels and plot title for better readability\nplt.xlabel(\"Cosine distance\")\nplt.ylabel(\"\")\nplt.title(\"Most similar to Biden's\")\n\nText(0.5, 1.0, \"Most similar to Biden's\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-4",
    "href": "software/presidents_analysis.html#interpretation-4",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nBiden‚Äôs address is most similar to that of Obama‚Äôs, follwed by Clinton and George Bush. As we can see, George Washington, John Adams and James Madison are the least similar to Biden. This can be explained by the different eras that each president lived. Speeches in 18th century are different than speeches of today. Biden‚Äôs speech similarity with Obama and Clinton makes sense also because they are all recently elected democrats.\n\nBonus points: (5 points): Develop and algorithm that can allow you to determine if the speech was given by a Democrat or by a republican.\n\nPS2: I will go over this homework on Thursday to help you think through how to solve it. You will be able to recycle a lot of code discussed.\n\nspeeches\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('public', 33),('law', 25),('service', 23),('...\n    \n    \n      1\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('industrial', 9),('work', 8),('recovery', 7)...\n    \n    \n      2\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('prosperity', 12),('production', 12),('power...\n    \n    \n      3\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('free', 16),('security', 16),('economy', 12)...\n    \n    \n      4\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('economic', 16),('development', 10),('peace'...\n    \n    \n      5\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('freedom', 12),('life', 9),('progress', 8),(...\n    \n    \n      6\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('peace', 27),('energy', 17),('war', 8),('pro...\n    \n    \n      7\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('energy', 25),('oil', 20),('tax', 17),('econ...\n    \n    \n      8\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      [('inflation', 17),('economic', 14),('tax', 13...\n    \n    \n      9\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('freedom', 20),('tax', 16),('growth', 14),('...\n    \n    \n      10\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('budget', 17),('work', 12),('hope', 10),('dr...\n    \n    \n      11\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('children', 24),('work', 21),('budget', 17),...\n    \n    \n      12\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('security', 29),('freedom', 20),('social', 1...\n    \n    \n      13\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('jobs', 32),('work', 20),('energy', 18),('fa...\n    \n    \n      14\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('tax', 15),('last', 13),('together', 13),('w...\n    \n    \n      15\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('folks', 19),('see', 15),('families', 15),('...\n    \n  \n\n\n\n\n\ndf2[[0]]\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      Address Before a Joint Session of the Congress...\n    \n    \n      1\n      Address Before a Joint Session of Congress on ...\n    \n    \n      2\n      Address Before a Joint Session of Congress on ...\n    \n    \n      3\n      Annual Message to the Congress on the State of...\n    \n    \n      4\n      Radio Address Summarizing the State of the Uni...\n    \n    \n      5\n      Fifth Annual Message | The American Presidency...\n    \n    \n      6\n      First Annual Message | The American Presidency...\n    \n    \n      7\n      First Annual Message | The American Presidency...\n    \n    \n      8\n      First Annual Message | The American Presidency...\n    \n    \n      9\n      Fifth Annual Message | The American Presidency...\n    \n    \n      10\n      Fifth Annual Message | The American Presidency...\n    \n    \n      11\n      State of the Union Message to the Congress: Ov...\n    \n    \n      12\n      State of the Union Message to the Congress on ...\n    \n    \n      13\n      Address Before a Joint Session of the Congress...\n    \n  \n\n\n\n\n\nstate_speeches = pd.read_excel('state_speeches.xlsx')\nstate_speeches\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      address\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      The Constitution requires that the President \"...\n    \n    \n      1\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      I COME before you at the opening of the Regula...\n    \n    \n      2\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      I am happy to report to this 81st Congress tha...\n    \n    \n      3\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      I appear before the Congress today to report o...\n    \n    \n      4\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      It is a pleasure to return from whence I came....\n    \n    \n      5\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      On this Hill which was my home, I am stirred b...\n    \n    \n      6\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      We meet here tonight at a time of great challe...\n    \n    \n      7\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      Twenty-six years ago, a freshman Congressman, ...\n    \n    \n      8\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      Two years ago today we had the first caucus in...\n    \n    \n      9\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      I come before you to report on the state of ou...\n    \n    \n      10\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      Mr. Speaker, Mr. President, and distinguished ...\n    \n    \n      11\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      Mr. Speaker, Mr. Vice President, Members of th...\n    \n    \n      12\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      As a new Congress gathers, all of us in the el...\n    \n    \n      13\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      Please, everybody, have a seat. Mr. Speaker, M...\n    \n    \n      14\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      The President. Mr. Speaker, Mr. Vice President...\n    \n    \n      15\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      The President. Thank you all very, very much. ...\n    \n  \n\n\n\n\n\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(state_speeches['address'], state_speeches['party'], test_size=0.3, \n                 random_state=53)\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n# Create count train and test variables\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ntfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train, y_train)\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\ncount_nb = MultinomialNB()\ncount_nb.fit(count_train, y_train)\ncount_nb_pred = count_nb.predict(count_test)\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.2\nNaiveBayes Count Score:  0.2\n\n\n\n%matplotlib inline\nfrom sklearn.metrics import plot_confusion_matrix\n\n\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred, labels=['republican', 'democrat'])\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred, labels=['republican', 'democrat'])\n\n# plot_confusion_matrix(tfidf_nb_cm, classes=['republican', 'democrat'], title=\"TF-IDF NB Confusion Matrix\")\n\n# plot_confusion_matrix(count_nb_cm, classes=['republican', 'democrat'], title=\"Count NB Confusion Matrix\", figure=1)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-5",
    "href": "software/presidents_analysis.html#interpretation-5",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nIt looks like that algorithm‚Äôs power to identify whether the speech comes from a democrat or a republican is only 20%. In this case, for the algorithm to get stronger, more speeches are necessary from both sides, and maybe more text cleaning.\n\npip install jupyterthemes\n\nRequirement already satisfied: jupyterthemes in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (0.20.0)\nRequirement already satisfied: notebook>=5.6.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (6.4.5)\nRequirement already satisfied: ipython>=5.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (7.29.0)\nRequirement already satisfied: lesscpy>=0.11.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (0.15.0)\nRequirement already satisfied: jupyter-core in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (4.8.1)\nRequirement already satisfied: matplotlib>=1.4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (3.4.3)\nRequirement already satisfied: matplotlib-inline in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\nRequirement already satisfied: backcall in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\nRequirement already satisfied: pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (2.10.0)\nRequirement already satisfied: appnope in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.20)\nRequirement already satisfied: jedi>=0.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.18.0)\nRequirement already satisfied: pexpect>4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (4.8.0)\nRequirement already satisfied: setuptools>=18.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (58.0.4)\nRequirement already satisfied: traitlets>=4.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.0)\nRequirement already satisfied: pickleshare in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\nRequirement already satisfied: decorator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=5.4.1->jupyterthemes) (0.8.2)\nRequirement already satisfied: ply in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\nRequirement already satisfied: six in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.16.0)\nRequirement already satisfied: pillow>=6.2.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (8.4.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (3.0.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\nRequirement already satisfied: numpy>=1.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.20.3)\nRequirement already satisfied: nbconvert in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.0)\nRequirement already satisfied: pyzmq>=17 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (22.2.1)\nRequirement already satisfied: ipython-genutils in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\nRequirement already satisfied: argon2-cffi in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (20.1.0)\nRequirement already satisfied: terminado>=0.8.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.9.4)\nRequirement already satisfied: tornado>=6.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1)\nRequirement already satisfied: jupyter-client>=5.3.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.12)\nRequirement already satisfied: prometheus-client in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.11.0)\nRequirement already satisfied: jinja2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.3)\nRequirement already satisfied: ipykernel in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.4.1)\nRequirement already satisfied: nbformat in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.3)\nRequirement already satisfied: Send2Trash>=1.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (1.8.0)\nRequirement already satisfied: ptyprocess>=0.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=5.4.1->jupyterthemes) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\nRequirement already satisfied: cffi>=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.14.6)\nRequirement already satisfied: pycparser in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.20)\nRequirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipykernel->notebook>=5.6.0->jupyterthemes) (1.4.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.1)\nRequirement already satisfied: testpath in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.3)\nRequirement already satisfied: jupyterlab-pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\nRequirement already satisfied: defusedxml in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.7.1)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.3)\nRequirement already satisfied: entrypoints>=0.2.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\nRequirement already satisfied: bleach in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (4.0.0)\nRequirement already satisfied: mistune<2,>=0.8.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\nRequirement already satisfied: nest-asyncio in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.5.1)\nRequirement already satisfied: async-generator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.10)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.18.0)\nRequirement already satisfied: attrs>=17.4.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (21.2.0)\n\n\nRequirement already satisfied: packaging in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (21.0)\nRequirement already satisfied: webencodings in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n!jt -l\n\nAvailable Themes: \n   chesterish\n   grade3\n   gruvboxd\n   gruvboxl\n   monokai\n   oceans16\n   onedork\n   solarizedd\n   solarizedl"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume",
    "section": "",
    "text": "Here is my up-to-date Resume\nI am currently pursuing a Master of Behavioral and Decision Sciences from the University of Pennsylvania, graduating in December 2022. Through my graduate coursework I am doing concentration in consumer analytics.\nAt UPenn, I am taking a challenging coursework including ‚ÄòStatistical Reasoning for Behavioral Science‚Äô, ‚ÄòData Science and Quantitative Modeling‚Äô, ‚ÄòJudgments and Decisions‚Äô, ‚ÄòPublic policy and Public Finance‚Äô. I was also elected as a student representative for GAPSA (Graduate and Professional Student Assembly) where I advocated for graduate student needs, and helped to organize activities that enhance the ties of the school‚Äôs graduate community.\nMy passion lies at the intersection of consumer behavior, marketing, technology, and data analysis. I believe behavioral science can inform marketing decisions to improve the consumer experience, satisfaction, and welfare. In my experience at previous positions, I developed analytical and problem-solving skills, an understanding of quantitative and qualitative methodologies, as well as strong interpersonal skills. While in school, I‚Äôve worked for non-governmental organizations and represented Cyprus in educational programs in Europe and the US.\nI am always seeking new ways to improve marketing and consumer relations using behavioral science and analytics. During school, I‚Äôve gained experience in Python, R, MS office, and SPSS. Looking forward to pursuing my interests and gaining experience in the field of consumer experience and market research."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some words I wrote\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\n\n\nThings to do in Copenhagen\n\n\nJun 9, 2022\n\n\n\n\n\nNo matching items"
  }
]