[
  {
    "objectID": "posts/2023-06-5-unproductive/index.html",
    "href": "posts/2023-06-5-unproductive/index.html",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "",
    "text": "You CAN do less! Believe in yourself\nIn our modern society, the pursuit of productivity has become a revered goal. We are constantly bombarded with messages and tips on maximizing efficiency, optimizing our time, and accomplishing more in less time. Does every second need to be exploited? Maybe. But definitely not in this article. Here you will explore the exact opposite concept of how can you make most (or ideally all) of your time useless.\nWhat if we were to challenge this notion and explore the beauty of embracing comfort and complacency? In this article, we delve into the idea of deliberately slowing down and not stopping until you reach nothingness. So, if you‚Äôre ready to break free from the relentless pursuit of productivity and discover the art of being less productive, read on. It might just be the transformative shift you need to find fulfillment in life ü§∑"
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#focus-on-the-things-you-cant-control",
    "href": "posts/2023-06-5-unproductive/index.html#focus-on-the-things-you-cant-control",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "1. Focus on the things you can‚Äôt control",
    "text": "1. Focus on the things you can‚Äôt control\nPonder over the weather, the options available in others‚Äô lives, and the potential contributions they can make to your life, among other similar topics. In other words, anything that is out of your control. Focusing on things you can‚Äôt control can create a mental and emotional burden which translates in a productivity barrier. It consumes your thoughts and energy, leaving you with less capacity to concentrate on the tasks and responsibilities that are within your sphere of influence. This can lead to a sense of stagnation and therefore inaction, as you become consumed by circumstances beyond your reach.\nSecondly, investing your time and energy in uncontrollable things can distract you from taking proactive steps toward achieving your goals. Hence, this can result in wasted time, and foster a strong inactivity momentum. Focusing on things you can not control is a great way to stay behind in many things in life.\n\n\n\nthink out of control to stay in control"
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#one-bird-two-stones",
    "href": "posts/2023-06-5-unproductive/index.html#one-bird-two-stones",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "2. One bird two stones",
    "text": "2. One bird two stones\nThis is a great one. When you use two stones to kill a bird it implies that you are devoting more effort and resources than necessary, which can result in wasted time and energy. That can make you effectively inefficient.\nYou can blend in methods from the rest of the article tips to maximize that and you will be opening yourself doors of opportunity to successfully allocate your resources inadequately. It is important that you never underestimate yourself on that, you can always waste more time and energy. If you achieve something by being productive along the way try to realize step by step how you did it in order to avoid doing so next time.\nYou can take it a step further and kill one bird with more than two stones. This ensures that you delay as much as possible as you also maximize the time and energy you can waste. While this requires more effort, you can practice and test your limits."
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#feed-your-procrastination-urge-act-on-your-impulses",
    "href": "posts/2023-06-5-unproductive/index.html#feed-your-procrastination-urge-act-on-your-impulses",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "3. Feed your procrastination urge; act on your impulses",
    "text": "3. Feed your procrastination urge; act on your impulses\nThis is crucial for achieving to maintain a status of complacency so make sure to take notes here.\n\nObserve your behavior and whenever you find yourself consuming something (in other words, sucking resources without giving anything back) you are in a good way. First, try to identify what that is. Let‚Äôs take scrolling on social media as an example. As soon as you figure out that this maintains your stagnation levels, find ways to do that more. For instance, you can do that by adopting strategies such as having your phone next to you all the time and enabling notifications from every app you have; silent mode is your enemy here. If you don‚Äôt have many distracting apps, try to download some; Click here for a guide to the ones that can keep you the most from doing productive stuff. Distracting yourself from work by acting on your impulses is a great way to establish and maintain a complacent status.\nFear of failure: Procrastination can stem from the fear of failure or the pressure of meeting high expectations. Try to identify potential things that scare you about a task and foster them even more. Write the fear down and try to make it more scary for you. The more you can internalize the fear the highest the chances of staying like a wardrobe. In case you acknowledge that mistakes and setbacks are a natural part of the learning process try to forget that. Bringing yourself down is crucial for maintaining low contribution levels to society. Moreover, whatever the case DO NOT break the task into smaller steps, as it usually makes it harder to procrastinate.\nPut off projects and deadlines as much as possible. As Oscar Wilde said: ‚ÄúNever put off till tomorrow what you can do the day after‚Äù."
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#embrace-chaos",
    "href": "posts/2023-06-5-unproductive/index.html#embrace-chaos",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "4. Embrace chaos",
    "text": "4. Embrace chaos\nNo one that wants to stay away from productivity tries to be organized. So try to learn from the masters by stopping yourself from time management activities. Tools like Google Calendar and Notion are really harmful to unproductivity. Having actionable steps and setting deadlines is even worse. Instead, try to live for the moment and embrace the adventure of the unknown. Enrich that behavior by delaying tasks and putting them off until the last minute. In general, nothing from time management principles helps you here.\nPrioritizing, as long as it is done right, is the only thing that can be helpful from the time management principles. How do you prioritize staying unproductive? Identify which tasks are the most important to get done and do them last. Especially when you work with others, you will get a lot of attention and be amazed at how fascinated people will be with your approach.\n\n\n\nI bet Rory was a procrastinator"
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#eat-sugar-and-fat",
    "href": "posts/2023-06-5-unproductive/index.html#eat-sugar-and-fat",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "5. Eat sugar and fat",
    "text": "5. Eat sugar and fat\nDiet is critical for performance. The saying ‚Äúwe are what we eat‚Äù emphasizes the idea that the food we consume has a direct impact on our physical and mental performance.\nSugar is vital for staying stagnant, the more you eat the more like a plant you feel. Consuming high amounts of sugar, especially refined sugars found in sweets and sugary drinks, can lead to energy crashes. Initially, sugar provides a quick burst of energy, so try to restrict the urge on doing stuff at that immediate moment. As long as you can keep yourself calm the first moments, a rapid drop in blood sugar levels follows, causing fatigue and a lack of focus.\nConsuming high-fat meals, particularly those high in unhealthy fats like saturated and trans fats, can leave you feeling sluggish and lethargic. These foods take longer to digest, diverting energy away from your brain and reducing mental clarity.\n\n\n\nDo not underestimate diet\n\n\nTherefore, with fatigue, lack of focus, and mental clarity, you are making it much easier for you to do less. Do not underestimate this advice, especially if you want to become a professional procrastinator.\nTo be better at this, try to include in your shopping list as many of these products as possible. They are usually cheap as well. The secret here is to keep your fridge and stock full of these fat/sugary treats so you maintain easy access to them."
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#talk-more-do-less",
    "href": "posts/2023-06-5-unproductive/index.html#talk-more-do-less",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "6. Talk more do less",
    "text": "6. Talk more do less\nThis does two main things: 1) can maintain things stagnant for longer periods of time, and 2) builds your reputation as a notorious procrastinator.\nOn the first point, talking excessively without taking action can solidify stagnation. Time spent in endless discussions or overplanning without execution ensures that resources like time, and energy can be drained effectively not only for you but also for the people that are around you. Taking action is dangerous here because it allows us to test our ideas, refine our strategies, and make necessary adjustments to achieve tangible outcomes. This is something you want to avoid at all costs! Bonus tip: Try to drag as many people to listen to you. One of the most popular and effective ways to do so is to make any email a meeting. In that way, you ensure that not only you does nothing tangible but also your team members.\nOn the second point, by talking without doing you ensure that you are not only doing nothing but also getting popular about it. This strategy is called by professionals ‚Äútalking the walk‚Äù and it is a renowned method to brand yourself as a non-doer.\n\n\n\n(Meeting + talking) &gt; (Email + doing)"
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#challenge-yourself-carefully",
    "href": "posts/2023-06-5-unproductive/index.html#challenge-yourself-carefully",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "7. Challenge yourself carefully",
    "text": "7. Challenge yourself carefully\nThis is tricky because as you challenge yourself in life you usually get better at things and then you set higher standards. Challenge yourself only in tasks that are carefully tailored in making you as inefficient as possible in life. Being focused and picky on what you challenge yourself at is crucial for maintaining the complacent status that you work so hard to establish."
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#suppress-your-motivation",
    "href": "posts/2023-06-5-unproductive/index.html#suppress-your-motivation",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "8. Suppress your motivation",
    "text": "8. Suppress your motivation\nWhen you are motivated about something, it does not let you procrastinate much. Try to make the subject you are motivated about uninteresting. One way you can do that is to isolate your connection with it or make it difficult to get to. For instance, if you are dedicated to going to the gym you can start by canceling your subscription. Of course, your motivated self might not accept it that easily so you might have to stay registered. If you face this kind of resistance try to subscribe somewhere far from you that it is difficult to get at.\n\n\n\nput barriers in productive motivation\n\n\nBonus tip: the more expensive the place you subscribe the better is for maintaining low productivity status. That is because you use more of your resources on something that does absolutely nothing.\nIf your motivation resistance is strong, and you are pushed to work out at home, double down resistance by following point 5. You never go wrong by eating trash when it comes to combatting motivation. If your motivation resistance is extra strong do not worry, there are other ways to derail it. That is by trying to be a perfectionist. Take the example of Howard Hughes and you will realize the importance of it.\nThe Spruce Goose, officially known as the Hughes H-4 Hercules, was a massive flying boat aircraft designed and built by Howard Hughes and his team during World War II. It was meant to serve as a transport aircraft for troops and supplies, and it was constructed primarily from wood due to wartime restrictions on aluminum. In 1942, the US government allocated to Hughes a budget of $18 million to build this aircraft for WW2.\nWhile the original contract called for a two-year development period, it took over five years for the aircraft to be completed. Hughes was a perfectionist, and his relentless pursuit of an ideal design led to numerous delays and budget overruns. By the time it was finally built, the war had already ended, rendering the original purpose of the Spruce Goose obsolete.\nThe Spruce Goose made only one short flight in 1947, piloted by Hughes himself. After that, it never flew again. The massive aircraft became a symbol of Hughes's perfectionism and the significant results you get when you prioritize perfection over practicality.\nThe Spruce Goose example demonstrates that the drive for perfection can lead to significant delays, increased costs, and inevitably to great success of being unproductive. So if you start working out by pure motivation for instance, try to perfect a technique of exercise, i.e. chest press, by using 0 weights and trying that (preferably without equipment) 1000+ times. Your unproductiveness will skyrocket."
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#find-something-that-makes-you-miserable",
    "href": "posts/2023-06-5-unproductive/index.html#find-something-that-makes-you-miserable",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "9. Find something that makes you miserable",
    "text": "9. Find something that makes you miserable\nMisery tends to foster a specific mindset and self-talk, where you focus on your problems, limitations, and failures. This thinking pattern solidifies an attitude of not doing anything and leads to self-doubt about you being able to do stuff. If you are able to find that in your life, you automatically got into an express line to get fewer things done."
  },
  {
    "objectID": "posts/2023-06-5-unproductive/index.html#conclusion",
    "href": "posts/2023-06-5-unproductive/index.html#conclusion",
    "title": "How to Be Unproductive: 9 Tips for Doing Less",
    "section": "Conclusion",
    "text": "Conclusion\nTry to celebrate times of complacency with long-lasting parties which drag more resources and don‚Äôt give something back to society. You will know when you become really good at procrastination when you realize that you are not even getting the parties done.\nAt that point you can spice things up by doing different unproductive stuff you did not have the chance to do before such as watch movies, stay up all night with no reason, go clubbing all day, play a ton of videogames, spend money on dog food without having a pet and much more! Please feel free to include more stuff that are not productive in the comments below, interested to hear your ideas!\nPlease clap üëè on medium if you find this post helpful or inspiring:)"
  },
  {
    "objectID": "posts/2023-07-15-colorfood/index.html",
    "href": "posts/2023-07-15-colorfood/index.html",
    "title": "Experiencing Colorado, One Bite at a Time¬†üçΩ",
    "section": "",
    "text": "Food environment (min $/max¬†$$$$)\n\n\nBreakfast/Dessert\n\n\n\nSam‚Äôs https://samsno3.com/\n\n\nSam‚Äôs ‚Ññ3 Glendale $$‚Ää‚Äî‚ÄäDenver‚Äôs favorite brunch place. They have 3 restaurants across town, and all three of them have 4.4 stars minimum and more than 4,000 reviews on google. Sam‚Äôs ‚Ññ3 was founded in 1927 by Sam Armatas, an immigrant from Greece who sought to provide hearty meals to Denver locals and visitors. The restaurant has a rich history and has been a part of Denver‚Äôs dining scene for over 90 years, making it one of the city‚Äôs oldest restaurants.\nHuge portions and a ton of options. The only downside is that the menu is without pictures so you can‚Äôt know what you order unless you see other people‚Äôs dishes or try to find pictures from Yelp and google maps.\nSnooze an A.M. Eatery $$‚Ää‚Äî‚ÄäAnother famous brunch place. This is a chain that has in multiple locations across the state (13 locations, mostly in Denver) and the country‚Ää‚Äî‚Ääin Arizona, California, Colorado, Georgia, Kansas, Missouri, Nevada, North Carolina, Tennessee, and Texas. They have all the main brunch options you need, from eggs and sausages to pancakes and waffles. They have better looking menu than Sam‚Äôs but fewer choices. Before you go make sure you join in their benefits rewards app so next time you go you get a $10 off from your next meal. If you subscribe, it needs a day to get in their system.\nCrepes n‚Äô Crepes $$‚Äî This European-style restaurant offers an extensive menu of both sweet and savory crepes, catering to a variety of tastes and preferences. From classic combinations like ham and cheese to unique creations featuring Nutella, fresh fruits, and more, there is a crepe for every craving. Crepes n Crepes was founded in 2001 by Julien and Peggy Maingot, a husband-and-wife team who shared a passion for French cuisine and the art of making crepes.\n\n\n\nCrepes n‚Äô¬†Crepes\n\n\n\n\nLunch/Dinner\nBeau Jo‚Äôs Pizza $$‚Ää‚Äî‚ÄäThis one is in Idaho Springs and the most famous pizza in Colorado. Beau Jo‚Äôs Pizza, famous for its distinctive Colorado-style mountain pizzas, traces its roots back to the small mountain town of Idaho Springs, Colorado. In 1973, a man named Chip Bair, who‚Äôd migrated to Colorado from the Midwest, founded Beau Jo‚Äôs.\nThe crust is so thick that it‚Äôs common practice at Beau Jo‚Äôs (actually all over Colorado!) to drizzle honey on it, turning the pizza crust into a delightful dessert. This unique style of pizza was quickly embraced as ‚Äúmountain pie,‚Äù a fitting name given its origins and hearty nature.\nThe first Beau Jo‚Äôs restaurant was a modest establishment, but it soon gained popularity, particularly among those traveling to and from the ski slopes. Beau Jo‚Äôs pizza quickly became a Colorado tradition, a must-visit for locals and tourists alike.\n\n\n\n14 pound pizza at Beau¬†Jo‚Äôs\n\n\nBeau Jo‚Äôs Pizza is known for its ‚ÄúBeau Jo‚Äôs Challenge.‚Äù This eating challenge involves consuming a 14-pound pizza (the ‚ÄúGrand Sicilian‚Äù) within one hour. If you can finish this massive pizza within the time limit, the pizza is free, and you also win $100 in cash. This challenge is popular among competitive eaters and those who enjoy a good food challenge. The waiter there told me that only 3% of people who try it manage to eat all of it.\nWatch one of the successful attempts here: https://www.youtube.com/watch?v=uzqqBw1kKbI\nLittle India $$‚Ää‚Äî‚ÄäIt seems they are two different Little India‚Äôs, one that has multiple locations and one other that is in Lakewood area. Both offer almost the same items and both are awesome. Little India offers a diverse menu featuring traditional Indian favorites. If you like spicy food or flavor variety, this is a must-go-to location. Last time we ordered with a colleague Saag with tofu, and the Chicken Korma, both 4/5 spicy (with 5 to be Indian Spicy), and I loved it.\nD‚ÄôCorazon Mexican Restaurant $‚Ää‚Äî‚ÄäOnce you get in it feels like you are in Mexico. Big portions, friendly staff, and tasty food. Free crisps serving starter is a plus.\nMethod Coffee Roasters $$‚Ää‚Äî‚ÄäGreat place to work/study. Locals come here to work and read books. The atmosphere is relaxing, chill music, and plugs everywhere for your laptop/devices.\nChef Zorba‚Äôs $$‚Ää‚Äî‚ÄäLocals love this place. While it is a popular dine-in restaurant in the area, families and friend groups come here even for brunch and breakfast. If you are wondering about the ‚Äògreekness‚Äô of the place, menu is greek, gyros is not as good. Menu is pretty compatible with authentic Greek food and portions are satisfactory.\nDownpours Coffee $‚Ää‚Äî‚ÄäMy favorite neighborhood cafe. It is my favorite because it is quiet enough that you can study, but also social because many locals go there, especially during the weekends. While it‚Äôs open only from 7 am to 1 pm every day, it is a place that has space to sit inside and outside with a beautiful street yard that also has charger plugs!\nCasa Bonita $$‚Ää‚Äî‚ÄäCasa Bonita, popular among locals Denver restaurant, reopened in May 2023 under the ownership of the South Park creators. This unique dining destination combines delicious Mexican cuisine with thrilling entertainment, including cliff divers and live shows. People were going there just for the cliff diver entertainment and cocktails because the food used to be terrible. However, a new chef is there right now determined to make it right. South Park season 7, episode 11 features the restaurant.\n\n\n\nCasa Bonita aka ‚ÄúThe South Park Restaurant‚Äù\n\n\nAvanti $$‚Ää‚Äî‚ÄäIf you are in a group of people where everybody feels like eating something different, this is the place for you. It is a place that has 10 restaurants of different cuisine varying from pizzas to noodles to Mexican food and fried chicken. I had the noodles from the Thai place and I have to say that the portions are pretty small for the price. Same with the fried chicken. However, the pizzas are a proper size. You can see it has a ton of reviews, while it‚Äôs nice for me it was a bit overrated.\n54Thirty Rooftop $$‚Ää‚Äî‚Ää54Thirty Rooftop is a rooftop bar situated in the city of Denver. Specifically, it is located on the 20th floor of the Le M√©ridien Hotel in Denver and it is the highest open-air rooftop bar in the city. From there you can see views of the city skyline and the Rocky Mountains.\nMehak India‚Äôs Aroma $$‚Ää‚Äî‚ÄäFlavourful Indian food (Punjabi in particular). There is a variety of options with different spices. If you do not like spices, no worries; you can choose from 1 to 5 (five is Indian spicy). Worths to give it a shot.\nBrooklyn‚Äôs At Ball Arena $$‚Ää‚Äî‚ÄäGreat bar to watch an NBA game. While tickets to basketball games can be expensive, you can still live the atmosphere of a game by going to Brooklyn‚Äôs. It is a bar right next to the Ball Arena stadium and the fan vibes there are the closest you can get to a basketball match. Check out the vibes from the Brooklyn‚Äôs during a game by clicking here.\nSnarf‚Äôs $‚Ää‚Äî‚ÄäA traditional Colorado sandwich place. My accounting manager who lives in Denver says ‚ÄúEverything is good at Snarfs! If you are from Colorado and you never had Snarfs people will start questioning you‚Äù. They have locations everywhere.\n\nThe story of Snarf‚Äôs began in 1996 when its founder, Jimmy ‚ÄúSnarf‚Äù Seidel, decided he wanted to create the world‚Äôs finest sandwich.\nTired of the corporate grind, Jimmy left his job as a broker in Chicago and moved to Boulder, Colorado. His quest was simple‚Ää‚Äî‚Ääto find the best sandwich out there. But his search was fruitless. So, he decided to take matters into his own hands and create the sandwich he was craving.\nHe found an old house in Boulder, converted it into the first Snarf‚Äôs location, and got to work creating the world‚Äôs best sandwich. His creative creations, coupled with a fun and quirky ambiance, quickly won over the residents of Boulder. Today, Snarf‚Äôs has grown to include multiple locations across Colorado, as well as Missouri, Illinois, and Texas.\nRead more about Snarf‚Äôs history here: https://www.eatsnarfs.com/about#:~:text=It%20all%20began%20with%20one,to%20as%20%22The%20Shack%22.\n\nHuHot Mongolian Grill$$‚Ää‚Äî‚ÄäHealthy all-you-can-eat food! Difficult to say no to that. Once you go inside, you can pick a noodle or a rice base and then include your own selection of ingredients in your bowl ~ or bowls, literally is all you can eat. Then you give the bowl to the cook who makes the food in front of you. HuHot is a chain in the US that has over 70 locations in more than 18 states.\n\n\nConclusion\nPlease clap üëè post helpful or inspiring. You can read this or see more articles and tips on medium or on my website here."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html",
    "href": "posts/2023-03-15-cyprus/index.html",
    "title": "Experience Cyprus Like a Local!",
    "section": "",
    "text": "Most viewed video about Cyprus üëà\nAre you planning a trip to the island of goddess Aphrodite? From beautiful beaches to majestic mountains, Cyprus is a place where you can do a wide variety of activities. In this article, you will find details such as currency, transportation, restaurant recommendations, cultural visits, and cool activities you can do. Cyprus is sunny more than 300 days a year, however, remember to check the weather before arriving."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#currency",
    "href": "posts/2023-03-15-cyprus/index.html#currency",
    "title": "Experience Cyprus Like a Local!",
    "section": "CURRENCY",
    "text": "CURRENCY\nThe main currency is Euro. Live currency converter. They accept card payments everywhere. If you have a foreign currency, with Revolut you should be fine. Keep in mind that the north of the island (37%) is occupied by Turkey since 1974, therefore, Turkish Lira is what was adopted and is still in use in that part of the island."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#transportation",
    "href": "posts/2023-03-15-cyprus/index.html#transportation",
    "title": "Experience Cyprus Like a Local!",
    "section": "TRANSPORTATION",
    "text": "TRANSPORTATION\nThe most efficient way to transport in Cyprus as a tourist would be to rent a car. This offers the flexibility to explore the island at your own pace, and reach remote destinations that may not be accessible by public transportation. Remember, people left-hand side of the road, like in the UK. It is recommended to book your rental car in advance, especially during peak tourist season, to ensure availability and secure a better rate.\nTaxis and buses are also options; While you might not be as flexible as with a car, buses at least can be cheaper ‚Äî routes and schedules."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#beach",
    "href": "posts/2023-03-15-cyprus/index.html#beach",
    "title": "Experience Cyprus Like a Local!",
    "section": "BEACH",
    "text": "BEACH\nThere are different kinds of beaches in Cyprus. The most popular among Cypriots are in Protaras and Ayia Napa area. Here are some options for you:\nMarcelo (wavy with some rocks. More popular for students).\nFig tree bay (sand only, with a more shallow surface. More popular for families).\nYianna Marie Beach (This is something hybrid between the two prementioned beaches and one of the top preferences of locals. Here there is sand, not wavy with occasional rocks.)\nFor nice scenery, you can visit Cape Greco, where you can see sea caves. There are also people that are swimming in the crystal clear waters there, and some risk-loving people that dive from the Cape Greco cliff in the sea (not recommended).\n\n\n\nCape Greco"
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#nature",
    "href": "posts/2023-03-15-cyprus/index.html#nature",
    "title": "Experience Cyprus Like a Local!",
    "section": "NATURE",
    "text": "NATURE\nFor greenery and nature, you can go to Troodos National Forest Park around Mount Olympus. It is an area of outstanding natural beauty, suitable for activities such as hiking, winter skiing, biking, nature study, camping, and picnics. The highest point is Chionistra (1,952 m) and the lowest is Moni forest (700m).\n\n\n\nXyliatos Dam\n\n\nBeautiful Villages to visit:\nKalopanagiotis ‚Äì It is known as the ornament of Marathasa Valley, a village that kept its old character while it evolved. With an old Unesco heritage church from the Venetian era, walking trails in the forest, and restaurants around, it attracts thousands of visitors every year. A lot of locals go there to relax, especially during summer.\nOmodos ‚Äì Beautiful village with traditional scenery and a war museum.\nAgros ‚Äì A cultural village. It is reknown for its Rose Factory, its infamous traditional sweets store ‚Äúta Glyka tis Nikis‚Äù, and the traditional Cyprus sausage that is produced there.\n\n\n\nKalopanagiotis"
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#activities",
    "href": "posts/2023-03-15-cyprus/index.html#activities",
    "title": "Experience Cyprus Like a Local!",
    "section": "ACTIVITIES",
    "text": "ACTIVITIES\nüö≤ Biking Nicosia‚Äôs Cycling route park, Pedieos linear park. It goes from Anthoupoli (Nicosia suburb) to downtown. Arguably, one of the most underrated experiences, it is a 14km(8.6 miles) ride with a lot of greenery, city nature, runners, and cats. You can also walk here on its pedestrian side.\nPedieos Linear Park - click here üëà\nüé≥ Bowling at Kykkos Bowling. It is the only bowling place in Nicosia. With $6/game you can have fun while showing off your talent.\n‚öΩ Football. If you are five people, you can call in advance one of the following football futsal fields and they will find you opponents to play against; Paeek, THOI (those are two good places I know; located in Nicosia).\nüî´ Paintball. While it is more expensive than the rest of the activities ($25/person) is a fun group activity ‚Äî min 8 people.\nüíª Remote working / Reading. Yfantourgeio is a great place to work remotely from. It is a quiet co-working space and in the heart of Nicosia. Another quiet place you can go is the library of the University of Cyprus. Moreover, you can go to one of the multiple coffee shops in Cyprus Nero, Costa Coffee, and Gloria Jeans (less quiet but more social)."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#food-min-max",
    "href": "posts/2023-03-15-cyprus/index.html#food-min-max",
    "title": "Experience Cyprus Like a Local!",
    "section": "FOOD min $ /max $$$$",
    "text": "FOOD min $ /max $$$$\nü•ñ Zorbas $ The most famous bakery in Cyprus where any local goes to. They are even operating in New York. For a quick breakfast, a sweet, or a traditional cooked meal for lunch or dinner, this is your go-to place. They have many locations in different cities in Cyprus.\n\n\n\nZorbas bakeries\n\n\nüçñ ETHA Egkomis $$ Traditional Cypriot food. Highly recommended.\nüçñ Ayia Anna Tavern $ One of the most traditional you can find. It is in a small village in the suburbs, it serves any kind of Cypriot food, and the waiters are dressed in the traditional Cypriot uniforms back in the 1900s (at least the time I went in 2019 they were like this).\nüçñ Zannetos $$ Traditional Cypriot food. Ideal for meze.\nüçñ Piatsa Gourounaki $ Traditional Cypriot food; fast food that it is conveniently located downtown.\nüçñ To anamma $ Traditional Cypriot food. Fast food traditional food also downtown. Has a beautiful inside sitting area.\nüçî The Garrison Bar $$ Burger bar with a Peaky Blinders theme and menu.\nüçî Babylon Bar $$ A local bar with a billiard and nice atmosphere.\nüçî Moondogs Bar $$ A local sports bar with billiard and a nice atmosphere.\nüç± China Spice $$$ Chinese restaurant. Great place for dinner, good value for money for its fanciness, with a variety of dishes. My favorite Chinese in Cyprus, especially on Tuesdays, there is an all-you-can-eat buffet for $25/person.\nüçï Alfa Pizza $ Cypriot fast food pizza chain.\nü•ô Avo $ If you are a backpacker this is your place. It is the cheapest place you can get food in Nicosia. While you can get all sorts of food from souvlaki to pizza, Armenian food and lahmacun is their specialty.\nü•û Edem‚Äôs Yard $$ Breakfast/brunch in Larnaca‚Äôs palm trees area.\nü•û Hari‚Äôs Creperie Special crepe and waffle combinations in good prices. Check it out.\nüç¶ Papafilippou Ice-Cream $ The most famous local ice-cream. Enjoy your ice cream while playing arcade games. You can find other Papafilipou branches in Cyprus, and you can also find Papafilipou ice-cream in grocery stores and any kiosk.\nüç¶ Heraclis Ice-Cream $ The oldest ice-cream shop in Cyprus. It opened in 1939, from a local man named Heraclis during the British occupation before Cyprus become independent.\nHopefully, this article helps you make a memorable and satisfactory Cyprus experience. Please clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html",
    "href": "posts/2023-03-25-nepal /index.html",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "",
    "text": "Kathmandu\nIf you‚Äôre thinking of visiting Nepal, there are several things you need to know to make the most out of your trip. Nepal is a destination known for its natural beauty, cultural richness, and friendly people. Whether you‚Äôre seeking adventure or a peaceful escape, Nepal has something for everyone.\nThis article provides a valuable firsthand account of a traveler‚Äôs experiences in Nepal. Here you can find practical advice and tips on currency exchange, transportation, and sightseeing in Kathmandu and Pokhara, as well as recommendations on where to eat and what to do. The article also highlights the friendliness and safety of the Nepalese people, which is important information for anyone considering a trip to Nepal.\nWhen planning your trip, there are two options to consider:\nVisiting Nepal with a travel agency can offer many advantages such as access to local knowledge and expertise, organized activities and transportation, and peace of mind. Especially if you are going for high-altitude hikes, a guide and a porter to carry your bags can be very helpful. On the other hand, planning a trip alone gives you more flexibility, freedom, and the opportunity to design a more personalized itinerary. I would recommend that option if your main plan is to explore Nepal‚Äôs culture and interact with locals. You save a lot and you have no one to deal with. Also, it is completely safe to make the trip to Nepal alone!\nIf you are going with a tourist agency, Himalayan Wonders is extremely organized that takes care of the whole trip for you. They answer fast when you email them. If you are price sensitive, they have a sister company, Adventurehero, which does the same trips but for cheaper prices ‚Äî less fancy hotels and jeep rides instead of internal flights. When organizing your trip with an agency, it can also be stressful if you leave everything for the last moment so consider finalizing the booking some days in advance.\nWhen I visited Nepal with a friend in February 2023, we went without a guide. While doing the proper research can be time-consuming, hopefully, this article saves you some time."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#tasks-before-visiting",
    "href": "posts/2023-03-25-nepal /index.html#tasks-before-visiting",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "TASKS BEFORE VISITING",
    "text": "TASKS BEFORE VISITING\nüíâHealth: Consult your doctor before going. Typically, there are suggested vaccines such as rabies, typhoid, hepatitis, and tetanus, but recommendations vary depending on each traveler. Malaria is uncommon there but also depends on the cities that you are going to. Usually, malaria cases are reported in the lower west side of Nepal e.g. Ilam. It is recommended to have international health insurance that covers you before going. I got mine from Revolut by upgrading from free to premium paying $8.99/month. If you don‚Äôt have revolut, feel free to create an account through my referral.\nIf you are going for a big hike i.e.¬†Everest base camp, it is recommended to get an insurance that covers evacuation with a helicopter, because if anything happens to you while in the mountain and you are uninsured, the bill you pay will be over $2500! I did not need it, but in case you do, here is the email of the guy that I asked for information from (this advice applies to you if you are from Cyprus); giorgostheodorou4@gmail.com ; He works at Metlife, and when I shared him a trip schedule that was including everest, he drafted me a $90 insurance.\nüç£ Food: If you are considering going hiking, bring with you some protein bars that will keep you during the day. It is not certain that you will find proper restaurants in the mountain while hiking. The following comes from my doctor‚Äôs advice‚Äî In general, eat only well-cooked food to minimize the risk of food poisoning. When it comes to water, bottled drinks are your go-to. Avoid using ice cubes or drinking from glasses. When brushing your teeth use bottled water and when shower with your mouth closed.\nüõÇ Visa: You can do it when you arrive there at the airport (highly recommended) or apply for it in advance. You will need two recent passport-size photos and a fee. There are three options: Visa for 15 days ($30), for 30 days ($50), and for 90 days ($125).\nü•æ Hiking: In Nepal, you need a permit to go hiking called TIMS, which you can do there. You will need two recent passport-size photos for this. In general, nature in Nepal can be dangerous and you can be lost if you try to do big hikes without taking someone experienced with you.\n\n\n\nBackpack essentials"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#currency-simcard",
    "href": "posts/2023-03-25-nepal /index.html#currency-simcard",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "CURRENCY & SIMCARD",
    "text": "CURRENCY & SIMCARD\nCurrency: Nepalese rupees. Do the conversion here before you buy something.\nYou can exchange your money at the airport (don‚Äôt do it before you pass the passport check), afterwards you find better rates. You also need a Sim card. You can get that also at the airport, after you arrive, the provider Ncell will have a brunch near the exit/entrance of the airport and you can get a sim card from there. I got mine with 25GB internet data and it was around 600 Rupees."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#tips-from-8-day-trip",
    "href": "posts/2023-03-25-nepal /index.html#tips-from-8-day-trip",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "tips from 8-day trip",
    "text": "tips from 8-day trip"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-1-arrival-in-kathmandu",
    "href": "posts/2023-03-25-nepal /index.html#day-1-arrival-in-kathmandu",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 1: Arrival in Kathmandu",
    "text": "DAY 1: Arrival in Kathmandu\nArrived in Kathmandu at 6 pm in the evening, and met with my friend in the airport. It‚Äôs important to note that when it comes to transportation, you should avoid anyone that asks to give you a ride beside the taxis with green labels, which are the official ones.\nWe took a taxi (1000 rupees) from the airport to our hotel (Flock Hostel Kathmandu) and we met there our potential trip guide. We were planning to go at Pokhara and stay there for a couple of nights, do some activities and some hikes around the area. The guide was not clear about his pricings during our negotiation and he asked us to pay for his expenses as well on top of his fee, so we decided to do the trip by ourselves.\nAt night we went out to get some dinner. We ended up at Roadhouse cafe and we had some oven pizza. It was a central place with nice atmoshpere inside, mostly tourists were in it though. After we returned home. The city, despite the functionally chaotic driving (i.e.¬†there are no traffic lights) and the fact that some people will try to sell you drugs, is safe; no robberies or violence. Honestly, when it comes to security and people, I felt safer than I feel in Philadelphia.\nRoadhouse cafe seems to be a local chain as we found couple of them in Pokhara and Kathmandu."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-2-exploring-kathmandu",
    "href": "posts/2023-03-25-nepal /index.html#day-2-exploring-kathmandu",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 2: Exploring Kathmandu",
    "text": "DAY 2: Exploring Kathmandu\nWoke up early and went to our respective embassies; the French embassy, and the consulate of Cyprus. French embassy asked from my friend to sign a document with his address in it; they did not allow us to enter. While the consulate of Cyprus was more accessible, there are no Cypriots working there. It is operated by locals, and they mostly issue tourist and student visas for Nepalese people that visit Cyprus.\nNext destination, Gaia Restro & Coffee Shop for breakfast. It is a relaxed place with open space to sit outside and nice food.\nAfterwards, we went to buy bus tickets for Pokhara, the place we were about to go the next day. We bought the tickets from East and West International Tours and Travels Ltd (front). Normal Bus was 1400Rupees, and the Luxury/more comfortable one was 1800Rupees ‚Äî normal prices, not overcharge. Luxury bus is recommended since the road to Kathmandu is bumpy. Normally it takes around 7‚Äì8 hours the trip from Kathmandu to Pokhara. As long as you have your ticket, make sure to be at the tourist bus stop early in the morning half an hour before your bus.\nThen we started walking towards the Monkey Temple. From there that we bought the tickets it was a 35-minute walk, however it was nice as we were able to stop along the way to shops and buy clothes and souvenirs.\nThe monkey temple is awesome because there is more trees, many monkeys, and a beautiful view of the city. The ticket to get in was 200 Rupees per person. Try not to feed the monkeys and keep your stuff on you cause they are sneaky animals! My advice; enjoy the view, take some pictures, and go continue exploring the city.\nNext, on the way to Durbar Square, we found a local football court, the one that is visible from the monkey temple, where many kids were playing football. It looked like they frequently play there. Playing football with the kids was for me the best experience of the day. Football is a universal sport that makes everyone forget life problems and enjoy the game and in that moment that was the definition we lived in.\nDurbar Square was a positive surprise for us. There we found many restaurants, coffee shops, and there are no cars in that area so many people walk there. Nearby you can also find the temple of Kumari Ghar.\nKumari Ghar is a historical place of worship for Hindus and Bhuddists.\nNote that some people try to scam tourists and ask them to pay to go to the square. We walked through the square and headed towards Civil Mall, one of the country‚Äôs biggest malls. Besides many shops, cafes, and restaurants (including the KKFC local fried chicken chain), on the 6th floor, you can find all sorts of games such as billiards, ping pong, bowling, and basketball throws. Great place to hang with friends. Before going home, we stopped at pizza hut for dinner."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-3-traveling-to-pokhara",
    "href": "posts/2023-03-25-nepal /index.html#day-3-traveling-to-pokhara",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 3: Traveling to Pokhara",
    "text": "DAY 3: Traveling to Pokhara\nSince the bus was leaving at 7 am, you have to be at the Tourist bus station half an hour earlier. The bus ride is bumpy but doable. The driver makes 3 stops until Pokhara. At the stops, there are restaurants and restrooms. We ate at one of the road restaurants just rice and noodles, since we are used to different diet we did not want to risk eating meat and uncooked vegetables.\nWe arrived at Pokhara at 5 pm, and then took a taxi (we also got the number of the driver for future rides) from the tourist bus park to our hotel, Hotel Middle Path and Spa. It is a very good hotel with a lot of facilities such as a gym, heated pool, spa, and sauna. We visited in February and a twin room was $25/night. Since we liked the hotel, we extended our accommodation there for two more nights. Then for dinner we went at Rice Garden Restaurant, a central restaurant in the main street; while it was not amazing, it was good enough. We had typical noodles and rice.\nMiddle Path & Spa Hotel: The best value for money hotel I have ever been!"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-4-sunrise-and-tibetan-refugee-camp",
    "href": "posts/2023-03-25-nepal /index.html#day-4-sunrise-and-tibetan-refugee-camp",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 4: Sunrise and Tibetan Refugee Camp",
    "text": "DAY 4: Sunrise and Tibetan Refugee Camp\nOur day started early. At 5.40 am, yesterday‚Äôs taxi driver picked us up from the hotel, and drove us to Sarangkot to see the sunrise. The view from there was amazing. You could see all the big hikes of the area such as Dhaulagiri (8167m), Machhapuchhare (6993m) and, Annapurna II (8,091m), and a panoramic view of Pokhara. Then the driver took us back to the hotel where we slept for a while. After, we went for breakfast at White Rabbit cafe nearby Pokhara‚Äôs lake. It was the best breakfast we had in Nepal so far! We also had a great lake view and we saw a football tournament going on next to the lake. So after our breakfast, we wanted to get closer to watch and play with them. The tournament seemed very well organized and was taken seriously by the participants. We found out that the prize for the winning team was $2000. We watched a very entertaining game with last-minute goals and a penalty shootout. Then, we borrowed a ball from the court and we played with the kids in the field. It was a beautiful experience.\nSubsequently, we left to buy a return bus ticket from Pokhara to Kathmandu. We asked a local tourist agency which sold us the bus ticket for 1600 rupees. My friend was asking to rent a motorcycle for the next week. He negotiated with the guy there and he promised him a fee if he was going to help him find a good motorcycle at a good price. Finally, he got an offer to rent it for $35/day. We left satisfied from there.\nIf you‚Äôre planning on renting a motorcycle in Nepal, there are a few important tips to keep in mind according to my friend. First, make sure to choose a trustworthy rental company and inspect the motorcycle carefully before renting it. You can do that by negotiating with multiple rental places. When it comes to places to go, Lower Mustang is a great option with recommended stops in Kalopani, Jomson, Kagbeni, and Muktinath, each for a day. Upper Mustang is another great option with a recommended 4-day trip from Kagbeni to the Tibetan border, including a night in Ghami and the other nights in the forbidden kingdom of Lo. Remember that you will need to obtain a permit to enter these regions, which costs 500‚Ç¨. While it‚Äôs possible to complete this itinerary in 7 days, it‚Äôs recommended to allow for a total of 10 days to enjoy the trip at a more relaxing pace.\nIn the afternoon, we took a taxi for Tibetan Refugee Camp.\nThe Tibetan Refugee Camp in Pokhara, Nepal, is home to thousands of Tibetan refugees who fled from their homeland in Tibet due to political and religious persecution by the Chinese government. After the Chinese takeover of Tibet in 1959, the 14th Dalai Lama and many Tibetans fled to India and Nepal, where they established a number of refugee settlements, including the camp in Pokhara. The camp in Pokhara was established in the 1960s and has since then provided a home and a sense of community for Tibetan refugees. The camp is run by the Tibetan Refugee Welfare Office and provides basic facilities, such as housing, schools, medical clinics, and workshops, to its residents.\nDespite the challenges Tibetan refugees face, such as limited resources and difficult living conditions, the Tibetan refugees in Pokhara have maintained their rich cultural heritage and traditions. They have established monasteries, schools, and community centers, where they can practice their religion, preserve their language, and pass their traditions down to future generations."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-5-hiking-to-australian-base-camp",
    "href": "posts/2023-03-25-nepal /index.html#day-5-hiking-to-australian-base-camp",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 5: Hiking to Australian Base Camp",
    "text": "DAY 5: Hiking to Australian Base Camp\nEarly morning we packed our stuff and went for breakfast at Crown restro and lounge cafe. It did not meet our expectations. They brought us different order, then they delayed bringing us our normal one, and at the end they mistakenly charged us 600Rupees more; all of this took us an hour.\nThen we called our taxi driver to drive us from our hotel to Phedi (drive costed 1500rupees) to start our hike. We passed through Dhampus and end up in Australian Base Camp. To reach Dhampus took us 1 hour. Dhampus to Australian camp distance is 4.7 kilometers / 2.9 miles. It takes 2 to 3 hours to reach the Australian camp passing through Dhampus Jungle. On Dhampus we found two travelers, Todd and Ruth. We briefly talked, and we exchanged Instagram accounts.\nWe reached at Australian Base Camp at 4 pm. There there are two options you can stay at 1) Angels Guest house hotel, 2) Hotel Gurans. We stayed at Hotel Gurans. Even if you book one in advance, the people there are flexible with canceling last minute. So when you go there, make sure to check and negotiate prices between the two. Both are very similar; the main differences are room availability, room view, and price."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-6-enjoying-the-himalayas-from-our-hotel",
    "href": "posts/2023-03-25-nepal /index.html#day-6-enjoying-the-himalayas-from-our-hotel",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 6: Enjoying the Himalayas from our hotel",
    "text": "DAY 6: Enjoying the Himalayas from our hotel\nWoke up to see the sunrise at 6.30 and we had an amazing view of the Himalayas. The view was so nice, that kept us there for half of our day; we ate breakfast (omelet and pancakes), listen to music, played football, and enjoyed the view. It was the most beautiful view I have ever seen! At 2 pm we left for Kande ‚Äî 1 hour to get down there. As we reached the street, we tried to call our driver. It would have taken him an hour to come so we hitchhiked and a family in a jeep drove us to Pokhara for 1000rupees.\nWe got some rest, and for the evening we arranged dinner with the travelers we met yesterday, Todd and Ruth. We had dinner at Soul Origin cafe and restaurant, and then dessert at French creperie; both amazing places. Todd and Ruth are an amazing duo! They are from California, they traveled in 58 countries, they love to immerse themselves in new cultures and both have great stories to tell throughout their global exploration journey; Todd is a great photographer (check his IG page), and Ruth is an amazing planner, she gave us some of the most useful traveling tips (check her article here).\nHighlights from our discussion:\n\nThe most adverse trip moments are usually accompanied by strong lessons and memorable experiences.\nThe little moments we share with people are undervalued. Traveling helps us understand better how the world works.\nOur lives in Europe and USA are much easier than we frequently think they are.\nBears in Alaska don‚Äôt bother people if their bellies are full of salmon.\nWhen traveling, keep your valuables safe with a tracker."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-7-shopping-in-pokhara",
    "href": "posts/2023-03-25-nepal /index.html#day-7-shopping-in-pokhara",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 7: Shopping in Pokhara",
    "text": "DAY 7: Shopping in Pokhara\nLast day in Pokhara started with a late wake-up. We had breakfast with Todd and Ruth and then went shopping at Pokhara‚Äôs main street. There are many quality products such as cashmere clothing, and brands such as NorthFace, Colombia, Patagonia that you can find much cheaper than Europe and America. On top of that negotiating is expected so you can get an even better price. After negotiating I got the following deals: a NorthFace jacket from 8500 initial price to 7000 rupees, a NorthFace sweatpants from 2500 to 1800 rupees, gloves for snow from 1000 to 900 rupees, and a cashmere sweater from 8500 to 5000 rupees.\nThen we had lunch at the restaurant Fresh Elements. Very good restaruant with a lot of options and good prices; only downside is that their portions are relatively small. Our hotel Middle Path & Spa, is collaborating with that restaurant so everything we were ordering from our hotel was coming from there.\nMeanwhile, my friend rented a motorcycle for $35/day from Pokhara with a plan to go to a 5-day ride starting from Pokhara to Jomsom, and ending to Upper Mustang. To ride at Upper Mustang, you need to pay for a permit $500, and have two more people with you for security. However, the pictures suggest that it is a worthwhile experience! In the evening I took the overnight bus at 7.30 pm for Kathmandu."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-8-departure-from-nepal",
    "href": "posts/2023-03-25-nepal /index.html#day-8-departure-from-nepal",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 8: Departure from Nepal",
    "text": "DAY 8: Departure from Nepal\nLast day in Nepal started early in Kathmandu. I arrived at 5 am. That time everything was closed. So I took a taxi for the reception of Kathmandu Guest House hotel. There is security, resting chairs to sleep in, and good breakfast. I left my luggage there, and I walked towards Durbar square. They asked me for a ticket once more (apparently if you don‚Äôt look local they ask you to pay 150 rupees). Don‚Äôt pay for this.\nHimalayan Java is the main coffee chain in Nepal. There is one in the middle of Durbar square so I went for pancakes. It seems fancier than the average coffee shop. It is the ‚ÄòStarbucks‚Äô of Nepal.\nIn the evening, I went back to the Guest House hotel to pick up my luggage and called the morning taxi driver to take me to the airport (we agreed in the morning 800 rupees for the airport ride).\nOverall, Nepal is among the best trips I have ever made and I‚Äôd love to revisit to see more nature and different cities. I hope you find this article useful for your trip planning to Nepal or satisfactory for your curiosity about the country and its culture.\nFollow my friend‚Äôs Instagram page to see what the world looks like when you are traveling in faraway places:)\nPlease clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html",
    "href": "posts/2022-06-09-denmark/index.html",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "",
    "text": "Copenhagen\nAre you visiting Copenhagen anytime soon? If yes, keep reading as you will find some related info and suggestions that might help you plan your visit! As an exchange student in Denmark back in 2018, I had enough time to explore it. While you might be for a few days there, this can help you allocate your time efficiently during your visit."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#currency",
    "href": "posts/2022-06-09-denmark/index.html#currency",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "CURRENCY",
    "text": "CURRENCY\nThe main currency is Danish Krone. Live currency converter. However, they accept card payments everywhere. With Revolut or Venmo you should be fine."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#transportation",
    "href": "posts/2022-06-09-denmark/index.html#transportation",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "TRANSPORTATION",
    "text": "TRANSPORTATION\n\nThe Metro is convenient and it takes you everywhere you want to go within the city:\n\nRejseplanen app - This is the app for the metro/train. It is designed specifically for them and it works like Google maps. Great navigation tool while you are in Denmark. There will be machines (also a person for support) at the airport where you can buy tickets. Also, Denmark is infamous for cycling with great infrastructure. You can rent bikes as well and ride across the city."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-visits",
    "href": "posts/2022-06-09-denmark/index.html#cool-visits",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "COOL VISITS",
    "text": "COOL VISITS\nNyhavn (The most iconic part of Copenhagen. Highly recommended. the Closest metro station: Kongens Nytorv station.)\n\n\n\nNyhavn\n\n\n\nSomething you can do is to walk through the shopping center of Copenhagen. You can go to Norreport station and walk towards Nyhavn. It is like a 1 to 1.5 mile (1.6‚Äì2.4km) walk. There are many stores and restaurants in between (along the yellow line on the map). While it is not a straightforward walk it is a pleasant one as you are passing through the shopping area of the city.\n\n\nRosenborg Castle & Botanical Garden (Nice places to walk around. Also, Rosenborg castle is a historic place that now operates as a museum ticket info. Have no opinion for the museum though as I haven‚Äôt been. Closest metro station: Norreport station)\nLittle Mermaid (You will often see it in souvenir shops as it is considered as one of the main attraction points in Copenhagen along Nyhavn. While you might not be impressed by it due to its tiny size, you may want to see it simply for its touristic outreach.) Osterport Station.\nAmalienborg (The equivalent ‚ÄòBuckingham palace‚Äô of Denmark. Cool for walking around and taking pictures with the royal guards. Have no opinion for the museum there though as I haven‚Äôt been. tick info)\nCarlsberg Factory (if you like beer this is a nice place to visit. There is a tour in the factory and some beer tasting in the end. Carlsberg Station.)\nTivoli Gardens/Park (Located in front of the Central Station. The most famous park in Copenhagen. Kobenhavn H station)\nFisketorvet (If you like Malls, this can be an option. Most famous mall in the city.)\nBlack Diamond (The largest library in Denmark. It is a modern building. A Nice place to visit. Christianshavn station.)"
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "href": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "COOL NEIGHBOURHOODS",
    "text": "COOL NEIGHBOURHOODS\n\nFreetown Christiania (It is considered an independent town within Copenhagen where people go there to chillax. It is a renowned marijuana place since Christiania is the only part of Copenhagen where Danish law is not enforced. However, it is a safe area that is worth a visit mainly due to its particularity. Highly recommended.) Metro station: Christianshavn.\nBeautiful Parks (especially for summer walks):\n\n\nFrederiksberg Have (One of the most beautiful parks in Copenhagen) Frederiksberg station.\nSuperkilen Park (Norrebro station). Fun fact, Norrebro is an area where many immigrants stay. Diverse area.\nVestre Kirkegard (near Carlsberg factory) Metro: Carlsberg Station\n\nFOOD & COOL PLACES min $ /max $$$$\n\nUnion Kitchen $$ Cool place for brunch. Heard great things from friends that have been there.\nSticks‚Äôn‚ÄôSushi $$$ Great sushi place. Known rooftop restaurant in the city.\nDalle Valle $ variety food buffet - alue for money place with plenty of food. Not fast food, but not a high-class restaurant. Almost always lots of people, decent food, a variety of options. Recommended for lunch.\nConditori La Glace $$$ Traditional Danish pastries, it is the oldest patisserie in Denmark - founded in 1870, good for dessert\nEspresso House $$ coffee place, despite its Swedish origin it is considered as the ‚ÄòStarbucks‚Äô of Denmark\nBastard cafe $$ a place full of board games. A nice place to go with friends for coffee/snacks/beer and board games.\nTaphouse $$ Famous bar in the city. If you like beer then this is the place for you. It has one of the largest beer selections in Europe.\n\nYou can always map your choices and combine visits that are close to each other. Also, since it is Scandinavia, remember to check the weather while preparing your bags! You are visiting arguably the most beautiful city in Scandinavia so try to enjoy every minute of it!\nHave fun and safe travels!"
  },
  {
    "objectID": "posts/2023-06-08-vegas/index.html",
    "href": "posts/2023-06-08-vegas/index.html",
    "title": "Four-Day Road Trip: Denver-Grand Canyon-Las Vegas",
    "section": "",
    "text": "Day 1‚Ää‚Äî‚ÄäDenver to Chinle\nThe journey began in Denver, Colorado. Destination for the day was the Thunderbird Lodge Hotel, situated in Chinle, Arizona, about an eight-hour drive from Denver without accounting for stops.\nOur first stop, approximately two hours into the trip, was Fairplay, Colorado. Fairplay is the real-life inspiration for the popular animated show South Park, and it showcases this duality of character quite effectively. One section of the town features classic wild western-style houses, while another displays brightly colored homes reminiscent of those in the cartoon. Went to the South Park Brewery for lunch. An interesting nugget of information from the bartender: the name ‚ÄòSouth Park‚Äô originates from the region, not the show, and referencing the cartoon in the restaurant could land them in hot water, legally. That‚Äôs why they had no references from the show.\n\nBack on the road, we covered substantial ground before making our next stop at Chick Fil A in Farmington, New Mexico. Many people were there; Americans are crazy for fried chicken.\nThe rest of the journey to Chinle was made under a beautiful evening dessert scenery. We finally reached the Thunderbird Lodge Hotel at 11 pm. It was a long drive, but the combination of diverse landscapes, engaging stops, and pleasant people made the day an excellent start to our road trip.\n\n\nDay 2‚Ää‚Äî‚ÄäGrand Canyon National¬†Park\nWoke up to a view of a calming garden at our motel For breakfast, we visited a nearby spot, breakfast chain Dennis, for a delicious/extra calorific meal.\nOn the road once again, four-hour drive to Grand Canyon National Park, reaching the Grand Canyon Visitor Center around 1 pm. Interestingly, we discovered an entrance fee of $35 for the day, or $70 for an annual pass. This fee granted us access to park our vehicle and utilize the convenient bus transportation system that shuttled visitors to various points within the canyon‚Ää‚Äî‚Ääthere is a bus every 15 minutes at the points of the Canyon. Bike rentals were available as well, at a cost of $40 per person for a two-hour duration. The summer heat, which could reach up to 46 degrees Celsius, so if you go, go prepared. There are a lot of signs that say do not overestimate your abilities because the hikes down the Canyon can get challenging really fast. The heat is too much, you need extra water, salty snacks, and sunscreen 100%, if you want to make your life easier.\n\n\n\nGrand Canyon\n\n\nKeep in mind, you take double the time to climb up than going down so it is wise to stop when you reach at 1/3rd of your energy capacity. Every year, approximately 150 people are rescued by helicopter. This is not to say that it is not a doable challenge, but if you plan to hike far, come prepared.\n\nThings to take with you on dessert hike¬†trips\n\nHat (brimmed hat preferably)\nWater/ Water bottle\nSunscreen\nSunglasses\nSnack bars‚Ää‚Äî‚Ääsalty snacks for keeping electrolyte balance\nFirst aid kit\n\nFor more tips read the Grand Canyon national park article.\nUpon parking, we proceeded to Yavapai Tavern for lunch. The pizza we tried not recommended due to its small size, suggest the burger instead. You dont have many options anyway, it is one of the few restaurants you will find around there.\nNext, we boarded a bus from the main station, and there we saw for the first time Grand Canyon. That was the highlight of the day because the magnitude of it gets you by surprise. You can see pictures of it, but experiencing it is something totally different. You get the feeling of insignificance. It is a sightseeing with 2 billion years of history.\nWe took the red line bus to Hermit‚Äôs Rest, where we explored the area. Walked around and took some pictures.\n\n\n\nGrand Canyon Rim¬†Trail\n\n\nConcluding the day, we took on the Rim‚Äôs Trail, a 3-kilometer path that closely follows the edge of the canyon. Spent time in the canyon before departing around 7 pm for next motel, the Aztec Motel in Seligman. Along the way, made a stop at a KFC and a gas station nearby. Arrived at the hotel by 9 pm along the iconic Route 66.\nSome other worthwhile places to admire Grand Canyon‚Äôs view are the following:\n\nGrand Canyon West Skywalk: The Skywalk is 70 feet long and extends out over the rim of the Grand Canyon, providing unobstructed views straight down to the canyon floor 4,000 feet below. The glass floor of the Skywalk offers a unique perspective and makes it feel like you‚Äôre walking on air.\nHorseshoe Bend: This iconic viewpoint overlooks the river from a height of about 1,000 feet, offering a breathtaking panorama that has become a poster image for the American Southwest. The curve of the river creates a horseshoe-like shape, hence the name. To reach the viewing area, visitors must hike a 1.5-mile round-trip trail from the parking area. It‚Äôs a relatively easy hike, but it can be hot in the summer, so it‚Äôs recommended to bring water and sun protection. The view is well worth the effort, but do take care near the edge as there are no railings. Remember, this is a very popular site and can get crowded, especially during the peak tourist season. It‚Äôs often photographed at sunrise or sunset when the light brings out the vibrant colors of the rocks.\n\n\n\n\nDay 3‚Ää‚Äî‚ÄäRoute 66 and exploring Las¬†Vegas\nDay started with a simple motel breakfast, setting out on the road by 9 am. The drive along Route 66 from Seligman to Las Vegas was an exceptionally smooth ride. Great road to drive for three reasons, straight, very few cars, and a nice dessert scenery.\n\nArrived in Las Vegas around 11:30 am, checked into our hotel, the Fairfield Inn, before going for lunch. Our chosen restaurant was the Mint Indian restaurant, located a mile away. Although the heat reached 115 degrees Fahrenheit or 46 degrees Celsius, we walked there (1 mile)‚Ää‚Äî‚ÄäDO NOT DO THIS, you might get a headache, and it is not a pleasant experience. Regarding the restaurant, the flavor variety of the authentic all-you-can-eat buffet made it well worth the journey. Definitely recommend it, especially if you are seeking South Indian cuisine.\nFollowing our meal, made our way to the renowned casinos along the Las Vegas Strip, such as Bellagio, Caesars Palace, Paris, The Venetian, etc. Immersed in the vibrant atmosphere, we tried our luck at roulette. On our way back we got bubble tea from Happy Lemon before returning to our hotel.\n\n\n\nLas Vegas¬†Strip\n\n\nAfter a restful break from 4:30 pm to 6:30 pm, we embarked on our evening adventures. Downtown Vegas, particularly Fremont Street, offers a distinct and lively hippie vibe, featuring an array of parties and vibrant lights. This area is certainly worth exploring for anyone visiting Las Vegas. Our next destinations included the iconic Las Vegas Sign and the Luxor Hotel with its pyramids. Returning to the Bellagio, we saw the water fountain display before going for dinner at Buffalo Wild Wings. The next stop was Caesars Palace, with the aim of reaching the roof for a breathtaking view. Although we managed to ascend to the 47th floor, a locked door prevented us from accessing the rooftop. Nonetheless, we continued to explore the casino, which had various games such as roulette, blackjack, and slot machines.\n\nVegas tips\nIf you plan to gamble in Las Vegas, it‚Äôs advisable to bring cash to avoid the casino‚Äôs ATM‚Äôs $10 withdrawal fee. Additionally, for international visitors, carrying your passport or state ID is recommended, as some instances may require them for entry. Although there is no strict dress code in casinos, it is advisable to use common sense when choosing attire.\nGiven the warm weather in Vegas, it is essential to have water, sunscreen, and a hat when walking during the day. To avoid purchasing bottled water, request a cup from a restaurant, preferably a fast food chain, and get it at no cost. If you like paying, casinos typically charge for water.\nConcluding our day, we returned to our hotel, and rested, to be prepared for the final day of our road trip, driving back home on Day 4.\n\n\n\nDay 4‚Ää‚Äî‚ÄäJourney Back to Denver\nDay 4 marked the final leg of our epic road trip as we embarked on an 11-and-a-half-hour drive covering a distance of 750 miles, returning to Denver from Las Vegas. Leaving Las Vegas behind, we traversed through the captivating landscapes of Utah.\nFun Fact: Utah is home to five national parks known as the Mighty 5. These parks include Zion National Park, Bryce Canyon National Park, Capitol Reef National Park, Arches National Park, and Canyonlands National Park. Each park offers its own unique natural wonders, from towering sandstone cliffs to stunning arch formations.\nDuring our journey, we made a bubble tea/ gas stop at Grand Junction, Colorado. As the day advanced, we steadily progressed and eventually arrived back in Denver by 10:15 pm.\n\nConclusion\nI hope the tips that accompany this article prove helpful to your plans in crafting your own trip in the future.\nSafe travels and happy exploring!"
  },
  {
    "objectID": "posts/2023-05-7-colorado/index.html",
    "href": "posts/2023-05-7-colorado/index.html",
    "title": "Comprehensive Activity Guide for Colorado",
    "section": "",
    "text": "Rocky Mountain National Park\n\n\n\nIntroduction\nThis guide outlines a broad spectrum of activities and locales to visit in Colorado. The beauty of Colorado lies in its beautiful landscapes, vibrant social scene, wild-west and natural history, and recreational activities that can cater to every lifestyle. Hopefully, the recommended activities/visits below to give you some ideas on how to spend your time while in Colorado.\n\n\nActivities\n\nRent a car. It is perfect for exploring Colorado‚Äôs stunning scenery and diverse attractions. It offers flexibility, and freedom, allowing you to discover the Rockies, mountain towns, and outdoor activities at your leisure. A rental car provides convenience and comfort, making it the ideal choice for an unforgettable Colorado adventure. There are many places worth visiting, such as the Rocky Mountain National Park, Boulder, Georgetown, Vail, Garden of the Gods, and many more.\nBike at Cherry Creek trail. Cherry Creek trail is the most popular trail in Denver because it starts from the Confluence Park in the heart of Denver and ends 25 miles southeast at the Cherry Creek Dam & Reservoir. Check out what it looks like: https://www.youtube.com/watch?v=rm1inEbk8m4\nGo to a Sports game. Basketball, Baseball, Football, or Ice Hockey you pick! The cheapest one is usually baseball and the most expensive is Football.¬†\n\n\n\n\nDenver Nuggets - NBA¬†game\n\n\n\nWatch a Red Rocks Concert. One of the most beautiful theaters in America; whatever you watch there it‚Äôs worth it they say. It is an outdoor amphitheater in nature surrounded by red rocks, and the acoustics are amazing. In the summer they do concerts every day there. You can see what they have and book here.\nBristlecone Shooting range. That is a fun activity to do with friends if you want to live something truly American. Bristlecone Shooting Range (video here) is the most popular shooting range in the area.\nBuffalo Bill Museumat Lookout Mountain‚Ää‚Äî‚ÄäWhile Bill‚Äôs grave is a touristy spot, you can combine it with a nice hike. The price to enter the museum is $5 (as of May 2023) and it takes about 45 minutes to see, they also show a movie about Buffalo Bill‚Äôs life.¬†\nAxe Throwing‚Ää‚Äî‚ÄäThere are a couple of those in Denver. All are about $30 for an hour. Calling in advance to check the traffic inside is recommended. I went to this one in Colfax. There you also have the option of throwing knives as well, which is a bit more difficult.\nImmersive Gamebox experience‚Äî‚ÄäGreat place to go with friends. You select your game and enjoy it with your group. There is a range of options with motion tracking, projection mapping, touch screens, and surround sound. The most popular game they have is Squid Game. The price is around $30-$35. When you are booking, try code PURPLE20, for a 20% discount.\nThe Denver Museum of Nature & Science‚Ää‚Äî‚ÄäA museum established in 1900, stands as a beacon of knowledge and exploration in the heart of Colorado. There you can see an array of diverse exhibits that dive into the wonders of the universe, ancient civilizations, and the mysteries of the natural world. They have free days + nights for everyone‚Ää‚Äî‚Ääfind them here. So if you are around Colorado during those days make sure to NOT miss them!\n\n\n\n\nDo not miss Museum‚Äôs free days‚Ää‚Äî‚Äächeck the link above\n\n\n\nDenver Comedy Underground - Check out their website for stand up comedy events. They usually have a $20 ticket, including pizza and one drink in the price, without minimum to-buy drink requirement. Sounds like a good deal!\n\n\n\n\n\nDenver Zoo - Nice weekend activity. They have free days in the year so make sure you check them out here\nEvents in town - Find events that are happening in town through this website or via SeatGeek.\n\n\n\nCool Visits\n\nRocky Mountain National Park [video]‚Ää‚Äî‚ÄäThe Rocky Mountain National Park, is an expansive area encompassing over 265,000 acres of diverse ecosystems, from montane forests to alpine tundra. The park is home to a variety of wildlife, including elk, bighorn sheep, and moose. Visitors can enjoy scenic drives, such as the famous Trail Ridge Road, and hike on over 350 miles of trails that offer breathtaking views of the Rockies. The park is also home to the famous Trail Ridge Road, which offers stunning panoramic views.\nUnion Station and Larimer Square‚Ää‚Äî‚ÄäIt is the main train station in Denver. It is a beautiful train station with restaurants and occasionally music. Great place to walk, the vibe outside reminds Central Europe.\n\n\n\n\nUnion Station\n\n\n\nEstes Park [video]‚Äî Estes Park, is known as the base for the Rocky Mountain National Park. The park itself offers more than 350 miles of hiking trails, varying in difficulty from easy walks to challenging climbs, along with opportunities for camping, wildlife viewing, and bird watching. The town of Estes Park has an elevation of 7,522 feet and maintains a relatively mild climate throughout the year. In the downtown area, you‚Äôll find over 300 shops and restaurants along the Estes Park Riverwalk, making it a lively center for dining and shopping. Visitors can also enjoy local events such as the annual Elk Fest and the Rooftop Rodeo. Estes Park provides a well-rounded experience, combining the tranquility of nature with the convenience of modern amenities.\nStanley Hotel [video] - Around Estes park area is the legendary Stanley Hotel. The Stanley Hotel is known for its luxurious accommodations and elegant architecture. It has a history dating back to its founding in 1909. It also gained fame as the inspiration for Stephen King‚Äôs novel, ‚ÄúThe Shining‚Äù (the movie was also filmed there). Also, the movie ‚ÄúDumb And Dumber‚Äù was filmed there. This grand hotel has hosted numerous celebrities and is considered a must-visit destination for those intrigued by its paranormal reputation.\nBoulder [video]‚Ää‚Äî‚Ää¬†Boulder is a beautiful city close to the mountains with Pearl street being the main piazza that people go to. The lively streets, charming shops, and lively atmosphere make Pearl Street an attractive destination in Boulder. According to a famous UPenn statistics professor in the Behavioral and Decision Sciences program, ‚ÄúAt Boulder is like you are in Switzerland‚Äù.\n\n\n\n\nPearl Street, Boulder¬†Colorado\n\n\n\nSt Mary‚Äôs Glacier [video]‚Ää‚Äî‚ÄäSaint Mary‚Äôs Glacier, located near Idaho Springs in Clear Creek County, Colorado, is a permanent snowfield, often mistaken as a glacier due to its year-round snow presence. Its beauty and accessibility make it a popular hiking destination. The trailhead begins at 10,400 feet and hikers ascend around 400 feet in under a mile to reach the snowfield. During summer, it‚Äôs common to see visitors hiking, fishing in the nearby lake, or even snowboarding and skiing on the snowfield.\nThe Buffalo Bill Museum and Grave [video]‚Ää‚Äî‚ÄäThe Buffalo Bill Museum and Grave in Golden, Colorado, is a captivating destination atop Lookout Mountain. The museum showcases artifacts, exhibits, and memorabilia that illuminate the life of Buffalo Bill Cody, featuring items from his buffalo hunting days, Pony Express involvement, and Wild West Show career. The gravesite, marked by a striking monument, offers panoramic views. This site provides a historical experience portraying Buffalo Bill‚Äôs significant role in shaping the American West‚Äôs legacy.\nGarden of the Gods [video]‚Ää‚Äî‚ÄäThe Garden of the Gods, a public park known for its remarkable red rock formations. These unique sandstone formations were created millions of years ago through geological uplift and erosion, and the park has a rich Native American history, with evidence of human habitation dating back thousands of years. The area was designated a public park in 1909.\n\n\n\n\nGarden Of The Gods, Colorado¬†Springs\n\n\n\nGeorgetown [video]‚Ää‚Äî‚ÄäGeorgetown, was founded during the Pike‚Äôs Peak Gold Rush in 1859. It was named after George Griffith, a prospector who discovered rich silver deposits in the area. Georgetown‚Äôs growth soared with the silver boom, and by the 1880s, it had become the center of the mining district. By the late 1890s, silver prices plummeted, leading to economic decline. Despite this, Georgetown managed to survive, thanks in part to its preserved Victorian architecture that drew tourists. Today, it‚Äôs known for its historic charm and outdoor recreational opportunities.\nGurudwara temples - You can find them in Denver, as well as in other major cities. They offer free food to any visitor, and it is delicious. It is an interesting visit to go for lunch, especially if you like to explore a different cultures. The most popular one in Denver is Colorado Singh Sabha, which is located 15 minutes drive outside of town.\n\nA ‚ÄúGurudwara‚Äù is a Sikh place of worship that centers around the teachings of the Guru Granth Sahib, the religion‚Äôs holy scripture, and is characterized by its inclusive community kitchen called ‚ÄúLangar‚Äù and principles of equality and selfless service. It welcomes individuals of all backgrounds and emphasizes the removal of shoes and covering of heads as signs of respect.\n\n\n\n\nConclusion\nThis article provides a guide to help you adapt some ideas and activities to your personal circumstances and needs. That would hopefully help you make the best use of your time in Colorado.¬†\nPlease clap üëè post helpful or inspiring. You can read this or see more articles and tips on medium or on my website here."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html",
    "href": "posts/2023-05-31-texasfood/index.html",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "",
    "text": "Texan food\nIf you ever find yourself in middle to low tier popularity cities in Texas or if you are driving through the state, then this article might help you navigate. This article spill the beans, or at least some of them, on the hidden gems and under-the-radar restaurants that often slip through the cracks of the tourist maps. Whether it‚Äôs exploring BBQ options, or experiencing Texan vibes, this article serves as a guide for a satisfying food adventure in Texas."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html#waco",
    "href": "posts/2023-05-31-texasfood/index.html#waco",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "WACO",
    "text": "WACO\nü•û Magnolia Table $$ Simple and neat. Not many options in the menu, but not little. Just about right and the description is very clear on what they offer. Great vibe in the restaurant. Magnolia‚Äôs table is owned by the famous Chip and Joanna Gaies who are well knowned from the popular ‚ÄúFixed Upper‚Äù TV series which was nominated for two Primetime Emmies awards.  The idea for Magnolia Table was born out of Chip and Joanna‚Äôs love for hospitality and creating spaces where people can gather and connect. They wanted to bring their unique style and vision to the restaurant scene, offering a comfortable and welcoming atmosphere for guests.\nThe building that houses Magnolia Table has a rich history of its own. It was originally a historic Elite Cafe, a well-known restaurant in Waco that had been serving customers since 1919. Chip and Joanna Gaines acquired the property and renovated it to transform it into Magnolia Table, preserving the building‚Äôs character and charm. \nThe renovated version of the restaurant first opened its doors in 2018 and quickly became a popular dining destination in the area. The menu at Magnolia Table features a variety of breakfast and brunch dishes, with a focus on fresh and locally sourced ingredients. Beyond its food offerings, Magnolia Table is known for its beautiful interior design. Chip and Joanna have infused their signature style into the space, creating a rustic yet modern ambiance with farmhouse-inspired decor.\nüçñ Vitek‚Äôs BBQ $$ Vitek‚Äôs BBQ is a renowned barbecue restaurant located in Waco, Texas. It has a rich history that dates back several decades. The restaurant was established in 1915 by Slovak immigrant William Martin Vitek and has remained in the Vitek family ever since.  Originally known as Vitek‚Äôs Grocery Store, the business started as a small corner store that sold basic groceries and household items. Over time, Joe Vitek decided to expand the store‚Äôs offerings by adding a barbecue pit and serving smoked meats to customers. This decision proved to be a turning point for the business, as the barbecue became incredibly popular among locals and visitors alike.\nIn 1972, the name changed from Vitek‚Äôs Grocery and Meat Market to Vitek‚Äôs BBQ. In 1983, Vitek‚Äôs most famous offering was created: the ‚ÄúGut Pak.‚Äù The Gut Pak is a hearty combination of smoked sausage, chopped beef, pickles, onions, and jalapenos served on a bed of Fritos corn chips. While it was recommended to us by the hotel receptionist, that day it was closed so we went to Rudy‚Äôs Bar-B-Q instead.\nüçñ Rudy‚Äôs Bar-B-Q $ American chain restaurant for brisket. Not amazing but not bad. Rudy‚Äôs Bar-B-Q is a popular chain that originated in Leon Springs, Texas, near San Antonio in 1989. While Rudy‚Äôs Bar-B-Q has expanded to various locations across Texas and other states, including Waco, it started as a small country store with a barbecue pit."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html#driftwood",
    "href": "posts/2023-05-31-texasfood/index.html#driftwood",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "DRIFTWOOD",
    "text": "DRIFTWOOD\nüçñ Salt Lick BBQ $$‚Äî The Salt Lick BBQ is an iconic barbecue restaurant located in Driftwood, Texas. Established in 1967 by Thurman Roberts, Sr., and his wife Hisako, its roots trace back to the mid-1800s when Thurman‚Äôs ancestors settled into the Texas Hill Country. They perfected a unique barbecuing method involving open flames instead of the more common indirect heat method.¬†\n\n\n\nThe Salt Lick¬†BBQ\n\n\nDrawing inspiration from a family recipe, Thurman and Hisako opened The Salt Lick with the goal of offering great food and a memorable experience. Over the years, it has gained immense popularity, becoming a must-visit spot for BBQ enthusiasts. It‚Äôs famous not only for its rich, smoky meats but also for its BYOB (bring your own beer) tradition and the picturesque vineyard setting. With more than 15,000 reviews Salt Lick maintains a tradition that Texans honor."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html#seguin",
    "href": "posts/2023-05-31-texasfood/index.html#seguin",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "SEGUIN",
    "text": "SEGUIN\nüçñ Rocket Wings & Dixie Grill $ Texan flags, stars, baseball and football shirts, wooden tables, it doesn‚Äôt get more American than that. Huge portions. Good place for the vibes and if you are hungry. If someone that respects food opened a local fast food that‚Äôs how would look like. It‚Äôs chicken wings though were mediocre. \nüçñ Burnt Bean Company $$ Heard great things about it from local scientists at GBRA Lab in Seguin. Popular place among the locals with lines for BBQ. It earned a place at ‚ÄúThe Texas Bucket List‚Äù.\nThe restaurant opened in January 2021. It did not take long to take off. Ernest Cevantes, the founder, mentions that his ‚Äúclaim to fame‚Äù can be explained by competition BBQ. Specifically he won 45 out of 50 great champions and everything you can win in Texas. He partnered up with one of his competitors in competition BBQ, David Kirkland."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html#mingus",
    "href": "posts/2023-05-31-texasfood/index.html#mingus",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "MINGUS",
    "text": "MINGUS\nü§å Beneventi‚Äôs Italian Restaurant A fine dining restaurant with many years of history. It seems well established as it features in the popular YouTube channel‚ÄùThe Texas Bucket List‚Äú.\n The owner of the restaurant is an Italian immigrant. He arrived in Ellis Island in New York, got in a train, and migrated in the town of Mingus in 1906. He bought the place, and it started as a grocery store. It was bought on site for $88 and it came with four little girls and one of them became his wife. That building is about 160 years old.\nYou can tell that the place has a history as you can see some of the tomato sauces they used to sell back in the time. That is super cool, probably my favorite place out of all because of the historical vibes. We did not eat there since we were on our way to another town, I got the unsweetened iced tea, freshly made and tasty."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html#strawn",
    "href": "posts/2023-05-31-texasfood/index.html#strawn",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "STRAWN",
    "text": "STRAWN\nüçñ Mary‚Äôs cafe Quite popular restaurant in the area. The restaurant is popular for its chicken fried steak. Fun fact, Mary serves over 200 thousand pounds of potatoes and 50 thousand pounds of steak every year; this means 137 pounds (=62kg) of steak and 548 pounds (247kg) of potatoes per day!\nMary got her first job at the (only) other restaurant in the city after the 8th grade. In 1984, Mary started working in that restaurant which back then it was called ‚ÄúThe polka dot‚Äù. By 1986 she owned the place and renamed it to ü•Å‚Ä¶ ‚ÄúMary‚Äôs Cafe‚Äù! In the ‚ÄúThe Texas Bucket List‚Äù video it says that Mary does it all and I can confirm that she at least does the waiter job; she was the one that took our order. I got the ‚Äòchicken fried chicken‚Äô and got to try the ‚Äòsteak fried chicken‚Äô. To be honest, I did not like either of these because I felt that the oil and meat amount was around 50%-50%, but what do I know, you go try and get your own opinion."
  },
  {
    "objectID": "posts/2023-05-31-texasfood/index.html#scattered-in-texas",
    "href": "posts/2023-05-31-texasfood/index.html#scattered-in-texas",
    "title": "Food pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn",
    "section": "SCATTERED IN TEXAS",
    "text": "SCATTERED IN TEXAS\n Buc-ee‚Äôs‚Ää‚Äî‚ÄäBuc-ee‚Äôs is a renowned convenience store and gas station chain that originated in Texas. Founded in 1982 by Arch ‚ÄúBeaver‚Äù Aplin III and Don Wasek, Buc-ee‚Äôs started as a modest convenience store in Lake Jackson, Texas. Unlike typical convenience stores, Buc-ee‚Äôs stands out with its oversized facilities, an abundance of gas pumps, immaculately clean restrooms, and an extensive range of products from travel snacks to home decor. Famous for their Beaver nuggets, jerky, and other proprietary snacks, Buc-ee‚Äôs has grown into a Texan travel institution.¬†\n\n\n\nBuc-ee‚Äôs new store opening¬†day\n\n\nTheir mascot, a beaver with a baseball cap, has become an iconic symbol representing a unique blend of Southern hospitality and road trip convenience. Over the years, Buc-ee‚Äôs has expanded significantly and has started to branch out beyond Texas borders, yet remains a beloved pit stop for both locals and tourists.\nPlease clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html",
    "href": "posts/2023-07-15-presentations/index.html",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "",
    "text": "Death by PowerPoint"
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#introduction",
    "href": "posts/2023-07-15-presentations/index.html#introduction",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "Introduction",
    "text": "Introduction\nAre your presentations drawing too much interest? Do people hang on to your every word, eagerly awaiting your next point? Do you find audience members nodding in agreement or even chuckling at your jokes? If that‚Äôs the case, your presentations might be a little too interesting! Don‚Äôt worry though, I got you covered. This guide will help you make your presentations less interesting and more forgettable. The article takes you on a journey to learn the secrets of a dull presentation, breaking all the rules of good storytelling and engaging conversations."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#the-more-information-the-better",
    "href": "posts/2023-07-15-presentations/index.html#the-more-information-the-better",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "1. The more information the¬†better",
    "text": "1. The more information the¬†better\n\nData, data, and more data. Drench your audience in facts, figures, statistics, percentages, ratios, quotients, calculations, measurements, foreign characters, emojis, and whatever else you can integrate. Ideally, every sentence should have at least one number. Ditch the narrative. Who needs a story when you have the fourth quarter financial report to discuss in excruciating detail? And remember, never explain your data‚Ää‚Äî‚Ääcreate a good guessing game for the audience. Give them all the data they never knew they didn‚Äôt want to know."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#master-your-phrasing",
    "href": "posts/2023-07-15-presentations/index.html#master-your-phrasing",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "2. Master your¬†phrasing",
    "text": "2. Master your¬†phrasing\nAvoid inflection or emotion in your voice. Speak at a steady pace without variation. Use bullet points wisely. No, not one, not two, but fifty bullet points per slide. Make it so overwhelming that the audience can‚Äôt remember where you started or where you‚Äôre going. If you want to go the extra mile, use mumbling. Don‚Äôt articulate, don‚Äôt modulate. Mumble your words, and speak as fast as you can. Ensure that the last person in the room can only hear a vague humming sound.\nBonus points if you use filling words during the pauses. While ‚Äúeeehms‚Äù, and ‚Äúaaahms‚Äù are usually people‚Äôs go-to options feel free to be creative.\n\n\n\nexpert phrase presenter skills"
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#time-limit-whats-that",
    "href": "posts/2023-07-15-presentations/index.html#time-limit-whats-that",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "3. Time limit, What‚Äôs¬†That?",
    "text": "3. Time limit, What‚Äôs¬†That?\nRun over your allotted time. Keep going. Don‚Äôt stop. Time limits are for weaklings. If your audience starts glancing at the clock or their watches, you‚Äôre doing it right. If you‚Äôre given 15 minutes to present, aim for an hour!\nRemember, in the quest to deliver a truly monotonous presentation, time is your greatest ally. The longer, more convoluted, and less structured your presentation is, the better. Overwhelming your audience with the quantity of information, rather than the quality, is a surefire way to win the crown of ‚ÄúMost Boring Presentation.‚Äù\n\n\n\nGaddafi speaks over an hour at UN General Assembly: A Test of¬†Patience\n\n\n\nMuammar Gaddafi‚Äôs infamous speech at the United Nations General Assembly on September 23, 2009, is remembered for its remarkable duration and significant time overrun. Instead of adhering to the designated 15-minute time limit, Gaddafi spoke for over 90 minutes, making it one of the longest speeches in UN history. His extended address surpassed the expectations of the audience. Gaddafi‚Äôs disregard for time constraints contributed to the speech‚Äôs notoriety and left a lasting impression on those in attendance."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#avoid-eye-contact",
    "href": "posts/2023-07-15-presentations/index.html#avoid-eye-contact",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "4. Avoid Eye¬†Contact",
    "text": "4. Avoid Eye¬†Contact\nLooking at your audience might make them feel seen, heard, and engaged. We can‚Äôt have that, can we? Keep your eyes firmly glued to your notes, some distant point at the back of the room, or, better yet, your shoelaces. Don‚Äôt look at your audience.\nBonus tip: If someone asks a question or comments, definitely do not make eye contact with them. This can give the impression that their input matters or that you value their engagement, which is not what we want in this case."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#the-off-topic-odyssey",
    "href": "posts/2023-07-15-presentations/index.html#the-off-topic-odyssey",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "5. The Off-topic Odyssey",
    "text": "5. The Off-topic Odyssey\nWhy come to the point when you can take a delightful detour around it? Feel free to venture off into a tangent that has nothing to do with your main topic. Remember, your goal is to baffle, not inform. To ‚Äúbeat around the bush‚Äù is your goal and you must stay determined. Keep your audience guessing about the purpose of your talk. Bonus points if, by the end, they‚Äôve forgotten what the initial topic was. Michael Scott is a master of this, you can see him doing it here."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#visuals-ignite-confusion",
    "href": "posts/2023-07-15-presentations/index.html#visuals-ignite-confusion",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "6. Visuals: Ignite Confusion",
    "text": "6. Visuals: Ignite Confusion\nAvoid the use of visuals, images, graphs, or anything that could make the information more digestible or engaging. If can‚Äôt hold the temptation of using them, make sure you put a ton of them and make it complex. The more complex, the better. And if you can, don‚Äôt bother explaining what they mean. Let the audience play ‚ÄòSpot the X-axis‚Äô while you enjoy their blank stares."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#avoid-interaction",
    "href": "posts/2023-07-15-presentations/index.html#avoid-interaction",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "7. Avoid Interaction",
    "text": "7. Avoid Interaction\nDo not ask questions or encourage participation from the audience. Make the presentation one-sided. Why ask for input when you can just lecture? Make sure the audience members know they are there just to listen. Let them daydream about the coffee break instead.\nAlso, make sure you skip the storytelling part. Who needs engaging narratives or interesting anecdotes? Go straight to the facts and figures, and strip them of any context or meaning. Remember, you‚Äôre a presenter, not a performer. Stand stiff and still. Avoid all hand gestures and facial expressions.\n\nIsaac Newton, as professor of mathematics at Cambridge, displayed little interest in teaching or engaging with his students. His lectures were often sparsely attended, and there were instances where no one showed up at all. Newton‚Äôs focus remained primarily on his own research and studies, neglecting his responsibilities as an educator. His disinterest in teaching his students contributed to his reputation as a disengaging teacher.\n\n\n\n\nThe man himself"
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#terminology-tornado",
    "href": "posts/2023-07-15-presentations/index.html#terminology-tornado",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "8. Terminology tornado",
    "text": "8. Terminology tornado\nWhy use one word when you could use ten? Why explain a simple concept in layman‚Äôs terms when you could dress it up with complexity? Spice it up by using technical jargon and long-winded explanations that nobody can follow. If somebody tries to ask a question, respond with an even more technical and long-lasting explanation. With that, a way to tell if you did well is if you see that the person who asked gets angry looks from the audience."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#no-personal-connection",
    "href": "posts/2023-07-15-presentations/index.html#no-personal-connection",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "9. No Personal Connection",
    "text": "9. No Personal Connection\nDo not make any personal connection with the audience. Avoid sharing personal experiences or anecdotes. That can make them feel that you share things in common which might elicit curiosity and engagement. If you think that is difficult, a great way to do so is to treat your slides like a script and read it word for word. Pretend you are doing an eye test. If you have extra confidence, try to make some mistakes while reading, and go over some lines again. It‚Äôs an absolutely splendid way to put them off.\nLaughter, smiles, a sense of fun‚Ää‚Äî‚Ääthese are frivolities you don‚Äôt have time for. Maintain the gravity of a state funeral. Remember, nothing says ‚Äòinteresting‚Äô like a presentation that feels like an obituary reading.\n\n\n\nRichard Nixon addressing the public ~ read through but avoid eye¬†contact!"
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#the-what-just-happened-effect",
    "href": "posts/2023-07-15-presentations/index.html#the-what-just-happened-effect",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "10. The ‚ÄúWhat Just Happened?‚Äù Effect",
    "text": "10. The ‚ÄúWhat Just Happened?‚Äù Effect\nEnsure your presentation lacks a clear introduction, body, and conclusion. Jump randomly from one point to another. Make sure to repeat yourself frequently, preferably in slightly different ways, for maximum confusion and minimal impact. Keep your audience guessing when you might actually finish. If you‚Äôve done it right, they will be begging for the end, staring at the clock, checking their phones‚Ää‚Äî‚Ääanything to distract from the never-ending monologue that you‚Äôve crafted.\nFor your grand finale, end abruptly. No summary, no conclusion, no ‚Äòthank you‚Äô. Leave them in a daze of confusion. After all, who doesn‚Äôt love a cliffhanger? Remember, the goal here isn‚Äôt to summarize or make impactful final points. No, it‚Äôs to induce sheer boredom and perhaps even a bit of existential dread."
  },
  {
    "objectID": "posts/2023-07-15-presentations/index.html#conclusion",
    "href": "posts/2023-07-15-presentations/index.html#conclusion",
    "title": "Mastering Monotony: 10 Techniques for Uninteresting Presentations",
    "section": "Conclusion",
    "text": "Conclusion\nArmed with these 10 strategies, you are now equipped to master the art of delivering uninteresting presentations. Embrace the power of boredom, and watch as your audience struggles to stay awake and comprehend the depths of tedium you have unleashed upon them. So go all in, and may your presentations be dull. Stay monotonous, stay uninteresting, and embrace the realm of mind-numbing monotony.\nPlease clap üëè post helpful or inspiring. You can read this or see more articles and tips on medium or on my website here."
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html",
    "href": "posts/2022-10-22-undergrad/index.html",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "",
    "text": "European University Cyprus\nEuropean University Cyprus, mainly has a Cypriot community where students come with their established friend groups from school, so you need to put more effort in socializing. For example, if you are coming from abroad you need to establish some common ground and try to do some activities with your colleagues. Moreover, the class atmosphere as well as in the cafeteria helps the newcomers to adjust socially to life at university. Even if you do something apart from university in the city (e.g.¬†join a gym, attend a student event) there is a high possibility of bumping into people that study at the same university. Therefore, I would say that it is not difficult to cover your social needs in that sense and overcome the social barriers during the first months ‚Äî just get involved!"
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#academics",
    "href": "posts/2022-10-22-undergrad/index.html#academics",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "ACADEMICS",
    "text": "ACADEMICS\nThe academic adjustment was more demanding in comparison with the social one. While I began university after a year and a half gap and I was not confident with my academic abilities, I did very well at school. As long as you are organized and focused and you prioritize, you will set yourself up for good grades and success. One strategy that helped me is that I kept track of the material for each course by filing it all separately. This eliminated unnecessary stress. This method, in conjunction with a systematic study approach by studying each week right after each lecture, allowed me to absorb the learning material in a very effective way. In addition, I was doing a 3 hour focused study sessions without distractions (i.e.¬†phone away, water/drinks next to me, and using the bathroom before so I don‚Äôt have excuse to stop). At the university, you will also have your personal advisor who, in coordination with your department, will support you academically. This is the person whom you will be addressing all the concerns you have regarding the courses. Having a personal advisor helps you to adjust at first. Having him/her throughout your studies gives you guidance on what classess to choose based on your goals, which is also important."
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#set-up-your-career",
    "href": "posts/2022-10-22-undergrad/index.html#set-up-your-career",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "SET-UP YOUR CAREER",
    "text": "SET-UP YOUR CAREER\nFor job opportunities and internships, the EUC Career Center has a lot of great resources. One of these is their own employment platform ‚Äî ‚ÄúCSM Simplicity‚Äù ‚Äî for our students. It is a great tool whether you are looking for a part-time job or to really launch your career after you‚Äôve finished your degree. Their Career Center‚Äôs priority is to help EUC students to achieve their career goals. They organize career workshops and share many opportunities with the students. They also offer personal consultation to students for writing a CV or acing an interview. Moreover, if you make clear that you are interested, they send you personally current job and internship offers tailored to your interests and study field."
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#connect-with-faculty",
    "href": "posts/2022-10-22-undergrad/index.html#connect-with-faculty",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "CONNECT WITH FACULTY",
    "text": "CONNECT WITH FACULTY\nIn my school journey, I have also had the support of the faculty. The professors are friendly and approachable. They have office hours and during that time they are open to discussing anything with the students. Discussions with my professors also gave me insights which then helped me to formulate tailored study methodology in each course. Another positive element that comes along with the interaction with professors is that you get to know about new program opportunities that under normal circumstances you wouldn‚Äôt know about. Something that for me created a snowball effect as it evolved to be something larger. For example, the interaction I have had with one of my professors, led me to get exposure to different programs outside school such as the Study of the US Institutes for Student Leaders; a fully funded program in the US for 6 weeks. I found out about this program as I was participating in a workshop outside of the university in which he organized and invited our class.\n\n\n\nStudy of the US Institutes for Student Leaders - USA"
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#pursue-opportunities-abroad",
    "href": "posts/2022-10-22-undergrad/index.html#pursue-opportunities-abroad",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "PURSUE OPPORTUNITIES ABROAD",
    "text": "PURSUE OPPORTUNITIES ABROAD\nThis brings us to the travelling opportunities that students have to explore the world. Another part of the university that I would like to talk about is the Erasmus exchange opportunity that offers. It gives you the ability to do an internship or to study in another EU country, which is the most popular option. Since I entered university, I have tried to get involved with the Erasmus program to experience what I had been hearing about that is so amazing. So, I spent my third semester in Denmark living as an exchange student. This was really a worthwhile experience. First, I made new friends and during that time frame, I had the chance to travel a lot. But even more significant was the effect upon my arrival back home. I came back with more confidence; higher ambitions and I was able to see more clearly what is happening in the international arena. Then I joined the Erasmus Student Network organization in Cyprus which deals with incoming exchange students. So I extended my Erasmus experience as I expanded it locally with all its benefits that come along. This experience also had a snowball effect. That is because then I have had many opportunities to participate in other short-term funded Erasmus exchange programs in Europe. Furthermore, through being a part of active youth in Cyprus I have also found out about, applied to, and been selected to be a part of the Cypriot delegation at Model United Nations at Harvard University in Boston. Something that opened the doors for me for being a part of even more activities beyond Europe.\n\n\n\nHarvard National Model United Nations\n\n\nRegarding the Erasmus experience that EUC offers, there are many countries that the university has an agreement with, and YOU as a student have the chance to attend. On the university‚Äôs website, you can find the Erasmus agreements that EUC has and choose according to your field of study and personal preferences.\nIn conclusion, there is a lot that you can gain from European University as long as you take initiative. It is up to YOU to chase those opportunities that are aligned with your interests, and you believe that have the highest potential. Show interest in the classes, invest in relationships with students and professors and stay up to date with the Erasmus office and the career center for opportunities at home and abroad. My biggest tips? Get involved, stay organized and never be afraid to really go for it! I had such great experiences at EUC by living by these words.\nLoizos Konstantinou, 2021‚Äô\nBusiness Economics Undergrad"
  },
  {
    "objectID": "posts/2023-07-30-coloradoMoving/index.html",
    "href": "posts/2023-07-30-coloradoMoving/index.html",
    "title": "Tips for Smooth Sailing: Moving to Denver, Colorado",
    "section": "",
    "text": "Welcome to Colorado Sign\n\n\n\nDenver, Colorado-specific tips\nMoving to a new city can be challenging. It would help if you did a couple of things before going to a new place:\n\nFind a place to¬†stay\n\nGo to Facebook groups and look for groups that are relevant to housing. Draft a post of who you are, what you are looking for, and your budget, and post it out there. That helps in getting noticed. You can see an example here:\nGo to Airbnb; message owners and try to get a long-term deal on their property.\nLook via Furnished Finder; a platform designed to connect traveling healthcare professionals, such as travel nurses and therapists, with property owners offering furnished, short-term housing options. The website aims to provide healthcare professionals with convenient and affordable housing solutions for their temporary assignments, typically lasting 13 weeks or more. You can still look there and let the owners know if you are not a healthcare professional.\n\n\n\nFind your group of¬†people\nBesides work, you might find it tough to meet new people who you share interests with. A great way to do it is through the Meetup app; a platform designed to help people connect with others who share similar interests and hobbies in their local area. Users can join existing meetup groups or create their own groups centered around specific interests. Members can browse and join local groups, attend events, and engage with other community members.\n\n\nTransportation\nCar: Consider getting a car. It is THE main mean of transport here in Denver, and in Colorado in general. It makes it super easy to go to the mountains, to get around, and to other cities. If you are getting around with other means it will take you A LOT of time. If you do not go out frequently, Uber/Lyft (get discount from my referral here) is the way to go, which are not cheap but on my occasion still cheaper overall than having one.\nBike: Denver is a bikable city. Short distances (less than 30min) and not many street ups and downs. There are also nice bike trails (i.e. Cherry Creek trail), and the streets are pretty safe to bike; there is space and good roads and pavements. If you don‚Äôt have a bike you can use the Lime app to rent one or rent a scooter on the spot.\nBus/Metro/Train: For these, there is a centralized app that you can download to make it easy to transport around; the RTD app. See this video for more details on how to use it ‚ÄòHow to Navigate the Denver RTD System‚Äô.\n\n\n\nRTD Denver Rail System¬†Map\n\n\n\n\nSketchy Neighborhoods\nWhile Denver is a safe city overall, as in any big city there are some areas in which you need to be more cautious. Here are the areas below, in case you want to stay out of trouble or get into a breathtaking adventure:\n\nColfax Street: especially east Colfax near the Aurora area and West Colfax near Sloan‚Äôs Lake.\nMarket and 16th Street Mall at night\nCapitol Hill area at night; and during the day near the bus stops.\n\n\n\nBackpacker tips\nGym: Planet Fitness is the cheapest one ($10 membership‚Ää‚Äî‚ÄäApril 2023), and with branches all over America can be a good deal for you. I was going to Carla Madison Recreation Center as it was closer to me and the membership was reasonable; $30/month‚Ää‚Äî‚Ää50% off if you go with someone who lives at the same address! So go ahead and find yourself a roommate as well:)\n\nIf you are there for a short time or you are indecisive, you can go for one-day trial to each of those PLUS Anytime fitness, and get some free exercise while deciding.\n\nSupermarket: Cheapest one from the chains is Trader Joe‚Äôs. Whole Foods, Sprouts, and Safeway are expensive, Kings Soopers is slightly less than them; Walmart is also there, and a bit more affordable than Target (which is also expensive). Not sure if there is a cheaper non-chain option; haven‚Äôt looked much for it because they are is none within a 25-minute bike radius from my house.\nShopping: Ross Dress for Less has value-for-money deals when it comes to clothing. Also, check out DWS if you are looking for shoes.\nHaircut: Aveda Institute Denver, a school cosmetology/beauty saloon, is the cheapest place. You can get a haircut for $20 (as of April 2023). Demand is high so reservation in advance is encouraged. You can get it for even cheaper if you call them to pre-book and ask for the 10% discount üòâ\n\n\n\nAveda Institute, Denver¬†Colorado\n\n\nShoe soles: This is more specific but it might be helpful. Having comfortable soles is important for your feet. After the advice of Margaret, a serial hiker (especially her husband), I went to Fleet Feet to get soles for my shoes. Without knowing much about foot science, this place seems solid. They make you stand in a digital machine that takes measurements of your feet, and they propose some solutions for you. You can also get the results via email if you‚Äôd like. Also, prices are decent ($50-$60 as of May 2023 for a normal pair of soles).\n\n\nConclusion\nThis article provides a guide to help you with your move and settling in. Hopefully, you would be able to adapt these guidelines to your personal circumstances and needs. For an activities and visits guide read this article here, and for restaurant recommendations check this one out.\nPlease clap üëè post helpful or inspiring. You can read this or see more articles and tips on medium or on my website here."
  },
  {
    "objectID": "posts/2023-10-12-jung/index.html",
    "href": "posts/2023-10-12-jung/index.html",
    "title": "Happy birthday!",
    "section": "",
    "text": "Carl Jung, founder of analytical psychology\n\n\nHAPPY BIRTHDAY ‚Äì&gt; Check out YOUR life stats HEREü§ìüìä\nIf you are younger than forty, hope Jung‚Äôs quote makes you feel better.\nIf you are older and still doing research‚Ä¶ what does Jung know? He might be wrong.\nHe was doing research after 40; I mean, he had 8 publications after his 40th birthday. Do you think that was the case because his name alludes to a not old person? I highly doubt it.\nHowever, not everything revolves around you. He might have just said it for himself. If that‚Äôs the case, it is different. Forty correlates with about the time that he becomes a celebrity, so that might have to do with this.\nWell, it‚Äôs all about how we choose to look at it at the end of the day, but anyways, hope you take a positive standpoint and enjoy your birthday no matter how old you are becoming; HAPPY BIRTHDAY!"
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html",
    "href": "posts/2023-03-10-susi/index.html",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "",
    "text": "SUSI group at the University of Tennesee at Chattanooga\nStudy of the US Institutes for Student Leaders is an intensive academic program for undergraduate students from Europe. I was a part of the Entrepreneurship and Economic Development 2019 cohort which took place in Tennessee, Chattanooga. Twenty (20) Students from seventeen (17) different European countries were selected for this program. We spent five weeks in the United States getting a deeper understanding of American culture, business environment in social and corporate settings, and enhancing our leadership skills. New friends, entrepreneurial exposure, business visits, US cultural learning, and travel experiences, and the opportunity to get funding for future projects on the benefits side, PLUS all of these for $0 dollars! On the cost side, only opportunity cost."
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#about-the-program",
    "href": "posts/2023-03-10-susi/index.html#about-the-program",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "About the program",
    "text": "About the program\nWhen I first came across the program, I read about the opportunity and I got excited about the potential learnings. I would have the opportunity to visit new places, spend my summer in a purposeful way learn about economic development, AND make new friends and experiences. Then, I also realized that the program would pay for ALL my travel and living expenses for five weeks in the U.S. It was an opportunity I couldn‚Äôt pass up!\nIntroductory SUSI video before arrival üëà"
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#experience",
    "href": "posts/2023-03-10-susi/index.html#experience",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "Experience",
    "text": "Experience\nSUSI experience in a nutshell üëà"
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#a-typical-weekday",
    "href": "posts/2023-03-10-susi/index.html#a-typical-weekday",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "A typical weekday",
    "text": "A typical weekday\nEarly in the mornings, from 8 to 11 am, we had classes about management, entrepreneurship, and inclusion and diversity where before each class each one of us had to present something in the class. Then we were having lunch in the university‚Äôs dining hall. Every day we had an abundant buffet of every food you can imagine; from burgers, chicken nuggets, and pizzas, to salmon, salads, fruits, waffles, and frozen yogurt.\nAfter lunch, we were had visits in companies and NGOs near the area. The company visit could have been anything from a small social enterprise, to a large multinational conglomerate. We were meeting chief executives and people from the team that were presenting us the ways that their company runs and its culture. Our schedule continued with activities which were either another visit excursion to a company or a cultural/bonding activity such as bowling, going sightseeing in the town, volunteering at an NGO, etc.\nIn the evening we had the option to eat dinner wherever we preferred. The program included allowance every week for us to spend which was more than enough to eat at a mid-range restaurant every day or cook at home and save more money for shopping and doing other activities.\n You can hereby see the schedule of SUSI 2018 ‚Äî one year before my cohort ‚Äî each year is different, and better in my opinion, but the core concept stays the same.\nBesides Chattanooga, we also visited other cities such as Nashville, and Atlanta. During the last week of our program, we spent 3 days in New York City, visiting companies and exploring the city. The last three days, we spent in Washington DC, where we presented our business idea to FHI360 headquarters (the NGO under the US State Department program was organized).\n\n\n\nMini SUSI reunion in Vienna, January 2020\n\n\nLiving 5 weeks in the US with the same people made everyone to come closer to each other. During SUSI I have made some really good friends in Europe whom I am glad to host anytime in Cyprus and I know that every time I will be in their country I will have someone to hang out with.\nPlease clap üëè on this on medium if you find this post helpful:)"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "import pandas as pd\n\n\nmyData = pd.read_csv('Header_data.csv', encoding=\"latin\")\nprint(myData.head())\n\n   √ø√æT\n0  NaN\n1  NaN"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Hi üëã I am Loizos! This is my personal website.\nHere you will (ideally) find interesting articles, and helpful resources\nI have a blog that is occasionally updated üìù\nSee projects, some things I‚Äôve done at school üõ†\nFeel free to get in contact below"
  },
  {
    "objectID": "software/credcardpred.html",
    "href": "software/credcardpred.html",
    "title": "Which algorithm you recommend? What accuracy it has? Why you measured accuracy the way you did?",
    "section": "",
    "text": "#1. Properly load the data into Jupyter Notebooks\n\nimport matplotlib.pyplot as plt #matplotlib generates graphs.\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC #i.e. Support Vector Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.read_csv(\"crx.data\", header=None)\nprint(data.columns)\n\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')\n\n\nChange the column names to correspond to the ‚Äúreal‚Äù labels from the second link.\n\ndata.rename(columns={1 : \"age\", \n                     2 : \"debt\",\n                     3 : \"married\",\n                     4 : \"bankcustomer\", \n                     5 : \"educationlevel\", \n                     6 : \"ethnicity\",\n                     7 : \"yearsemployed\", \n                     8 : \"priordefault\",\n                     9 : \"employed\",\n                     10 : \"creditscore\",\n                     11 : \"driverslicense\",\n                     12 : \"citizen\",\n                     13 : \"zipcode\",\n                     14 : \"income\",\n                     15 : \"approved\"}, inplace=True)\nprint(data.head())\n\n   0    age   debt married bankcustomer educationlevel ethnicity  \\\n0  b  30.83  0.000       u            g              w         v   \n1  a  58.67  4.460       u            g              q         h   \n2  a  24.50  0.500       u            g              q         h   \n3  b  27.83  1.540       u            g              w         v   \n4  b  20.17  5.625       u            g              w         v   \n\n   yearsemployed priordefault employed  creditscore driverslicense citizen  \\\n0           1.25            t        t            1              f       g   \n1           3.04            t        t            6              f       g   \n2           1.50            t        f            0              f       g   \n3           3.75            t        t            5              t       g   \n4           1.71            t        f            0              f       s   \n\n  zipcode  income approved  \n0   00202       0        +  \n1   00043     560        +  \n2   00280     824        +  \n3   00100       3        +  \n4   00120       0        +  \n\n\nRemove all question marks from value ‚Äòage‚Äô, and convert it to numerical.\n\ndata.loc[83,'age'] = \"\"\ndata.loc[86,'age'] = \"\"\ndata.loc[92,'age'] = \"\"\ndata.loc[97,'age'] = \"\"\ndata.loc[254,'age'] = \"\"\ndata.loc[286,'age'] = \"\"\ndata.loc[329,'age'] = \"\"\ndata.loc[445,'age'] = \"\"\ndata.loc[450,'age'] = \"\"\ndata.loc[500,'age'] = \"\"\ndata.loc[515,'age'] = \"\"\ndata.loc[608,'age'] = \"\"\n\n\n\ndata['age'] = pd.to_numeric(data['age'])\n\n#2. Summarize the data.Frequency tables for categorical variables and histograms for continuous variables.\n\n%matplotlib inline\nz = data.hist(column=['debt', 'age', 'yearsemployed', 'income', 'creditscore'],bins=5, figsize=(12,7))\nprint(z) #is not adding them i.e. it creates a longer list.\n\nsummary_data = data.describe()\nprint(summary_data)\n\n#Categorical variables:\n#             \"married\", \n#             \"bankcustomer\", \n#             'educationlevel', \n#             'ethnicity',  \n#             'priordefault', \n#             'employed', \n#             'driverslicense',\n#             'citizen',\n#             'approved'\n\nmarried = data.loc[:,\"married\"].value_counts()\nprint(married)\n\nbankcustomer = data.loc[:,\"bankcustomer\"].value_counts()\nprint(bankcustomer)\n\neducationlevel = data.loc[:,\"educationlevel\"].value_counts()\nprint(educationlevel)\n\nethnicity = data.loc[:,\"ethnicity\"].value_counts()\nprint(ethnicity)\n\npriordefault = data.loc[:,\"priordefault\"].value_counts()\nprint(priordefault)\n\nemployed = data.loc[:,\"employed\"].value_counts()\nprint(employed)\n\ndriverslicense = data.loc[:,\"driverslicense\"].value_counts()\nprint(driverslicense)\n\napproved = data.loc[:,\"approved\"].value_counts()\nprint(approved)\n\ncitizen = data.loc[:,\"citizen\"].value_counts()\nprint(citizen)\n\n\n[[&lt;AxesSubplot:title={'center':'debt'}&gt;\n  &lt;AxesSubplot:title={'center':'age'}&gt;]\n [&lt;AxesSubplot:title={'center':'yearsemployed'}&gt;\n  &lt;AxesSubplot:title={'center':'income'}&gt;]\n [&lt;AxesSubplot:title={'center':'creditscore'}&gt; &lt;AxesSubplot:&gt;]]\n              age        debt  yearsemployed  creditscore         income\ncount  678.000000  690.000000     690.000000    690.00000     690.000000\nmean    31.568171    4.758725       2.223406      2.40000    1017.385507\nstd     11.957862    4.978163       3.346513      4.86294    5210.102598\nmin     13.750000    0.000000       0.000000      0.00000       0.000000\n25%     22.602500    1.000000       0.165000      0.00000       0.000000\n50%     28.460000    2.750000       1.000000      0.00000       5.000000\n75%     38.230000    7.207500       2.625000      3.00000     395.500000\nmax     80.250000   28.000000      28.500000     67.00000  100000.000000\nu    519\ny    163\n?      6\nl      2\nName: married, dtype: int64\ng     519\np     163\n?       6\ngg      2\nName: bankcustomer, dtype: int64\nc     137\nq      78\nw      64\ni      59\naa     54\nff     53\nk      51\ncc     41\nm      38\nx      38\nd      30\ne      25\nj      10\n?       9\nr       3\nName: educationlevel, dtype: int64\nv     399\nh     138\nbb     59\nff     57\n?       9\nj       8\nz       8\ndd      6\nn       4\no       2\nName: ethnicity, dtype: int64\nt    361\nf    329\nName: priordefault, dtype: int64\nf    395\nt    295\nName: employed, dtype: int64\nf    374\nt    316\nName: driverslicense, dtype: int64\n-    383\n+    307\nName: approved, dtype: int64\ng    625\ns     57\np      8\nName: citizen, dtype: int64\n\n\n\n\n\nOur data is skewed towards the left of the distribution because there are some outliers in our data, such as customers with a high income and credit score relative to the overall dataset. However, we are working with a limited number of data and it is still in the initial stage of development, we can leave it as-is for now. We can try to remove the outliers when we are tuning our model or when we have more data to work with.\n#3. Split the sample into a test set and a train set with 20% of data being in the test dataset. Your random seed should be 808.\n\n#split dataset into train and test, and set the random_state so it maintain the same result\ndata_features = data[['debt', \"age\", \"married\",\"bankcustomer\", \"educationlevel\", \"ethnicity\", \"yearsemployed\", \"priordefault\",\"employed\",\"creditscore\",\n\"driverslicense\",\"citizen\",\"income\"]]\ndata_target = data['approved']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data_features,\n                                                    data_target,\n                                                    test_size = 0.2,\n                                                    random_state = 808)\n\n#4. Try the following algorithms and choose the one that generates the best accuracy:\n\n#First, we convert categorical variables to numerical.\ndata1 = pd.get_dummies(data).dropna()\nprint(data1)\n\n\nprint(list(data1.columns))\n\n       age    debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n0    30.83   0.000           1.25            1       0    0    0    1   \n1    58.67   4.460           3.04            6     560    0    1    0   \n2    24.50   0.500           1.50            0     824    0    1    0   \n3    27.83   1.540           3.75            5       3    0    0    1   \n4    20.17   5.625           1.71            0       0    0    0    1   \n..     ...     ...            ...          ...     ...  ...  ...  ...   \n685  21.08  10.085           1.25            0       0    0    0    1   \n686  22.67   0.750           2.00            2     394    0    1    0   \n687  25.25  13.500           2.00            1       1    0    1    0   \n688  17.92   0.205           0.04            0     750    0    0    1   \n689  35.00   3.375           8.29            0       0    0    0    1   \n\n     married_?  married_l  ...  zipcode_00720  zipcode_00760  zipcode_00840  \\\n0            0          0  ...              0              0              0   \n1            0          0  ...              0              0              0   \n2            0          0  ...              0              0              0   \n3            0          0  ...              0              0              0   \n4            0          0  ...              0              0              0   \n..         ...        ...  ...            ...            ...            ...   \n685          0          0  ...              0              0              0   \n686          0          0  ...              0              0              0   \n687          0          0  ...              0              0              0   \n688          0          0  ...              0              0              0   \n689          0          0  ...              0              0              0   \n\n     zipcode_00928  zipcode_00980  zipcode_01160  zipcode_02000  zipcode_?  \\\n0                0              0              0              0          0   \n1                0              0              0              0          0   \n2                0              0              0              0          0   \n3                0              0              0              0          0   \n4                0              0              0              0          0   \n..             ...            ...            ...            ...        ...   \n685              0              0              0              0          0   \n686              0              0              0              0          0   \n687              0              0              0              0          0   \n688              0              0              0              0          0   \n689              0              0              0              0          0   \n\n     approved_+  approved_-  \n0             1           0  \n1             1           0  \n2             1           0  \n3             1           0  \n4             1           0  \n..          ...         ...  \n685           0           1  \n686           0           1  \n687           0           1  \n688           0           1  \n689           0           1  \n\n[678 rows x 223 columns]\n['age', 'debt', 'yearsemployed', 'creditscore', 'income', '0_?', '0_a', '0_b', 'married_?', 'married_l', 'married_u', 'married_y', 'bankcustomer_?', 'bankcustomer_g', 'bankcustomer_gg', 'bankcustomer_p', 'educationlevel_?', 'educationlevel_aa', 'educationlevel_c', 'educationlevel_cc', 'educationlevel_d', 'educationlevel_e', 'educationlevel_ff', 'educationlevel_i', 'educationlevel_j', 'educationlevel_k', 'educationlevel_m', 'educationlevel_q', 'educationlevel_r', 'educationlevel_w', 'educationlevel_x', 'ethnicity_?', 'ethnicity_bb', 'ethnicity_dd', 'ethnicity_ff', 'ethnicity_h', 'ethnicity_j', 'ethnicity_n', 'ethnicity_o', 'ethnicity_v', 'ethnicity_z', 'priordefault_f', 'priordefault_t', 'employed_f', 'employed_t', 'driverslicense_f', 'driverslicense_t', 'citizen_g', 'citizen_p', 'citizen_s', 'zipcode_00000', 'zipcode_00017', 'zipcode_00020', 'zipcode_00021', 'zipcode_00022', 'zipcode_00024', 'zipcode_00028', 'zipcode_00029', 'zipcode_00030', 'zipcode_00032', 'zipcode_00040', 'zipcode_00043', 'zipcode_00045', 'zipcode_00049', 'zipcode_00050', 'zipcode_00052', 'zipcode_00056', 'zipcode_00060', 'zipcode_00062', 'zipcode_00070', 'zipcode_00073', 'zipcode_00075', 'zipcode_00076', 'zipcode_00080', 'zipcode_00086', 'zipcode_00088', 'zipcode_00092', 'zipcode_00093', 'zipcode_00094', 'zipcode_00096', 'zipcode_00099', 'zipcode_00100', 'zipcode_00102', 'zipcode_00108', 'zipcode_00110', 'zipcode_00112', 'zipcode_00117', 'zipcode_00120', 'zipcode_00121', 'zipcode_00128', 'zipcode_00129', 'zipcode_00130', 'zipcode_00132', 'zipcode_00136', 'zipcode_00140', 'zipcode_00141', 'zipcode_00144', 'zipcode_00145', 'zipcode_00150', 'zipcode_00152', 'zipcode_00154', 'zipcode_00156', 'zipcode_00160', 'zipcode_00163', 'zipcode_00164', 'zipcode_00167', 'zipcode_00168', 'zipcode_00170', 'zipcode_00171', 'zipcode_00174', 'zipcode_00176', 'zipcode_00178', 'zipcode_00180', 'zipcode_00181', 'zipcode_00186', 'zipcode_00188', 'zipcode_00195', 'zipcode_00200', 'zipcode_00202', 'zipcode_00204', 'zipcode_00208', 'zipcode_00210', 'zipcode_00211', 'zipcode_00212', 'zipcode_00216', 'zipcode_00220', 'zipcode_00221', 'zipcode_00224', 'zipcode_00225', 'zipcode_00228', 'zipcode_00230', 'zipcode_00231', 'zipcode_00232', 'zipcode_00239', 'zipcode_00240', 'zipcode_00250', 'zipcode_00252', 'zipcode_00253', 'zipcode_00254', 'zipcode_00256', 'zipcode_00260', 'zipcode_00263', 'zipcode_00268', 'zipcode_00272', 'zipcode_00274', 'zipcode_00276', 'zipcode_00280', 'zipcode_00288', 'zipcode_00290', 'zipcode_00292', 'zipcode_00300', 'zipcode_00303', 'zipcode_00309', 'zipcode_00311', 'zipcode_00312', 'zipcode_00320', 'zipcode_00329', 'zipcode_00330', 'zipcode_00333', 'zipcode_00340', 'zipcode_00348', 'zipcode_00349', 'zipcode_00350', 'zipcode_00352', 'zipcode_00356', 'zipcode_00360', 'zipcode_00368', 'zipcode_00369', 'zipcode_00370', 'zipcode_00371', 'zipcode_00372', 'zipcode_00375', 'zipcode_00380', 'zipcode_00381', 'zipcode_00383', 'zipcode_00393', 'zipcode_00395', 'zipcode_00396', 'zipcode_00399', 'zipcode_00400', 'zipcode_00408', 'zipcode_00410', 'zipcode_00411', 'zipcode_00416', 'zipcode_00420', 'zipcode_00422', 'zipcode_00431', 'zipcode_00432', 'zipcode_00434', 'zipcode_00440', 'zipcode_00443', 'zipcode_00450', 'zipcode_00454', 'zipcode_00455', 'zipcode_00460', 'zipcode_00465', 'zipcode_00470', 'zipcode_00480', 'zipcode_00487', 'zipcode_00491', 'zipcode_00500', 'zipcode_00510', 'zipcode_00515', 'zipcode_00519', 'zipcode_00520', 'zipcode_00523', 'zipcode_00550', 'zipcode_00560', 'zipcode_00583', 'zipcode_00600', 'zipcode_00640', 'zipcode_00680', 'zipcode_00711', 'zipcode_00720', 'zipcode_00760', 'zipcode_00840', 'zipcode_00928', 'zipcode_00980', 'zipcode_01160', 'zipcode_02000', 'zipcode_?', 'approved_+', 'approved_-']\n\n\nAs before, we split the sample into a test set and a train set with 20% of data being in the test dataset. Your random seed should be 808. This needs to be done after the ‚Äúget_dummies()‚Äù command for decision tree to work.\n\ndata_features = data1.loc[:,\"age\":\"citizen_s\"]\ndata_target = data1['approved_+']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data_features,\n                                                    data_target,\n                                                    test_size = 0.2,\n                                                    random_state = 808)\n\n\nprint(x_train, x_test, y_train, y_test)\n\n       age    debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n27   56.58  18.500         15.000           17       0    0    0    1   \n100  37.50   1.750          0.250            0     400    0    0    1   \n132  47.42   8.000          6.500            6   51100    0    1    0   \n404  34.00   5.085          1.085            0       0    0    0    1   \n401  28.92   0.375          0.290            0     140    0    0    1   \n..     ...     ...            ...          ...     ...  ...  ...  ...   \n480  16.92   0.500          0.165            6      35    0    1    0   \n384  22.08  11.460          1.585            0    1212    0    0    1   \n300  57.58   2.000          6.500            1      10    0    1    0   \n249  21.83  11.000          0.290            6       0    0    0    1   \n471  21.08   4.125          0.040            0     100    0    0    1   \n\n     married_?  married_l  ...  ethnicity_z  priordefault_f  priordefault_t  \\\n27           0          0  ...            0               0               1   \n100          0          0  ...            0               0               1   \n132          0          0  ...            0               0               1   \n404          0          0  ...            0               1               0   \n401          0          0  ...            0               1               0   \n..         ...        ...  ...          ...             ...             ...   \n480          0          0  ...            0               1               0   \n384          0          0  ...            0               1               0   \n300          0          0  ...            0               1               0   \n249          0          0  ...            0               0               1   \n471          0          0  ...            0               1               0   \n\n     employed_f  employed_t  driverslicense_f  driverslicense_t  citizen_g  \\\n27            0           1                 0                 1          1   \n100           1           0                 0                 1          1   \n132           0           1                 1                 0          1   \n404           1           0                 0                 1          1   \n401           1           0                 1                 0          1   \n..          ...         ...               ...               ...        ...   \n480           0           1                 0                 1          1   \n384           1           0                 0                 1          1   \n300           0           1                 1                 0          1   \n249           0           1                 1                 0          1   \n471           1           0                 1                 0          1   \n\n     citizen_p  citizen_s  \n27           0          0  \n100          0          0  \n132          0          0  \n404          0          0  \n401          0          0  \n..         ...        ...  \n480          0          0  \n384          0          0  \n300          0          0  \n249          0          0  \n471          0          0  \n\n[542 rows x 50 columns]        age   debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n207  28.67  9.335          5.665            6     168    0    0    1   \n406  40.33  8.125          0.165            2      18    0    1    0   \n231  47.42  3.000         13.875            2    1704    0    1    0   \n452  36.50  4.250          3.500            0      50    0    0    1   \n567  25.17  2.875          0.875            0       0    0    1    0   \n..     ...    ...            ...          ...     ...  ...  ...  ...   \n457  29.67  0.750          0.040            0       0    0    0    1   \n544  30.08  1.040          0.500           10      28    0    0    1   \n145  32.83  2.500          2.750            6    2072    0    0    1   \n342  26.92  2.250          0.500            0    4000    0    0    1   \n323  48.58  0.205          0.250           11    2732    0    0    1   \n\n     married_?  married_l  ...  ethnicity_z  priordefault_f  priordefault_t  \\\n207          0          0  ...            0               0               1   \n406          0          0  ...            0               1               0   \n231          0          0  ...            0               0               1   \n452          0          0  ...            0               1               0   \n567          0          0  ...            0               0               1   \n..         ...        ...  ...          ...             ...             ...   \n457          0          0  ...            0               1               0   \n544          0          0  ...            0               0               1   \n145          0          0  ...            0               0               1   \n342          0          0  ...            0               1               0   \n323          0          0  ...            0               0               1   \n\n     employed_f  employed_t  driverslicense_f  driverslicense_t  citizen_g  \\\n207           0           1                 1                 0          1   \n406           0           1                 1                 0          1   \n231           0           1                 0                 1          1   \n452           1           0                 1                 0          1   \n567           1           0                 1                 0          1   \n..          ...         ...               ...               ...        ...   \n457           1           0                 1                 0          1   \n544           0           1                 0                 1          1   \n145           0           1                 1                 0          1   \n342           1           0                 0                 1          1   \n323           0           1                 1                 0          1   \n\n     citizen_p  citizen_s  \n207          0          0  \n406          0          0  \n231          0          0  \n452          0          0  \n567          0          0  \n..         ...        ...  \n457          0          0  \n544          0          0  \n145          0          0  \n342          0          0  \n323          0          0  \n\n[136 rows x 50 columns] 27     1\n100    0\n132    1\n404    0\n401    0\n      ..\n480    0\n384    0\n300    0\n249    1\n471    0\nName: approved_+, Length: 542, dtype: uint8 207    1\n406    0\n231    1\n452    0\n567    1\n      ..\n457    0\n544    0\n145    1\n342    0\n323    1\nName: approved_+, Length: 136, dtype: uint8\n\n\n\ndata_features.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 678 entries, 0 to 689\nData columns (total 50 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   age                678 non-null    float64\n 1   debt               678 non-null    float64\n 2   yearsemployed      678 non-null    float64\n 3   creditscore        678 non-null    int64  \n 4   income             678 non-null    int64  \n 5   0_?                678 non-null    uint8  \n 6   0_a                678 non-null    uint8  \n 7   0_b                678 non-null    uint8  \n 8   married_?          678 non-null    uint8  \n 9   married_l          678 non-null    uint8  \n 10  married_u          678 non-null    uint8  \n 11  married_y          678 non-null    uint8  \n 12  bankcustomer_?     678 non-null    uint8  \n 13  bankcustomer_g     678 non-null    uint8  \n 14  bankcustomer_gg    678 non-null    uint8  \n 15  bankcustomer_p     678 non-null    uint8  \n 16  educationlevel_?   678 non-null    uint8  \n 17  educationlevel_aa  678 non-null    uint8  \n 18  educationlevel_c   678 non-null    uint8  \n 19  educationlevel_cc  678 non-null    uint8  \n 20  educationlevel_d   678 non-null    uint8  \n 21  educationlevel_e   678 non-null    uint8  \n 22  educationlevel_ff  678 non-null    uint8  \n 23  educationlevel_i   678 non-null    uint8  \n 24  educationlevel_j   678 non-null    uint8  \n 25  educationlevel_k   678 non-null    uint8  \n 26  educationlevel_m   678 non-null    uint8  \n 27  educationlevel_q   678 non-null    uint8  \n 28  educationlevel_r   678 non-null    uint8  \n 29  educationlevel_w   678 non-null    uint8  \n 30  educationlevel_x   678 non-null    uint8  \n 31  ethnicity_?        678 non-null    uint8  \n 32  ethnicity_bb       678 non-null    uint8  \n 33  ethnicity_dd       678 non-null    uint8  \n 34  ethnicity_ff       678 non-null    uint8  \n 35  ethnicity_h        678 non-null    uint8  \n 36  ethnicity_j        678 non-null    uint8  \n 37  ethnicity_n        678 non-null    uint8  \n 38  ethnicity_o        678 non-null    uint8  \n 39  ethnicity_v        678 non-null    uint8  \n 40  ethnicity_z        678 non-null    uint8  \n 41  priordefault_f     678 non-null    uint8  \n 42  priordefault_t     678 non-null    uint8  \n 43  employed_f         678 non-null    uint8  \n 44  employed_t         678 non-null    uint8  \n 45  driverslicense_f   678 non-null    uint8  \n 46  driverslicense_t   678 non-null    uint8  \n 47  citizen_g          678 non-null    uint8  \n 48  citizen_p          678 non-null    uint8  \n 49  citizen_s          678 non-null    uint8  \ndtypes: float64(3), int64(2), uint8(45)\nmemory usage: 61.6 KB\n\n\n\ndata1.head()\n\n\n\n\n\n\n\n\nage\ndebt\nyearsemployed\ncreditscore\nincome\n0_?\n0_a\n0_b\nmarried_?\nmarried_l\n...\nzipcode_00720\nzipcode_00760\nzipcode_00840\nzipcode_00928\nzipcode_00980\nzipcode_01160\nzipcode_02000\nzipcode_?\napproved_+\napproved_-\n\n\n\n\n0\n30.83\n0.000\n1.25\n1\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n58.67\n4.460\n3.04\n6\n560\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n24.50\n0.500\n1.50\n0\n824\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n27.83\n1.540\n3.75\n5\n3\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n20.17\n5.625\n1.71\n0\n0\n0\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n5 rows √ó 223 columns\n\n\n\n#a. Decision Trees\n\nfrom sklearn.tree import DecisionTreeClassifier\ninit_model = DecisionTreeClassifier ()\nfitted_model = init_model.fit(x_train ,y_train)\ntest_predictions = fitted_model.predict(x_test)\naccuracy_score = fitted_model.score(x_test,y_test)\nprint(accuracy_score)\n\n#overall how many of the predictions are correct\n\n\n#overall out of those defaults how many of those 0.17 are correct and not correct?\n\n0.8455882352941176\n\n\n#b. Logistic Regression\n\n#Logistic regression\nmodel_lr = LogisticRegression()\nfitted_model_lr = model_lr.fit(x_train, y_train)\ntest_predictions_lr = fitted_model_lr.predict(x_test)\naccuracy_lr = fitted_model_lr.score(x_test,y_test)\nprint(fitted_model_lr.coef_)\nprint(accuracy_lr)\n\n#overall how many of the predictions are correct\n\n[[ 4.84069824e-03 -2.46678805e-02  1.20267584e-01  1.64279116e-01\n   5.13339863e-04 -7.02796069e-03 -1.76746656e-01 -2.76489739e-02\n   1.08863388e-01  3.10703871e-02 -8.26650419e-02 -2.68692324e-01\n   1.08863388e-01 -8.26650419e-02  3.10703871e-02 -2.68692324e-01\n   1.08863388e-01 -9.05194758e-02 -9.94068100e-02  1.87520056e-01\n  -2.93554456e-02  3.49157435e-02 -3.61569082e-01 -2.52229191e-01\n  -9.00501443e-04 -2.10370142e-01 -8.76721486e-02  1.76003560e-01\n  -5.84801526e-03  1.76604192e-01  2.42540281e-01  1.08863388e-01\n  -1.29883594e-01 -2.29978369e-02 -3.51268838e-01  1.80746065e-01\n   5.79890564e-02  1.89933137e-02 -1.73749301e-08 -9.58931264e-02\n   2.20279985e-02 -1.72796178e+00  1.51653819e+00 -3.04359173e-01\n   9.29355818e-02 -1.62785055e-03 -2.09795740e-01 -2.90778257e-01\n   9.97707085e-02 -2.04160423e-02]]\n0.8602941176470589\n\n\n/Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n#c.¬†Bagging\n\n#bagging, need to explore the concept and what each parameter does\nfrom sklearn.ensemble import BaggingClassifier\n\n#n_estimators is a number of randomly sampled datasets, similar to cv\nmodel_bag = BaggingClassifier(\n    base_estimator = tree.DecisionTreeClassifier(),\n    n_estimators = 400,\n    max_samples = 0.8,\n    oob_score = True,\n    random_state = 808)\n\nfitted_model_bag = model_bag.fit(x_train,y_train)\ntest_predictions_bag = fitted_model_bag.predict(x_test)\naccuracy_bag = fitted_model_bag.score(x_test,y_test)#validation accuracy, because I want to see if the model works generalize in new data\nprint(accuracy_bag)\nprint(model_bag.oob_score_)\n\n0.8823529411764706\n0.8616236162361623\n\n\n#d.¬†Boosting\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\nmodel_boost = GradientBoostingClassifier()\n\nfitted_model_boost = model_boost.fit(x_train,y_train)\ntest_predictions_boost = fitted_model_boost.predict(x_test)\naccuracy_boost = fitted_model_boost.score(x_test,y_test)\nprint(accuracy_boost)\nprint(model_boost)\n\n0.8529411764705882\nGradientBoostingClassifier()\n\n\n#e. Random Forest\n\nrandomforest = RandomForestClassifier (random_state = 808)\nmodel_randforest = RandomForestClassifier()\n\nfitted_model_randforest = model_randforest.fit(x_train,y_train)\ntest_predictions_randforest = fitted_model_randforest.predict(x_test)\naccuracy_randforest = fitted_model_randforest.score(x_test,y_test)\nprint(accuracy_randforest)\n\n0.8382352941176471\n\n\n#f.¬†SVM\n\nfrom sklearn.svm import SVC #i.e. Support Vector Classifier\nsvm=svm.SVC(random_state = 808)\n\nprint(svm.fit(x_train, y_train).score(x_test,y_test))\n\n0.6985294117647058\n\n\n#g. Passive Aggressive Classifier (Links to an external site.)\n\npa=PassiveAggressiveClassifier(random_state = 808)\n\nprint(pa.fit(x_train, y_train).score(x_test,y_test))\n\n0.7573529411764706\n\n\n#h. Radius Neighbors Classifier (Links to an external site.)\n\nmodel_rn = RadiusNeighborsClassifier(radius=11700)\nfitted_model_rn = model_rn.fit(x_train,y_train)\ntest_predictions_rn = fitted_model_rn.predict(x_test)\naccuracy_rn = fitted_model_rn.score(x_test,y_test)\nprint(accuracy_rn)\n\n0.6470588235294118\n\n\n#5. After completing Step 4, explain: The accuracy of the algorithms above indicate the percentage of predictions that are actually correct. I would reccommend the bagging algorithm because it has the highest accuracy percentage (88,23%).\nThe confusion matrix below indicates that Bagging is the most suitable approach in reaching to the accuracy of the algorithms. In particular, the confusion matrix above shows us that the Bagging models have a higher number of true positives and true negatives than rest accuracy models. In this example above, we can see that bagging is compared with logistic regression and random forest classifiers, the 2nd and the 3rd highest accuracy models.\n\nfrom sklearn.metrics import confusion_matrix\n\nprint('Confusion matrix: Bagging Classifiers')\nprint(confusion_matrix(y_test, test_predictions_bag))\n\nprint('Confusion matrix: Logistic Regression')\nprint(confusion_matrix(y_test, test_predictions_lr))\n\nprint('Confusion matrix: Random Forest Classifiers')\nprint(confusion_matrix(y_test, test_predictions_randforest))\n\nConfusion matrix: Bagging Classifiers\n[[75 12]\n [ 4 45]]\nConfusion matrix: Logistic Regression\n[[70 17]\n [ 2 47]]\nConfusion matrix: Random Forest Classifiers\n[[70 17]\n [ 5 44]]\n\n\n\n6. Brief overview of the last two classifiers (Passive Agressive & Radius Neighbors).\nPassive Agressive classifier is an online learning predictor best suited for systems that receive data in a continuous stream. The calculation passively corrects for the classifications and penalizes ‚Äòaggressive‚Äô for any miscalculation. While the model can perfectly predict all data, it will not change the algorithm; hence it is called passive. The term aggressive referst to the fact that when the model fails to predict the outcome variable just in the slightest, it will change the algorithm to compensate for the failed prediction for every set of a new sample of data.\nRadius Neighbors classifer is similar to the KNN (k-nearest-neighbours) concept and makes predictions based on the data within a radius. Instead of locating the k-neighbors, the Radius Neighbors Classifier locates all examples in the dataset that are within a given radius of the new example. The radius neighbors are then used to make a prediction for the new example. The radius is defined in the feature space and generally assumes that the input variables are numeric and scaled to the range 0-1, e.g.¬†normalized. The radius-based approach to locating neighbors is appropriate for those datasets where it is desirable for the contribution of neighbors to be proportional to the density of examples in the feature space."
  },
  {
    "objectID": "software/titanic.html",
    "href": "software/titanic.html",
    "title": "Titanic Dataset",
    "section": "",
    "text": "Let's work with the titanic dataset.\n1. Construct a table showing the distribution of passengers by class and survival.\n\ntitanic %$% table(survived, pclass)\n\n        pclass\nsurvived   1   2   3\n       0 123 158 528\n       1 200 119 181\n\n# magrittr does this --&gt; table(titanic$survived, titanic$pclass)\n\n2. Construct a logistic regression model that links survival to the passenger class. Write out the equation first without Running it in R. HINT: Class is a factor variable\n\\[\nlog(odds(y)) = Œ≤0 + Œ≤1*pclass2 + Œ≤2*plcass3\n\\]\n\\[\nodds(y) = e^{Œ≤_0} * e^{Œ≤_1 * pclass2} * e^{Œ≤_2 * pclass3}\n\\]\nŒ≤0 = intercept\ny = survival\n3. Using hand-calculations, determine the coefficients in the model and interpret them (HINT: all you need to do is to use the table, calculate odds for the default category and the odds-ratios for the other categories versus the default)\ne.g.¬†not survival is the default exercise (i.e.¬†=0). prob of survival of:\nfirst class =&gt; 200/(200+123) = 62% of people survived\nsecond class =&gt; 119/(119+158) = 43% of people survived\nthird class =&gt; 181/(181+528) = 26% of people survived\nProbabilities vary between zero and one. Instead, for the purpose of the logistical regression we can calculate odds of survival of:\nfirst class =&gt;200/123=1.62 =&gt; for each first class person who died, 1.62 first class people survived.\nsecond class =&gt;119/158 = 0.75 =&gt; for each second class person who died, 0.75 second class person survived.\nthird class =&gt;181/528 = 0.34 =&gt; for each third class person who died, 0.34 third class person survived.\nWhen we take the log of odds, the result varies from - infinity to + infinity. log of survival of:\nfirst class people: log(200/123)=0.486\nsecond class people: log(119/158)=-0.28\nthird class people: log(181/528)=-1.07\n4. Now Run the model in R. Confirm that you got the same results as in part c). Interpret the results and talk about significance (both statistical and substantive).\n\ntitanic %$% summary(glm(survived ~ factor(pclass), family = \"binomial\"))\n\n\nCall:\nglm(formula = survived ~ factor(pclass), family = \"binomial\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3896  -0.7678  -0.7678   0.9791   1.6525  \n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.4861     0.1146   4.242 2.21e-05 ***\nfactor(pclass)2  -0.7696     0.1669  -4.611 4.02e-06 ***\nfactor(pclass)3  -1.5567     0.1433 -10.860  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1741.0  on 1308  degrees of freedom\nResidual deviance: 1613.3  on 1306  degrees of freedom\nAIC: 1619.3\n\nNumber of Fisher Scoring iterations: 4\n\nquestion2 &lt;- titanic %$% glm(survived ~ factor(pclass), family = \"binomial\")\nexp(question2$coefficients)\n\n    (Intercept) factor(pclass)2 factor(pclass)3 \n      1.6260163       0.4631962       0.2108239 \n\n\nThis code leaves us with the following equation of log odds:\nSurvived = 0.486 -0.77class2 -1.56class3\nThe significant p-values are telling us that the intercept is not zero. It means that the beta one (Œ≤1) is different from zero. This implies that there is a difference between classes and survival. We do not know yet what is the magnitude of the difference because this is a difference in log odds.\nSince is negative it means that classes 2 and 3 have lower survival chances from the first class. The 3rd class has the lowest chances of survival since its Œ≤1 coefficient has the highest negative number.\n5. What's the probability of survival for each class of passengers?\nFirst class had the odds of survival of 1.62. We can calculate the probability of survival as (1.62)/(1 + 1.62) = 0.6183206 or about 61.8%.\nSecond class had the odds of survival of 0.75. We can calculate the probability of survival as (0.75)/(1 + 0.75) = 0.4285714 or about 42.8%\nThird class had the odds of survival of 0.34. We can calculate the probability of survival as (0.34)/(1 + 0.34) = 0.2537313 or about 25.3%\n6. Construct a model that interacts class of passenger and his/her gender. Interpret the results the same way you did before.\nFirst we conduct the model just like as before but this time we add gender (aka ‚Äòsex‚Äô) as a factor variable. This time though we either add or multiply the one variable with the other. This model would help us make predictions for a passenger.\nBecause here the model assumes interaction between class and gender we construct the model with a ‚Äò*‚Äô sign. If we add a ‚Äò*‚Äô sign we assume that there are interactions; meaning that the effect of class depends on gender OR the effect of gender depends on class.\n‚ÄîThe alternative was the ‚Äò+‚Äô sign. If we add a ‚Äò+‚Äô sign we assume that there are no interactions; meaning that the effect of class does not depend on gender. This was our assumption when we were doing linear models.‚Äî\n\nsummary(glm(survived~factor(pclass) * factor(sex), family = \"binomial\", data = titanic))\n\n\nCall:\nglm(formula = survived ~ factor(pclass) * factor(sex), family = \"binomial\", \n    data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5924  -0.5745  -0.5745   0.4902   1.9610  \n\nCoefficients:\n                                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       3.3250     0.4549   7.309 2.68e-13 ***\nfactor(pclass)2                  -1.2666     0.5485  -2.309   0.0209 *  \nfactor(pclass)3                  -3.3621     0.4748  -7.081 1.43e-12 ***\nfactor(sex)male                  -3.9848     0.4815  -8.277  &lt; 2e-16 ***\nfactor(pclass)2:factor(sex)male   0.1617     0.6104   0.265   0.7911    \nfactor(pclass)3:factor(sex)male   2.3039     0.5158   4.467 7.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1741  on 1308  degrees of freedom\nResidual deviance: 1210  on 1303  degrees of freedom\nAIC: 1222\n\nNumber of Fisher Scoring iterations: 5\n\n\nLet‚Äôs make an assumption for a male sitting in first class.\nlog odds of survival =&gt; 3.325 - 3.9848 = -0.6598\nodds of survival =&gt; exp(-0.6598) = 0.5169547\nprobability of survival =&gt; 0.5169547/(1+0.5169547) = 0.3407845 or 34%\nLet‚Äôs make an assumption for a male sitting in second class.\nlog odds of survival =&gt; 3.325 -1.2666 - 3.9848+0.1617 = -1.7647\nodds of survival =&gt; exp(-1.7647) = 0.1712382\nprobability of survival =&gt; 0.1712382/(1+0.1712382) = 0.1462027 or 14.6%\nLet‚Äôs make an assumption for a male sitting in third class.\nlog odds of survival =&gt; 3.325 -3.3621 - 3.9848 +2.3039 = -1.718\nodds of survival =&gt; exp(-1.718) = 0.1794246\nprobability of survival =&gt; 0.1794246/(1+0.1794246) = 0.1521289 or 15.21%\nLet‚Äôs make an assumption for a female sitting in first class.\nlog odds of survival =&gt; 3.325\nodds of survival =&gt; exp(3.325) = 27.799\nprobability of survival =&gt; 27.799/(1+27.799) = 0.9652766 or 96.5%\nLet‚Äôs make an assumption for a female sitting in second class.\nlog odds of survival =&gt; 3.325 -1.2666 = 2.0584\nodds of survival =&gt; exp(2.0584) = 7.833426\nprobability of survival =&gt; 7.833426/(1+7.833426) = 0.8867936 or 88.6%\nLet‚Äôs make an assumption for a female sitting in third class.\nlog odds of survival =&gt; 3.325 -3.3621 = -0.0371\nodds of survival =&gt; exp(-0.0371) = 0.9635798\nprobability of survival =&gt; 0.9635798/(1+0.9635798) = 0.4907261 or 49%\nInterpretation of results\nIt looks like the closer someone is in the first class, the better the chance to survive. However, there is an interesting result between men of third and second category, where the men in third category had slightly higher chances of survival than the men in second class. In general, females had higher chances of survival. Even the females in the third class had 15% higher chance of survival than the males in the first class (49% vs 34%). Around 9 out of 10 women survived in classes 1 and 2. One out of two women survived out of the third class. Only one out of three men in the first class survived while in classes 2 and 3 men had the lowest chances of survival (14.6% and 15.2%). This slight difference though can also be due to random noise.\nHow we arrived to those results:\nAfter running the logistical regression model we come up with the intercept and coefficient number odds which where in log form; hence we had to convert those in actual odds. Before converting them to odds, we sum up the related variables that we want to find in order to also have the log odds of survival (e.g.¬†for female in second class we used the intercept 3.325 and the pclass2 value -1.2666).\nThen, we exponentiate the log odds of survival to find the actual odds of survival (e.g.¬†female in second class = 7.83 ‚Äî which means each female who died, 7.83 survived).\nBecause human brain is not well designed enough to perceive odds as a measurement, we converted those to probability using the formula:\n\\[\nprobability = odds/1+odds\n\\]\nWe did that for each one of the six outcomes\n\nmale 1st class = 34%\nmale 2nd class = 14.6%\nmale 3rd class = 15.2%\nfemale 1st class = 96.5%\nfemale 2nd class = 88.6%\nfemale 3rd class = 49%"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html",
    "href": "software/Analysing_Spotify_Data_in_SQL.html",
    "title": "SQL analysis in big data",
    "section": "",
    "text": "This assignment focuses on helping you get to grips with a new tool: SQL.\nThrough this assignment, we will be working with SQL (specifically pandasql) by exploring a Spotify dataset containing song reviews and statistics. At parts 3 and 4 you can see some text analysis of song reviews."
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#working-with-sql-to-understand-songs-in-spotify",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#working-with-sql-to-understand-songs-in-spotify",
    "title": "SQL analysis in big data",
    "section": "",
    "text": "This assignment focuses on helping you get to grips with a new tool: SQL.\nThrough this assignment, we will be working with SQL (specifically pandasql) by exploring a Spotify dataset containing song reviews and statistics. At parts 3 and 4 you can see some text analysis of song reviews."
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-0-libraries-and-set-up-jargon-the-usual-wall-of-imports",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-0-libraries-and-set-up-jargon-the-usual-wall-of-imports",
    "title": "SQL analysis in big data",
    "section": "Part 0: Libraries and Set Up Jargon (The usual wall of imports)",
    "text": "Part 0: Libraries and Set Up Jargon (The usual wall of imports)\n\n%pip install pandasql\n%pip3 install pandas\n%pip3 install pandasql\n%pip3 install nltk\n%pip3 install wordcloud\n%pip3 install seaborn\n\n1870.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\nUsageError: Line magic function `%pip3` not found.\n\n\nRequirement already satisfied: pandasql in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.7.3)\nRequirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandasql) (1.5.1)\nRequirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandasql) (1.4.42)\nRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandasql) (1.23.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas-&gt;pandasql) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas-&gt;pandasql) (2022.5)\nRequirement already satisfied: six&gt;=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;pandasql) (1.16.0)\nRequirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sqlalchemy-&gt;pandasql) (1.1.3.post0)\nWARNING: You are using pip version 21.2.3; however, version 22.3 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pandas as pd\nimport datetime as dt\nimport pandasql as ps \nimport nltk\nnltk.download('punkt')\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt \n\n[nltk_data] Downloading package punkt to /Users/loizoskon/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# Three datasets that I am using (you can download them from here if the code does not work for you put them in the working directory)\n! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_features.csv\n! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_songs.csv\n! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_rankings.csv\n\n1877.21s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n1882.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n1888.20s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nzsh:1: command not found: wget\nzsh:1: command not found: wget\nzsh:1: command not found: wget\n\n\n\nprint(pd.__version__ )\n\n1.5.1"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-1-loading-processing-our-datasets",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-1-loading-processing-our-datasets",
    "title": "SQL analysis in big data",
    "section": "Part 1: Loading & Processing our Datasets",
    "text": "Part 1: Loading & Processing our Datasets\nBefore we get into the data, we first need to load and clean our datasets."
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-2-exploring-the-data-with-pandassql-and-pandas",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-2-exploring-the-data-with-pandassql-and-pandas",
    "title": "SQL analysis in big data",
    "section": "Part 2: Exploring the Data with PandasSQL (and Pandas)",
    "text": "Part 2: Exploring the Data with PandasSQL (and Pandas)\nNow that we‚Äôre more familiar with the dataset, we‚Äôll now go in SQL language. Specifically, I‚Äôll be using pandasql\nThe typical flow of using pandasql (shortened to ps) is as follows: 1. Write a SQL query in the form of a string (Tip: use triple quotes ‚Äú‚Äú‚Äúx‚Äù‚Äú‚Äù to write multi-line strings) 2. Run the query using ps.sqldf(your_query, locals())\nPandaSQL is convenient as it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the dataframes songs_df, rankings_df and features_df that you have created above!\n###2.1 Bruno Mars songs\n\n2.1.1 How many Bruno Mars‚Äô songs were popular in 2017?\nThe dataframe songs_df contains all top songs in 2017. We want to know whether Bruno Mars was a part of it (he obviously was) - but which of his songs made it to the top?\nUsing pandas, let‚Äôs filter out the songs and reviews from songs_df that were by Bruno Mars. Then, let‚Äôs save this data to a DataFrame called bruno_df that has the following schema:\n\n\n\n\nname\nreviews\n\n\n\n\n\n\n\n#Using pandas to obtain songs by `Bruno Mars`\n# bruno_df = songs_df.groupby('artists' == 'Bruno Mars')\nbruno_df = songs_df[songs_df['artists'] == 'Bruno Mars']\n# bruno_df = songs_df.groupby() #how to get b mars\nbruno_df = bruno_df[['name', 'reviews']]\nbruno_df\n\n\n\n\n\n\n\n\nname\nreviews\n\n\n\n\n8\nThat's What I Like\nYou have a really cool voice! I like the way y...\n\n\n59\n24K Magic\nLyrics are very good. Backing sounds nice.\n\n\n\n\n\n\n\n\n\n2.1.2 How many of Bruno Mars‚Äô songs were deemed ‚Äúgood‚Äù?\nWe now want to see which of these songs contained the word ‚Äúgood‚Äù in the reviews column.\nHere we update bruno_df so that it only contains songs that have the word ‚Äògood‚Äô in the reviews column.\n\n#Using pandasql to obtain only \"good\" songs of bruno mars\ngood_song_query = \"\"\"SELECT * FROM bruno_df WHERE reviews LIKE '%good%'\"\"\" # syntax after like == wildcards - look at the string and ignore everything before% and after% 'good'\nbruno_df = ps.sqldf(good_song_query, locals())\nbruno_df\n\n\n\n\n\n\n\n\nname\nreviews\n\n\n\n\n0\n24K Magic\nLyrics are very good. Backing sounds nice.\n\n\n\n\n\n\n\n###2.2 You can seeing the hit songs\n\n\n2.2.1 Extract the total no. of streams\nWe now want to see what songs formed the top 75% of the year 2017 from rankings_df. We can measure the popularity of the songs using the total number of streams the song received.\n\nYou can see the total number of streams per song and save it into a dataframe called streams_df\nUnderstand the quartile ranges in streams_df\n\n\nrankings_df.describe()\n# we use `.describe()` to understand quartiles. It would be helpful to save the necessary quartile value to use in the querying section that follows.\n\n\n\n\n\n\n\n\nPosition\nStreams\n\n\n\n\ncount\n3.440540e+06\n3.440540e+06\n\n\nmean\n9.465220e+01\n5.188452e+04\n\n\nstd\n5.739412e+01\n2.017733e+05\n\n\nmin\n1.000000e+00\n1.001000e+03\n\n\n25%\n4.500000e+01\n3.321000e+03\n\n\n50%\n9.200000e+01\n9.226000e+03\n\n\n75%\n1.430000e+02\n2.965600e+04\n\n\nmax\n2.000000e+02\n1.138152e+07\n\n\n\n\n\n\n\n\nrankings_df\n\n\n\n\n\n\n\n\nPosition\nTrack Name\nArtist\nStreams\nDate\nRegion\nID\n\n\n\n\n0\n1\nReggaet√≥n Lento (Bailemos)\nCNCO\n19272\n2017-01-01\nec\n3AEZUABDXNtecAOSC1qTf\n\n\n2658317\n118\nSteady 1234 (feat. Jasmine Thompson & Skizzy M...\nVice\n15142\n2017-01-01\nnl\n40UroIGvsMPLPBYwH8rMN\n\n\n2658316\n117\nDynamite (feat. Pretty Sister)\nNause\n15152\n2017-01-01\nnl\n2Ae5awwKvQpTBKQHr1TYC\n\n\n2658315\n116\nHello\nAdele\n15170\n2017-01-01\nnl\n4sPmO7WMQUAf45kwMOtON\n\n\n2658314\n115\nOne Night Stand\nB-Brave\n15510\n2017-01-01\nnl\n2no9x9FRytP9PnB3CQPYS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1498830\n109\nSky Walker\nMiguel\n1581\n2018-01-09\nhu\n5WoaF1B5XIEnWfmb5NZik\n\n\n1498829\n108\nBack to You (feat. Bebe Rexha & Digital Farm A...\nLouis Tomlinson\n1582\n2018-01-09\nhu\n7F9vK8hNFMml4GtHsaXui\n\n\n1498828\n107\nBetrayed\nLil Xan\n1582\n2018-01-09\nhu\n6NWl2m8asvH83xjuXVNsu\n\n\n1498841\n120\nRockabye (feat. Sean Paul & Anne-Marie)\nClean Bandit\n1490\n2018-01-09\nhu\n5knuzwU65gJK7IF5yJsua\n\n\n3441196\n200\nLet Her Go\nPassenger\n2088\n2018-01-09\nhk\n2jyjhRf6DVbMPU5zxagN2\n\n\n\n\n3440540 rows √ó 7 columns\n\n\n\n\nstreams_df = rankings_df.groupby(by = 'Track Name').sum()\nstreams_df\n# Using pandas extract the total number of streams per song from rankings_df\n# streams_df = rankings_df['Streams'&gt;2.965800e+04]\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/1744104746.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  streams_df = rankings_df.groupby(by = 'Track Name').sum()\n\n\n\n\n\n\n\n\n\nPosition\nStreams\n\n\nTrack Name\n\n\n\n\n\n\n\"All That Is or Ever Was or Ever Will Be\"\n383\n7311\n\n\n\"Read All About It, Pt. III\"\n2306\n57025\n\n\n#99\n1126\n31826\n\n\n#Askip\n3184\n296862\n\n\n#Biziz - feat. Lil Bege\n4488\n403591\n\n\n...\n...\n...\n\n\nÏ†ÑÏïº ÂâçÂ§ú The Eve\n17063\n976392\n\n\nÏßÄÎÇòÍ∞à ÌÖåÎãà Been Through\n1180\n87456\n\n\nÏ©îÏñ¥ DOPE\n7545\n209493\n\n\nÌîº ÎïÄ ÎààÎ¨º\n2757\n68673\n\n\nÌîºÏπ¥Î∂Ä Peek-A-Boo\n24895\n2078217\n\n\n\n\n18597 rows √ó 2 columns\n\n\n\n\nstreams_df.describe()\n\n\n\n\n\n\n\n\nPosition\nStreams\n\n\n\n\ncount\n1.859700e+04\n1.859700e+04\n\n\nmean\n1.751114e+04\n9.598901e+06\n\n\nstd\n7.505320e+04\n6.903221e+07\n\n\nmin\n1.000000e+01\n1.001000e+03\n\n\n25%\n3.200000e+02\n2.133400e+04\n\n\n50%\n1.362000e+03\n1.225160e+05\n\n\n75%\n7.620000e+03\n1.072171e+06\n\n\nmax\n1.604987e+06\n2.993989e+09\n\n\n\n\n\n\n\n\n\n2.2.2 Top 75% of streams\nNow that we‚Äôve seen the distribution of the streams, we‚Äôd like to extract songs with streams within the top 75%.\nWe filter out songs from streams_df whose stream count is in the top 75%, then save this data as the Pandas dataframe pd_top_streams.\n\nstreams_df\n\n\n\n\n\n\n\n\nPosition\nStreams\n\n\nTrack Name\n\n\n\n\n\n\n\"All That Is or Ever Was or Ever Will Be\"\n383\n7311\n\n\n\"Read All About It, Pt. III\"\n2306\n57025\n\n\n#99\n1126\n31826\n\n\n#Askip\n3184\n296862\n\n\n#Biziz - feat. Lil Bege\n4488\n403591\n\n\n...\n...\n...\n\n\nÏ†ÑÏïº ÂâçÂ§ú The Eve\n17063\n976392\n\n\nÏßÄÎÇòÍ∞à ÌÖåÎãà Been Through\n1180\n87456\n\n\nÏ©îÏñ¥ DOPE\n7545\n209493\n\n\nÌîº ÎïÄ ÎààÎ¨º\n2757\n68673\n\n\nÌîºÏπ¥Î∂Ä Peek-A-Boo\n24895\n2078217\n\n\n\n\n18597 rows √ó 2 columns\n\n\n\n\n#Using pandasql extract the top 75% based on number of streams\ntop_query = \"\"\"\nSELECT * \nFROM streams_df\nWHERE Streams &gt; 21334\n\"\"\"\n\nsql_top_streams = ps.sqldf(top_query, locals())\nsql_top_streams\n\n\n\n\n\n\n\n\nTrack Name\nPosition\nStreams\n\n\n\n\n0\n\"Read All About It, Pt. III\"\n2306\n57025\n\n\n1\n#99\n1126\n31826\n\n\n2\n#Askip\n3184\n296862\n\n\n3\n#Biziz - feat. Lil Bege\n4488\n403591\n\n\n4\n#CTZK\n20515\n669563\n\n\n...\n...\n...\n...\n\n\n13941\nÏ†ÑÏïº ÂâçÂ§ú The Eve\n17063\n976392\n\n\n13942\nÏßÄÎÇòÍ∞à ÌÖåÎãà Been Through\n1180\n87456\n\n\n13943\nÏ©îÏñ¥ DOPE\n7545\n209493\n\n\n13944\nÌîº ÎïÄ ÎààÎ¨º\n2757\n68673\n\n\n13945\nÌîºÏπ¥Î∂Ä Peek-A-Boo\n24895\n2078217\n\n\n\n\n13946 rows √ó 3 columns\n\n\n\n\n\n2.3 Duration of songs\nNow that we know which songs are hits, we‚Äôd like to listen to songs that are not too short nor too long.\nUsing pandas, we filter out songs from songs_df whose duration is between 3 and 5 minutes.\n\nCreating a new column in songs_df called ‚Äúduration_min‚Äù that converts the duration in ‚Äúduration_ms‚Äù from milliseconds to minutes\nExtracting only songs whose duration is at least 3 minutes and at most 5 minutes. Then, saving the output to ideal_songs_df.\n\n\nsongs_df\n\n\n\n\n\n\n\n\nid\nname\nartists\nduration_ms\nreviews\n\n\n\n\n0\n7qiZfU4dY1lWllzX7mPBI\nShape of You\nEd Sheeran\n233713\nvocal has a nice warm quality.\n\n\n1\n5CtI0qwDJkDQGwXD1H1cL\nDespacito - Remix\nLuis Fonsi\n228827\nVery European feeling. I like that the singer ...\n\n\n2\n4aWmUDTfIPGksMNLV2rQP\nDespacito (Featuring Daddy Yankee)\nLuis Fonsi\n228200\nUnique and quirky. Arrangement was good and ni...\n\n\n3\n6RUKPb4LETWmmr3iAEQkt\nSomething Just Like This\nThe Chainsmokers\n247160\nTastefully put together song. Good instrumenta...\n\n\n4\n3DXncPQOG4VBw3QHh3S81\nI'm the One\nDJ Khaled\n288600\nYour voice is awesome\n\n\n...\n...\n...\n...\n...\n...\n\n\n95\n1PSBzsahR2AKwLJgx8ehB\nBad Things (with Camila Cabello)\nMachine Gun Kelly\n239293\nGood strong voice\n\n\n96\n0QsvXIfqM0zZoerQfsI9l\nDon't Let Me Down\nThe Chainsmokers\n208053\ngood movie sound track\n\n\n97\n7mldq42yDuxiUNn08nvzH\nBody Like A Back Road\nSam Hunt\n165387\nGood melody. A little different\n\n\n98\n7i2DJ88J7jQ8K7zqFX2fW\nNow Or Never\nHalsey\n214802\nGood lyrics...\n\n\n99\n1j4kHkkpqZRBwE0A4CN4Y\nDusk Till Dawn - Radio Edit\nZAYN\n239000\nGood hook - unique vocal presentation - I like...\n\n\n\n\n100 rows √ó 5 columns\n\n\n\n\nsongs_df #convert ms to min\nsongs_df['duration_min'] =  songs_df['duration_ms'] / 60000\nideal_songs_df = songs_df[(songs_df['duration_min'] &gt;= 3) & (songs_df['duration_min'] &lt;=5)]\nideal_songs_df\n\n\n\n\n\n\n\n\nid\nname\nartists\nduration_ms\nreviews\nduration_min\n\n\n\n\n0\n7qiZfU4dY1lWllzX7mPBI\nShape of You\nEd Sheeran\n233713\nvocal has a nice warm quality.\n3.895217\n\n\n1\n5CtI0qwDJkDQGwXD1H1cL\nDespacito - Remix\nLuis Fonsi\n228827\nVery European feeling. I like that the singer ...\n3.813783\n\n\n2\n4aWmUDTfIPGksMNLV2rQP\nDespacito (Featuring Daddy Yankee)\nLuis Fonsi\n228200\nUnique and quirky. Arrangement was good and ni...\n3.803333\n\n\n3\n6RUKPb4LETWmmr3iAEQkt\nSomething Just Like This\nThe Chainsmokers\n247160\nTastefully put together song. Good instrumenta...\n4.119333\n\n\n4\n3DXncPQOG4VBw3QHh3S81\nI'm the One\nDJ Khaled\n288600\nYour voice is awesome\n4.810000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n94\n2fQrGHiQOvpL9UgPvtYy6\nBank Account\n21 Savage\n220307\nGood vocal, Good arrangement, Good subject mat...\n3.671783\n\n\n95\n1PSBzsahR2AKwLJgx8ehB\nBad Things (with Camila Cabello)\nMachine Gun Kelly\n239293\nGood strong voice\n3.988217\n\n\n96\n0QsvXIfqM0zZoerQfsI9l\nDon't Let Me Down\nThe Chainsmokers\n208053\ngood movie sound track\n3.467550\n\n\n98\n7i2DJ88J7jQ8K7zqFX2fW\nNow Or Never\nHalsey\n214802\nGood lyrics...\n3.580033\n\n\n99\n1j4kHkkpqZRBwE0A4CN4Y\nDusk Till Dawn - Radio Edit\nZAYN\n239000\nGood hook - unique vocal presentation - I like...\n3.983333\n\n\n\n\n90 rows √ó 6 columns\n\n\n\n\n\n2.4 Who are the highest ranked artists?\nWhich artists have been ranked #1 the most times in 2017?\nUsing the dataframe rankings_df, we perform the following tasks on pandasql: - We extract the names of artist that have position as 1, and store this data in pd_pos_df - Using pd_pos_df, You can see the number of times each artist was ranked #1 - Then we get the Top 10 artists, i.e.¬†the 10 artists which have been ranked #1 the most times.\nThe dataframe pd_pos_df should have the following schema:\n\n\n\n\nArtist\nPosition\n\n\n\n\n\n\n\n# pandasql\npos_query = \"\"\" \nSELECT Artist, COUNT (Position) AS Position\nFROM rankings_df\nWHERE Position = 1\nGROUP BY Artist\nORDER BY Position DESC\nLIMIT 10\n\"\"\"\n\nsql_pos_df = ps.sqldf(pos_query, locals())\nsql_pos_df\n\n\n\n\n\n\n\n\nArtist\nPosition\n\n\n\n\n0\nLuis Fonsi\n4085\n\n\n1\nEd Sheeran\n3780\n\n\n2\nPost Malone\n1737\n\n\n3\nJ Balvin\n1195\n\n\n4\nMaluma\n900\n\n\n5\nNatti Natasha\n416\n\n\n6\nBad Bunny\n311\n\n\n7\nTaylor Swift\n304\n\n\n8\nDanny Ocean\n291\n\n\n9\nCamila Cabello\n272\n\n\n\n\n\n\n\n\n\n2.5 Popular Artists!\nAre there artists whose songs are streamed more often than others? Let‚Äôs see!\n\nWe consider rows in rankings_df that are during Summer 2017\n\nI assume that the duration of summer is from 15th June 2017 to 16th September 2017 (both dates inclusive)\n\nThen I am finding the total number of streams corresponding to each artist, then storing this data in a new column called Number.\nThen I am sorting this dataframe on the Number column so that the most popular artists appear first (i.e.¬†sort according to the Number column in descending order).\n\nCalling the output dataframe sql_summer_df.\n\n#pandasql\nsummer_query = \"\"\"\nSELECT Artist, SUM(Streams) AS Number\nFROM rankings_df\nWHERE strftime ('%Y-%m-%d', DATE) BETWEEN '2017-06-15' AND '2017-09-16'\nGROUP BY Artist\nORDER BY Number DESC\n\"\"\"\n\nsql_summer_df = ps.sqldf(summer_query, locals())\nsql_summer_df\n\n\n\n\n\n\n\n\nArtist\nNumber\n\n\n\n\n0\nEd Sheeran\n1331033447\n\n\n1\nDJ Khaled\n1298364289\n\n\n2\nLuis Fonsi\n1186212514\n\n\n3\nCalvin Harris\n1033045563\n\n\n4\nJ Balvin\n1021849673\n\n\n...\n...\n...\n\n\n3096\n√Å M√≥ti S√≥l\n1052\n\n\n3097\nHuntar\n1049\n\n\n3098\nThe Panas\n1032\n\n\n3099\nStef√°n Hilmarsson\n1022\n\n\n3100\nDelano\n1010\n\n\n\n\n3101 rows √ó 2 columns\n\n\n\n\n\n2.6 Which songs are danceable but also mellow?\nNow let us switch gears and examine songs_df and features_df. In particular, we want to You can see the songs with high danceability and low tempo.\nSTEPS to to solution: - Round the danceability column to one decimal place, and calling the resultant column r_danceability. This will allow us to conduct a more general (coarser) analysis of the data. - Merge songs_df and features_df, then sort the songs with danceability in descending order and tempo in ascending order. (When sorting, make sure to use the r_danceability column.) - Call the output dataframe sql_songs_features_df.\n\n#pandasql\nsong_feature_query = \"\"\"\nSELECT *, ROUND (danceability, 1) AS r_danceability\nFROM features_df\nJOIN songs_df ON features_df.id = songs_df.id\nORDER BY ROUND (danceability, 1) DESC, tempo ASC\n\"\"\"\n\n\nsql_song_features_df = ps.sqldf(song_feature_query, locals())\nsql_song_features_df\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nid\nname\nartists\nduration_ms\nreviews\nduration_min\nr_danceability\n\n\n\n\n0\n94\n2fQrGHiQOvpL9UgPvtYy6\n0.884\n0.346\n8.0\n-8.228\n0.0\n0.3510\n0.0151\n0.000007\n0.0871\n0.3760\n75.016\n2fQrGHiQOvpL9UgPvtYy6\nBank Account\n21 Savage\n220307\nGood vocal, Good arrangement, Good subject mat...\n3.671783\n0.9\n\n\n1\n42\n5bcTCxgc7xVfSaMV3RuVk\n0.893\n0.745\n11.0\n-3.105\n0.0\n0.0571\n0.0642\n0.000000\n0.0943\n0.8720\n101.018\n5bcTCxgc7xVfSaMV3RuVk\nFeels\nCalvin Harris\n223413\nOverall quite a nice sound.\n3.723550\n0.9\n\n\n2\n47\n6mICuAdrwEjh6Y6lroV2K\n0.852\n0.773\n8.0\n-2.921\n0.0\n0.0776\n0.1870\n0.000030\n0.1590\n0.9070\n102.034\n6mICuAdrwEjh6Y6lroV2K\nChantaje\nShakira\n195840\nNICE TUNE WITH SOME NEAT CHORD CHANGES....VERY...\n3.264000\n0.9\n\n\n3\n38\n6EpRaXYhGOB3fj4V2uDkM\n0.869\n0.485\n6.0\n-5.595\n1.0\n0.0545\n0.2460\n0.000000\n0.0765\n0.5270\n106.028\n6EpRaXYhGOB3fj4V2uDkM\nStrip That Down\nLiam Payne\n204502\nRich clean vocals. Nice incidental instrumenta...\n3.408367\n0.9\n\n\n4\n91\n4c2W3VKsOFoIg2SFaO6DY\n0.855\n0.624\n1.0\n-4.093\n1.0\n0.0488\n0.1580\n0.000000\n0.0513\n0.9620\n117.959\n4c2W3VKsOFoIg2SFaO6DY\nYour Song\nRita Ora\n180757\nGreat music ,Great voice ,arrangement, etc.\n3.012617\n0.9\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n52\n4pdPtRcBmOSQDlJ3Fk945\n0.476\n0.718\n8.0\n-5.309\n1.0\n0.0576\n0.0784\n0.000010\n0.1220\n0.1420\n199.864\n4pdPtRcBmOSQDlJ3Fk945\nLet Me Love You\nDJ Snake\n205947\nNice melodies, especially in the beginning. Ha...\n3.432450\n0.5\n\n\n96\n22\n5uCax9HTNlzGybIStD3vD\n0.358\n0.557\n10.0\n-7.398\n1.0\n0.0590\n0.6950\n0.000000\n0.0902\n0.4940\n85.043\n5uCax9HTNlzGybIStD3vD\nSay You Won't Let Go\nJames Arthur\n211467\ntruely unique, i like it\n3.524450\n0.4\n\n\n97\n63\n6520aj0B4FSKGVuKNsOCO\n0.448\n0.801\n0.0\n-5.363\n1.0\n0.1650\n0.0733\n0.000000\n0.1460\n0.4620\n189.798\n6520aj0B4FSKGVuKNsOCO\nChained To The Rhythm\nKaty Perry\n237734\nLike the recording .nice and clear,smooth\n3.962233\n0.4\n\n\n98\n66\n5hYTyyh2odQKphUbMqc5g\n0.314\n0.555\n9.0\n-9.601\n1.0\n0.3700\n0.1570\n0.000108\n0.0670\n0.1590\n179.666\n5hYTyyh2odQKphUbMqc5g\nHow Far I'll Go - From \"Moana\"\nAlessia Cara\n175517\nKind of reminds me of new music.\n2.925283\n0.3\n\n\n99\n99\n1j4kHkkpqZRBwE0A4CN4Y\n0.258\n0.437\n11.0\n-6.593\n0.0\n0.0390\n0.1010\n0.000001\n0.1060\n0.0967\n180.043\n1j4kHkkpqZRBwE0A4CN4Y\nDusk Till Dawn - Radio Edit\nZAYN\n239000\nGood hook - unique vocal presentation - I like...\n3.983333\n0.3\n\n\n\n\n100 rows √ó 20 columns\n\n\n\n\n\n2.7 Do we like the same songs?\n\n2.7.1 Which regions have the most streams?\n\nExtracted rows belonging to the top 2 regions that have the most streams.\nStoreed output in a new dataframe called sql_top_regions_df. The schema is as rankings_df.\n\nNote: Since we want to focus on specific regions, we should disregard rows where the Region column has the value \"global\".\n\n#pandas\npd_top_regions_df = rankings_df[rankings_df['Region'] != 'global'] #remove global from column region\npd_top_regions_df = pd_top_regions_df.groupby(by = 'Region').sum().reset_index() #group by Region\npd_top_regions_df = pd_top_regions_df.sort_values(by = ['Streams'], ascending = [False]).head(2) #We want the ones with most Streams\npd_top_regions_df = rankings_df[rankings_df['Region'].isin(['us', 'gb'])] #checking for us and gb\npd_top_regions_df #we use that in pandasql below in our subquery\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/905598242.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  pd_top_regions_df = pd_top_regions_df.groupby(by = 'Region').sum().reset_index() #group by Region\n\n\n\n\n\n\n\n\n\nPosition\nTrack Name\nArtist\nStreams\nDate\nRegion\nID\n\n\n\n\n3042585\n143\nThe Sound\nThe 1975\n30799\n2017-01-01\ngb\n316r1KLN0bcmpr7TZcMCX\n\n\n3042584\n142\nIs This Love - Remix\nBob Marley & The Wailers\n31000\n2017-01-01\ngb\n1w5sLDYzYAGI0AkLc6FPl\n\n\n3042583\n141\nSidewalks\nThe Weeknd\n31453\n2017-01-01\ngb\n4h90qkbnW1Qq6pBhoPvwk\n\n\n3042582\n140\nFalse Alarm\nMatoma\n31527\n2017-01-01\ngb\n7gZQfdEQpmwAoPHSbEHzm\n\n\n3042581\n139\nJumpman\nDrake\n31716\n2017-01-01\ngb\n27GmP9AWRs744SzKcpJsT\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n845636\n15\nPick It Up (feat. A$AP Rocky)\nFamous Dex\n687929\n2018-01-09\nus\n3ncgNpxLoBQ65ABk4djDy\n\n\n845635\n14\nRiver (feat. Ed Sheeran)\nEminem\n724892\n2018-01-09\nus\n5UEnHoDYpsxlfzWLZIc7L\n\n\n845634\n13\nCandy Paint\nPost Malone\n735421\n2018-01-09\nus\n42CeaId2XNlxugDvyqHfD\n\n\n845632\n11\nCodeine Dreaming (feat. Lil Wayne)\nKodak Black\n839826\n2018-01-09\nus\n4DTpngLjoHj5gFxEZFeD3\n\n\n845662\n41\nBad At Love\nHalsey\n474255\n2018-01-09\nus\n7y9iMe8SOB6z3NoHE2OfX\n\n\n\n\n148374 rows √ó 7 columns\n\n\n\n\n#pandasql\ntop_regions_query = \"\"\"\nSELECT *\nFROM rankings_df\nWHERE Region IN (\n  SELECT Region FROM pd_top_regions_df\n  WHERE Region&lt;&gt;'global'\n  LIMIT 2\n)\nORDER BY Streams DESC\n\"\"\"\n\n\nsql_top_regions_df = ps.sqldf(top_regions_query, locals())\nsql_top_regions_df\n\n\n\n\n\n\n\n\nPosition\nTrack Name\nArtist\nStreams\nDate\nRegion\nID\n\n\n\n\n0\n1\nLast Christmas\nWham!\n1357938\n2017-12-25 00:00:00.000000\ngb\n2FRnf9qhLbvw8fu4IBXx7\n\n\n1\n1\nShape of You\nEd Sheeran\n1323982\n2017-01-09 00:00:00.000000\ngb\n7qiZfU4dY1lWllzX7mPBI\n\n\n2\n2\nAll I Want for Christmas Is You\nMariah Carey\n1299377\n2017-12-25 00:00:00.000000\ngb\n0bYg9bo50gSsH3LtXe2SQ\n\n\n3\n1\nShape of You\nEd Sheeran\n1280898\n2017-01-06 00:00:00.000000\ngb\n7qiZfU4dY1lWllzX7mPBI\n\n\n4\n1\nShape of You\nEd Sheeran\n1258743\n2017-01-10 00:00:00.000000\ngb\n7qiZfU4dY1lWllzX7mPBI\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n74185\n196\nFamous\nKanye West\n25248\n2017-01-01 00:00:00.000000\ngb\n19a3JfW8BQwqHWUMbcqSx\n\n\n74186\n197\nGirls Just Want to Have Fun\nCyndi Lauper\n25176\n2017-01-01 00:00:00.000000\ngb\n4y1LsJpmMti1PfRQV9AWW\n\n\n74187\n198\nDog Days Are Over\nFlorence + The Machine\n24904\n2017-01-01 00:00:00.000000\ngb\n1YLJVmuzeM2YSUkCCaTNU\n\n\n74188\n199\nHalf the World Away - Remastered\nOasis\n24806\n2017-01-01 00:00:00.000000\ngb\n6z1Xz89avh1KA7d3Ek7DQ\n\n\n74189\n200\nShut Up\nStormzy\n24727\n2017-01-01 00:00:00.000000\ngb\n2LPUvD5DDOO4UYGkWgjI2\n\n\n\n\n74190 rows √ó 7 columns\n\n\n\n\n\n2.7.2 Do the regions with the most streams like different songs?\n\nFinding the songs that the two regions (found in 2.7.1) DO NOT have in common\nStoring the result in a new dataframe called sql_diff_tracks_df.\n\n\n#pandasql\n\ndiff_tracks_query = \"\"\"\nWITH USdf AS(\n   SELECT DISTINCT `Track Name`, Artist, ID\n   FROM pd_top_regions_df\n   WHERE Region='us'),\n\nGBdf AS (\n  SELECT DISTINCT `Track Name`, Artist, ID\n  FROM pd_top_regions_df\n  WHERE Region='gb'),\n\nUSonly AS (\n  SELECT USdf.`Track Name`, USdf.Artist, USdf.ID as ID \n  FROM USdf\n  LEFT JOIN GBdf ON USdf.id = GBdf.id\n  WHERE GBdf.id IS NULL),\n\n GBonly AS (\n   SELECT USdf.`Track Name`, USdf.Artist, GBdf.ID as ID\n   FROM GBdf\n   LEFT JOIN USdf ON USdf.id = GBdf.id\n   WHERE USdf.id IS NULL)\n\n SELECT * FROM USonly\n UNION\n SELECT * FROM GBonly\n\"\"\"\nsql_diff_tracks_df = ps.sqldf(diff_tracks_query, locals())\nsql_diff_tracks_df\n\n\n\n\n\n\n\n\nTrack Name\nArtist\nID\n\n\n\n\n0\nNone\nNone\n00l1uBtEO4WwmsfxqbeTW\n\n\n1\nNone\nNone\n0181HMomm7xM3Ks5YNlA9\n\n\n2\nNone\nNone\n01VXGDL8Ox3SWmvM8ZyvS\n\n\n3\nNone\nNone\n023lag1AgeOf7YChojecR\n\n\n4\nNone\nNone\n02KgB1Qyk4PrFweUMGl9N\n\n\n...\n...\n...\n...\n\n\n2447\nplayboy shit (feat. lil aaron)\nblackbear\n031QO44Ql8D7D3ePdI7fk\n\n\n2448\nsanta monica & la brea\nblackbear\n3Tvs5NbIqszxl7ctruIqa\n\n\n2449\ntop priority (with Ne-Yo)\nblackbear\n2yjwPmZ3XKKoVVPzKQ9d0\n\n\n2450\nup in this (with Tinashe)\nblackbear\n7xmaCmiSOCtQ6nFENK22b\n\n\n2451\nwokeuplikethis*\nPlayboi Carti\n59J5nzL1KniFHnU120dQz\n\n\n\n\n2452 rows √ó 3 columns\n\n\n\n\n\n\n2.8 New Years Eve Party!\nWho doesn‚Äôt love to dance? Let‚Äôs You can see some songs to groove to!\n\nYou can see the songs that made it to the charts in December 2017 (2017-12-01 to 2017-12-31) and whose duration is longer than 3 minutes\nYou can see the artist and the danceability of these songs. Be sure to only include songs with danceability &gt; 0.5.\nThe result is stored in a new dataframe called sql_dance_df that has the following schema:\n\n\n\n\n\nArtist\nTrack Name\ndanceability\n\n\n\n\n\n\nHint: Think about which data resides in which table!\n\n#SQL SKELETON\n\n# FROM\n# WHERE\n# _______\n# GROUP BY\n# HAVING\n# ________\n# SELECT\n# _______\n# ORDER BY\n# LIMIT\n\n\nrankings_df.rename(columns = {'ID':'id'}, inplace = True)\nrankings_df\n#making ID work as id\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/2501768206.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  rankings_df.rename(columns = {'ID':'id'}, inplace = True)\n\n\n\n\n\n\n\n\n\nPosition\nTrack Name\nArtist\nStreams\nDate\nRegion\nid\n\n\n\n\n0\n1\nReggaet√≥n Lento (Bailemos)\nCNCO\n19272\n2017-01-01\nec\n3AEZUABDXNtecAOSC1qTf\n\n\n2658317\n118\nSteady 1234 (feat. Jasmine Thompson & Skizzy M...\nVice\n15142\n2017-01-01\nnl\n40UroIGvsMPLPBYwH8rMN\n\n\n2658316\n117\nDynamite (feat. Pretty Sister)\nNause\n15152\n2017-01-01\nnl\n2Ae5awwKvQpTBKQHr1TYC\n\n\n2658315\n116\nHello\nAdele\n15170\n2017-01-01\nnl\n4sPmO7WMQUAf45kwMOtON\n\n\n2658314\n115\nOne Night Stand\nB-Brave\n15510\n2017-01-01\nnl\n2no9x9FRytP9PnB3CQPYS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1498830\n109\nSky Walker\nMiguel\n1581\n2018-01-09\nhu\n5WoaF1B5XIEnWfmb5NZik\n\n\n1498829\n108\nBack to You (feat. Bebe Rexha & Digital Farm A...\nLouis Tomlinson\n1582\n2018-01-09\nhu\n7F9vK8hNFMml4GtHsaXui\n\n\n1498828\n107\nBetrayed\nLil Xan\n1582\n2018-01-09\nhu\n6NWl2m8asvH83xjuXVNsu\n\n\n1498841\n120\nRockabye (feat. Sean Paul & Anne-Marie)\nClean Bandit\n1490\n2018-01-09\nhu\n5knuzwU65gJK7IF5yJsua\n\n\n3441196\n200\nLet Her Go\nPassenger\n2088\n2018-01-09\nhk\n2jyjhRf6DVbMPU5zxagN2\n\n\n\n\n3440540 rows √ó 7 columns\n\n\n\n\n#pandasql\ndance_query = \"\"\"\nSELECT DISTINCT Artist, `Track Name`,   danceability\nFROM songs_df AS s \n  JOIN rankings_df AS r ON r.id= s.id\n  JOIN features_df AS f ON f.id=r.id\nWHERE duration_min &gt; 3\n  AND strftime('%Y-%m-%d', DATE) BETWEEN '2017-12-01' AND '2017-12-31'\n  AND danceability  &gt; 0.5\n\"\"\"\n\nsql_dance_df = ps.sqldf(dance_query, locals())\nsql_dance_df\n\n\n\n\n\n\n\n\nArtist\nTrack Name\ndanceability\n\n\n\n\n0\nEd Sheeran\nShape of You\n0.825\n\n\n1\nLuis Fonsi\nDespacito - Remix\n0.694\n\n\n2\nLuis Fonsi\nDespacito (Featuring Daddy Yankee)\n0.660\n\n\n3\nThe Chainsmokers\nSomething Just Like This\n0.617\n\n\n4\nDJ Khaled\nI'm the One\n0.609\n\n\n...\n...\n...\n...\n\n\n80\nJustin Bieber\nFriends (with BloodPop¬Æ)\n0.744\n\n\n81\n21 Savage\nBank Account\n0.884\n\n\n82\nMachine Gun Kelly\nBad Things (with Camila Cabello)\n0.675\n\n\n83\nThe Chainsmokers\nDon't Let Me Down\n0.542\n\n\n84\nHalsey\nNow Or Never\n0.658\n\n\n\n\n85 rows √ó 3 columns"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-3-data-visualization",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-3-data-visualization",
    "title": "SQL analysis in big data",
    "section": "Part 3: Data Visualization",
    "text": "Part 3: Data Visualization\nThe popularity of songs fluctuates as time progresses. We want to create a graph that illustrates the no. of streams for the most and least popular songs during each month in 2017.\nPerform the following tasks: - You can see the song that had the most streams on 2017-01-01 - You can see the song that had the least streams on 2017-01-01 - You can see the no. of streams that these two songs received on the first day of each month in 2017 (eg. 2017-01-01, 2017-02-01, 2017-03-01 ‚Ä¶ 2017-12-01)\nPlot a line graph which shows the trend you found! Make sure you use the ID of the two songs when creating this graph.\nuseful resource: https://seaborn.pydata.org/generated/seaborn.lineplot.html\nThe line graph has the following features: 1. The X-axis should be labelled ‚ÄúDate‚Äù, and the Y-axis should be labelled ‚ÄúStreams‚Äù. 2. There should be markers on the plot to specify the no. of streams each song received ia particular month. 3. The lines corresponding to the two songs should have different colors.\n\n#first we take a look which songs are streamed the most\nfilter_rankings = rankings_df[(rankings_df['Date'] == '2017-01-01')] #setting the date asked\nfilter_rankings = filter_rankings.groupby('id').sum().reset_index()\nsorted_rankings = filter_rankings.sort_values(by=['Streams'], ascending=False).reset_index() #sorting from most streams to least\ntop_bottom_rankings = pd.concat([sorted_rankings.head(1), sorted_rankings.tail(1)]) #getting the most and least streamed song \ndisplay(top_bottom_rankings)\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/745237006.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  filter_rankings = filter_rankings.groupby('id').sum().reset_index()\n\n\n\n\n\n\n\n\n\nindex\nid\nPosition\nStreams\n\n\n\n\n0\n1690\n5aAx2yezTd8zXrkmtKl66\n422\n6266206\n\n\n2360\n2302\n7qxgfIAuWUY9VHLn35Sqw\n199\n1001\n\n\n\n\n\n\n\n\n#You can see the no. of streams that these two songs received on the first day of each month in 2017\nlist_of_dates = ['2017-01-01', '2017-02-01', '2017-03-01', '2017-04-01', '2017-05-01', '2017-06-01', '2017-07-01', '2017-08-01', '2017-09-01',\n                 '2017-10-01', '2017-11-01', '2017-12-01']\n\nall_months_rankings = rankings_df[rankings_df['Date'].isin(list_of_dates)] #filter for list_of_dates asked\nall_months_rankings = all_months_rankings[all_months_rankings['id'].isin(top_bottom_rankings['id'])]\nall_months_rankings\n\n\n\n\n\n\n\n\n\nPosition\nTrack Name\nArtist\nStreams\nDate\nRegion\nid\n\n\n\n\n2755014\n18\nStarboy\nThe Weeknd\n16013\n2017-01-01\nco\n5aAx2yezTd8zXrkmtKl66\n\n\n2732400\n1\nStarboy\nThe Weeknd\n2197\n2017-01-01\nsk\n5aAx2yezTd8zXrkmtKl66\n\n\n462008\n10\nStarboy\nThe Weeknd\n104708\n2017-01-01\nph\n5aAx2yezTd8zXrkmtKl66\n\n\n2658204\n5\nStarboy\nThe Weeknd\n76076\n2017-01-01\nnl\n5aAx2yezTd8zXrkmtKl66\n\n\n445200\n1\nStarboy\nThe Weeknd\n2583\n2017-01-01\nlt\n5aAx2yezTd8zXrkmtKl66\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2272744\n185\nStarboy\nThe Weeknd\n44109\n2017-12-01\nmx\n5aAx2yezTd8zXrkmtKl66\n\n\n2124327\n168\nStarboy\nThe Weeknd\n2765\n2017-12-01\npt\n5aAx2yezTd8zXrkmtKl66\n\n\n837998\n177\nStarboy\nThe Weeknd\n219100\n2017-12-01\nus\n5aAx2yezTd8zXrkmtKl66\n\n\n763686\n65\nStarboy\nThe Weeknd\n13676\n2017-12-01\ntr\n5aAx2yezTd8zXrkmtKl66\n\n\n951109\n195\nStarboy\nThe Weeknd\n2393\n2017-12-01\ncr\n5aAx2yezTd8zXrkmtKl66\n\n\n\n\n456 rows √ó 7 columns\n\n\n\n\nIDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\nIDdateGroup\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/1523896782.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  IDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\n\n\n\n\n\n\n\n\n\nid\nDate\nPosition\nStreams\n\n\n\n\n0\n5aAx2yezTd8zXrkmtKl66\n2017-01-01\n422\n6266206\n\n\n1\n5aAx2yezTd8zXrkmtKl66\n2017-02-01\n939\n5697420\n\n\n2\n5aAx2yezTd8zXrkmtKl66\n2017-03-01\n1803\n4259480\n\n\n3\n5aAx2yezTd8zXrkmtKl66\n2017-04-01\n2609\n3263163\n\n\n4\n5aAx2yezTd8zXrkmtKl66\n2017-05-01\n4094\n2179513\n\n\n5\n5aAx2yezTd8zXrkmtKl66\n2017-06-01\n4306\n1906610\n\n\n6\n5aAx2yezTd8zXrkmtKl66\n2017-07-01\n4027\n1641376\n\n\n7\n5aAx2yezTd8zXrkmtKl66\n2017-08-01\n3973\n1348133\n\n\n8\n5aAx2yezTd8zXrkmtKl66\n2017-09-01\n4190\n1380447\n\n\n9\n5aAx2yezTd8zXrkmtKl66\n2017-10-01\n3558\n1061926\n\n\n10\n5aAx2yezTd8zXrkmtKl66\n2017-11-01\n2525\n1040093\n\n\n11\n5aAx2yezTd8zXrkmtKl66\n2017-12-01\n1521\n999344\n\n\n12\n7qxgfIAuWUY9VHLn35Sqw\n2017-01-01\n199\n1001\n\n\n13\n7qxgfIAuWUY9VHLn35Sqw\n2017-02-01\n122\n1440\n\n\n14\n7qxgfIAuWUY9VHLn35Sqw\n2017-03-01\n144\n1245\n\n\n15\n7qxgfIAuWUY9VHLn35Sqw\n2017-04-01\n147\n1198\n\n\n16\n7qxgfIAuWUY9VHLn35Sqw\n2017-07-01\n160\n1112\n\n\n\n\n\n\n\n\nIDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\nIDdateGroup\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/1523896782.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  IDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\n\n\n\n\n\n\n\n\n\nid\nDate\nPosition\nStreams\n\n\n\n\n0\n5aAx2yezTd8zXrkmtKl66\n2017-01-01\n422\n6266206\n\n\n1\n5aAx2yezTd8zXrkmtKl66\n2017-02-01\n939\n5697420\n\n\n2\n5aAx2yezTd8zXrkmtKl66\n2017-03-01\n1803\n4259480\n\n\n3\n5aAx2yezTd8zXrkmtKl66\n2017-04-01\n2609\n3263163\n\n\n4\n5aAx2yezTd8zXrkmtKl66\n2017-05-01\n4094\n2179513\n\n\n5\n5aAx2yezTd8zXrkmtKl66\n2017-06-01\n4306\n1906610\n\n\n6\n5aAx2yezTd8zXrkmtKl66\n2017-07-01\n4027\n1641376\n\n\n7\n5aAx2yezTd8zXrkmtKl66\n2017-08-01\n3973\n1348133\n\n\n8\n5aAx2yezTd8zXrkmtKl66\n2017-09-01\n4190\n1380447\n\n\n9\n5aAx2yezTd8zXrkmtKl66\n2017-10-01\n3558\n1061926\n\n\n10\n5aAx2yezTd8zXrkmtKl66\n2017-11-01\n2525\n1040093\n\n\n11\n5aAx2yezTd8zXrkmtKl66\n2017-12-01\n1521\n999344\n\n\n12\n7qxgfIAuWUY9VHLn35Sqw\n2017-01-01\n199\n1001\n\n\n13\n7qxgfIAuWUY9VHLn35Sqw\n2017-02-01\n122\n1440\n\n\n14\n7qxgfIAuWUY9VHLn35Sqw\n2017-03-01\n144\n1245\n\n\n15\n7qxgfIAuWUY9VHLn35Sqw\n2017-04-01\n147\n1198\n\n\n16\n7qxgfIAuWUY9VHLn35Sqw\n2017-07-01\n160\n1112\n\n\n\n\n\n\n\n\nimport seaborn\nseaborn.lineplot(data=IDdateGroup, x='Date', y='Streams', markers=True, hue='id')\n\n&lt;AxesSubplot: xlabel='Date', ylabel='Streams'&gt;"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-4-working-with-text-data",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-4-working-with-text-data",
    "title": "SQL analysis in big data",
    "section": "Part 4: Working with Text Data",
    "text": "Part 4: Working with Text Data\nNow, let‚Äôs switch gears and try to text-based analysis. Textual data is complex, but can also be used to generate extremely interpretable results, making it both valuable and interesting.\nThroughout this section, we will attempt to answer the following question:\nAccording to the songs_df dataframe, what do the reviews for the Top Tracks of 2017 look like?\n###4.1 Tokenizing the text\nWe are going to split the contents of in the Reviews column into a list of words. We will use the nltk library, which contains an extensive set of tools for text processing. We are only going to use the following components of the library: - nltk.word_tokenize(): a function used to tokenize text - nltk.corpus.stopwords: a list of commonly used words such as ‚Äúa‚Äù, ‚Äúan‚Äù,‚Äúin‚Äù that are often ignored in text analysis\nNote that for this question, we didn‚Äôt have to clean the text data first as our original dataset was well-formatted. However, in practice, we would typically clean the text first using regular expressions (regex). Keep this in mind as you work on the project later on in the semester.\n\nUsing nltk.corpus.stopwords to create a set containing the most common English stopwords.\nImplementing the function tokenized_content(content), which takes in a string and does the following:\n\n\nTokenizing the text\nKeeping tokens that only contain alphabetic characters (i.e.¬†tokens with no punctuation)\nConverting each token to lowercase\nRemoving stopwords (commonly used words such as ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúin‚Äù)\n\n\nimport nltk\nnltk.__version__\n\n'3.7'\n\n\n\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('stopwords')\nstopwords = set(stopwords.words('english'))\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/loizoskon/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nfrom nltk.tokenize import word_tokenize\n\ndef tokenize_content(content):\n  content = word_tokenize(content)\n  content = [word for word in content if word.isalpha()]\n  content = [word.lower() for word in content]\n  content = [word for word in content if word not in stopwords]\n  return content\n\nWhat is happening in the following columns - Extracting the reviews column of songs_df as a list called reviews. - Applying your tokenize_content() function to each item in the list reviews. Calling the resultant list top_tokens_list. - Flattening the list top_tokens_list, and call the resultant list top_tokens.\n\nreviews = songs_df['reviews'].tolist() #Extract the reviews column of songs_df as a list called reviews.\n\n\n# tokenize and flatten\n\ntop_tokens_list = []\nfor i in reviews:\n     top_tokens_list.append(tokenize_content(i))\n\ntop_tokens = [item for sublist in top_tokens_list for item in sublist]\n\n\n4.2 Most Frequent Words\nHere you can see the 20 most common words in the list top_tokens. Saving the result as a list of (word, count) tuples, in descending order of count.\nFor this question, Counter from the Python collections library is used: https://docs.python.org/2/library/collections.html#counter-objects\n\nfrom collections import Counter\ncount = Counter(top_tokens)\ntop_most_common = count.most_common(20)\nprint(top_most_common)\n\n[('good', 35), ('nice', 31), ('like', 26), ('song', 21), ('voice', 18), ('great', 14), ('really', 13), ('unique', 12), ('lyrics', 12), ('sound', 11), ('love', 9), ('interesting', 9), ('vocal', 8), ('tune', 8), ('vocals', 8), ('instrumentation', 7), ('melody', 7), ('music', 7), ('feel', 6), ('overall', 6)]\n\n\n\n\n4.3 Word Clouds\nBefore we move on from this dataset, let‚Äôs visualize our results using a word cloud.\nHere I create a word cloud containing all the words in the list top_tokens (created in question 4.1). The WordCloud documentation contains instructions on how to do this.\n\nimport re #Load the regular expression library\nimport wordcloud # Import the wordcloud library\n\n\n# Print the titles of the first rows \n#Here I create a word cloud for top tokens\nplt.subplots(figsize = (20,10))\n\nwordcloud = WordCloud (\n                    background_color = 'white',\n                    width = 510,\n                    height = 380\n                        ).generate_from_frequencies(count)\nplt.imshow(wordcloud) # image show\nplt.axis('off') # to off the axis of x and y\nplt.show()"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some things I made\n\nRStudio projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      Google Trends\n      Top 3 American Animated Sitcoms\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Class Die Experiment\n      Correct guesses Vs cheats ~ summary statistics\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Titanic Dataset\n      Probabilities of surviving in terms of class and gender\n   \n  \n\nPython projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      State of The Union Addresses\n      US presidents speech, keyword similarity analysis\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Credit Card Application\n      Predicting a  customer's credit card application outcome\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      SQL - Sorting and Analysing Spotify Data\n      Performing SQL functions on a dataset containing Top 2017 Spotify Tracks, along with their reviews and rankings.\n   \n  \n\n\nNo matching items"
  },
  {
    "objectID": "software/expoftruth.html",
    "href": "software/expoftruth.html",
    "title": "Class Die Experiment",
    "section": "",
    "text": "library(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úî ggplot2 3.3.5      ‚úî purrr   0.3.4 \n‚úî tibble  3.1.6      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.0 \n‚úî readr   2.0.1      ‚úî forcats 0.5.1 \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(descr)\nlibrary(infer)\n\nHere we will have to work with the experiment data we collected for this class.\n\nWe are interested in whether some people were more likely to make correct guesses. What are the summary statistics for all draws? Plot a histogram. Are there any outliers in the data (i.e. individuals who guess 0 times correctly and individuals who guessed all 20 times correctly?)\nwe draw a histogram based on the summary statistics. To find\n\nlibrary(readxl)\nexp21 &lt;- read_excel(\"experiment_2021C(1)copy.xlsx\", col_names = FALSE,\n    skip = 2)\n\nNew names:\n‚Ä¢ `` -&gt; `...1`\n‚Ä¢ `` -&gt; `...2`\n‚Ä¢ `` -&gt; `...3`\n‚Ä¢ `` -&gt; `...4`\n‚Ä¢ `` -&gt; `...5`\n‚Ä¢ `` -&gt; `...6`\n‚Ä¢ `` -&gt; `...7`\n‚Ä¢ `` -&gt; `...8`\n‚Ä¢ `` -&gt; `...9`\n‚Ä¢ `` -&gt; `...10`\n‚Ä¢ `` -&gt; `...11`\n‚Ä¢ `` -&gt; `...12`\n‚Ä¢ `` -&gt; `...13`\n‚Ä¢ `` -&gt; `...14`\n‚Ä¢ `` -&gt; `...15`\n‚Ä¢ `` -&gt; `...16`\n‚Ä¢ `` -&gt; `...17`\n‚Ä¢ `` -&gt; `...18`\n‚Ä¢ `` -&gt; `...19`\n‚Ä¢ `` -&gt; `...20`\n‚Ä¢ `` -&gt; `...21`\n‚Ä¢ `` -&gt; `...22`\n‚Ä¢ `` -&gt; `...23`\n‚Ä¢ `` -&gt; `...24`\n‚Ä¢ `` -&gt; `...25`\n‚Ä¢ `` -&gt; `...26`\n‚Ä¢ `` -&gt; `...27`\n‚Ä¢ `` -&gt; `...28`\n‚Ä¢ `` -&gt; `...29`\n‚Ä¢ `` -&gt; `...30`\n‚Ä¢ `` -&gt; `...31`\n‚Ä¢ `` -&gt; `...32`\n‚Ä¢ `` -&gt; `...33`\n‚Ä¢ `` -&gt; `...34`\n‚Ä¢ `` -&gt; `...35`\n‚Ä¢ `` -&gt; `...36`\n‚Ä¢ `` -&gt; `...37`\n‚Ä¢ `` -&gt; `...38`\n‚Ä¢ `` -&gt; `...39`\n‚Ä¢ `` -&gt; `...40`\n‚Ä¢ `` -&gt; `...41`\n‚Ä¢ `` -&gt; `...42`\n‚Ä¢ `` -&gt; `...43`\n‚Ä¢ `` -&gt; `...44`\n‚Ä¢ `` -&gt; `...45`\n‚Ä¢ `` -&gt; `...46`\n‚Ä¢ `` -&gt; `...47`\n‚Ä¢ `` -&gt; `...48`\n‚Ä¢ `` -&gt; `...49`\n‚Ä¢ `` -&gt; `...50`\n‚Ä¢ `` -&gt; `...51`\n‚Ä¢ `` -&gt; `...52`\n‚Ä¢ `` -&gt; `...53`\n‚Ä¢ `` -&gt; `...54`\n‚Ä¢ `` -&gt; `...55`\n‚Ä¢ `` -&gt; `...56`\n‚Ä¢ `` -&gt; `...57`\n‚Ä¢ `` -&gt; `...58`\n‚Ä¢ `` -&gt; `...59`\n‚Ä¢ `` -&gt; `...60`\n‚Ä¢ `` -&gt; `...61`\n‚Ä¢ `` -&gt; `...62`\n‚Ä¢ `` -&gt; `...63`\n‚Ä¢ `` -&gt; `...64`\n‚Ä¢ `` -&gt; `...65`\n‚Ä¢ `` -&gt; `...66`\n‚Ä¢ `` -&gt; `...67`\n‚Ä¢ `` -&gt; `...68`\n‚Ä¢ `` -&gt; `...69`\n‚Ä¢ `` -&gt; `...70`\n‚Ä¢ `` -&gt; `...71`\n‚Ä¢ `` -&gt; `...72`\n‚Ä¢ `` -&gt; `...73`\n‚Ä¢ `` -&gt; `...74`\n‚Ä¢ `` -&gt; `...75`\n‚Ä¢ `` -&gt; `...76`\n‚Ä¢ `` -&gt; `...77`\n‚Ä¢ `` -&gt; `...78`\n‚Ä¢ `` -&gt; `...79`\n‚Ä¢ `` -&gt; `...80`\n‚Ä¢ `` -&gt; `...81`\n‚Ä¢ `` -&gt; `...82`\n‚Ä¢ `` -&gt; `...83`\n‚Ä¢ `` -&gt; `...84`\n‚Ä¢ `` -&gt; `...85`\n‚Ä¢ `` -&gt; `...86`\n‚Ä¢ `` -&gt; `...87`\n‚Ä¢ `` -&gt; `...88`\n‚Ä¢ `` -&gt; `...89`\n‚Ä¢ `` -&gt; `...90`\n‚Ä¢ `` -&gt; `...91`\n‚Ä¢ `` -&gt; `...92`\n‚Ä¢ `` -&gt; `...93`\n‚Ä¢ `` -&gt; `...94`\n‚Ä¢ `` -&gt; `...95`\n‚Ä¢ `` -&gt; `...96`\n‚Ä¢ `` -&gt; `...97`\n‚Ä¢ `` -&gt; `...98`\n‚Ä¢ `` -&gt; `...99`\n‚Ä¢ `` -&gt; `...100`\n‚Ä¢ `` -&gt; `...101`\n‚Ä¢ `` -&gt; `...102`\n‚Ä¢ `` -&gt; `...103`\n‚Ä¢ `` -&gt; `...104`\n‚Ä¢ `` -&gt; `...105`\n‚Ä¢ `` -&gt; `...106`\n‚Ä¢ `` -&gt; `...107`\n‚Ä¢ `` -&gt; `...108`\n‚Ä¢ `` -&gt; `...109`\n‚Ä¢ `` -&gt; `...110`\n‚Ä¢ `` -&gt; `...111`\n‚Ä¢ `` -&gt; `...112`\n‚Ä¢ `` -&gt; `...113`\n‚Ä¢ `` -&gt; `...114`\n‚Ä¢ `` -&gt; `...115`\n‚Ä¢ `` -&gt; `...116`\n‚Ä¢ `` -&gt; `...117`\n‚Ä¢ `` -&gt; `...118`\n‚Ä¢ `` -&gt; `...119`\n‚Ä¢ `` -&gt; `...120`\n‚Ä¢ `` -&gt; `...121`\n‚Ä¢ `` -&gt; `...122`\n‚Ä¢ `` -&gt; `...123`\n‚Ä¢ `` -&gt; `...124`\n‚Ä¢ `` -&gt; `...125`\n‚Ä¢ `` -&gt; `...126`\n‚Ä¢ `` -&gt; `...127`\n‚Ä¢ `` -&gt; `...128`\n‚Ä¢ `` -&gt; `...129`\n‚Ä¢ `` -&gt; `...130`\n‚Ä¢ `` -&gt; `...131`\n‚Ä¢ `` -&gt; `...132`\n‚Ä¢ `` -&gt; `...133`\n‚Ä¢ `` -&gt; `...134`\n‚Ä¢ `` -&gt; `...135`\n‚Ä¢ `` -&gt; `...136`\n‚Ä¢ `` -&gt; `...137`\n‚Ä¢ `` -&gt; `...138`\n‚Ä¢ `` -&gt; `...139`\n‚Ä¢ `` -&gt; `...140`\n‚Ä¢ `` -&gt; `...141`\n‚Ä¢ `` -&gt; `...142`\n‚Ä¢ `` -&gt; `...143`\n‚Ä¢ `` -&gt; `...144`\n‚Ä¢ `` -&gt; `...145`\n‚Ä¢ `` -&gt; `...146`\n‚Ä¢ `` -&gt; `...147`\n‚Ä¢ `` -&gt; `...148`\n‚Ä¢ `` -&gt; `...149`\n‚Ä¢ `` -&gt; `...150`\n‚Ä¢ `` -&gt; `...151`\n‚Ä¢ `` -&gt; `...152`\n‚Ä¢ `` -&gt; `...153`\n‚Ä¢ `` -&gt; `...154`\n‚Ä¢ `` -&gt; `...155`\n‚Ä¢ `` -&gt; `...156`\n\n\nI find all the outcome columns and I name them after the variable names of our dataset.\n\nvar_names &lt;- read_excel(\"experiment_2021C(1)copy.xlsx\", n_max = 1) \n\nNew names:\n‚Ä¢ `Q171_First Click` -&gt; `Q171_First Click...46`\n‚Ä¢ `Q171_Last Click` -&gt; `Q171_Last Click...47`\n‚Ä¢ `Q171_Page Submit` -&gt; `Q171_Page Submit...48`\n‚Ä¢ `Q171_Click Count` -&gt; `Q171_Click Count...49`\n‚Ä¢ `Q171_First Click` -&gt; `Q171_First Click...82`\n‚Ä¢ `Q171_Last Click` -&gt; `Q171_Last Click...83`\n‚Ä¢ `Q171_Page Submit` -&gt; `Q171_Page Submit...84`\n‚Ä¢ `Q171_Click Count` -&gt; `Q171_Click Count...85`\n\ncolnames(exp21) &lt;- colnames(var_names)\n\nrm(var_names) \n\ncolumnss &lt;- exp21 %&gt;% \n  select(starts_with(\"Outcome\"), \"Age\", \"Risk\", \"Gender\", \"SC0\") \n\nNow we pivot longer to have all the outcomes in line. This helps to name the correct answers as ‚Äú1‚Äù and the wrong answers as ‚Äú0‚Äù.\n\ncountcol &lt;- columnss %&gt;% \n  pivot_longer(cols = starts_with(\"Outcome\"), names_to= \"draw\", values_to = \"answer\") %&gt;%\n  mutate(count= case_when(answer == \"Yes (bonus payment of $0.10)\" ~ 1,\n         answer == \"No (no bonus payment)\" ~ 0))\n\nAfter we pivot wider back in its first format. This helps us to have the data\n\npilpi &lt;- countcol %&gt;% select(-answer) %&gt;% mutate(id = rep(1:133, each = 20)) %&gt;%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender), names_from = draw, values_from = count)\n\nWe use pivot longer once more in order to have all the observations in one column. Having the observations in one column, is easier for us to group by the id column we created before. So we groupby id. This helps us identify what each person done. In addition, we add a column using mutate in order to have the clear result of each person.\n\npi &lt;- pilpi %&gt;% pivot_longer(cols = starts_with(\"Outcome\"), names_to= \"draw\", values_to = \"answer\") %&gt;% group_by(id) \n\nti&lt;- pi %&gt;% mutate(summm=sum(answer))\n\ntii&lt;- pi %&gt;% mutate(summm=sum(answer)) %&gt;% separate(draw, into = c(\"rolsep\", \"rolnum\"), sep = \"e\") %&gt;% group_by(id)\n\n# boxplot.stats(data$score)$out\n# \n# rr &lt;- ti %&gt;% separate(draw, into = c(\"rolsep\", \"rolnum\"), sep = \"e\") %&gt;% group_by(id, rolnum)\n\nNow we pivot wider once more so the data can be presented in the form that shows how people are doing from the 1st one until the last (133rd).\n\nfinal &lt;- ti %&gt;%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender, summm), names_from = draw, values_from = answer)\n\n# finally &lt;- tii %&gt;%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender, summm), names_from = rolnum, values_from = answer)\n# \n#   fin &lt;- final %&gt;% inner_join(ex22, by = \"id\")\n\nWe use the code below to do our Histogram.\n\nggplot(final, aes(summm)) + geom_histogram(bins = 21) + geom_rug()\n\n\n\n\nAnalyze the data by gender, the draw number, and by experimental condition. Are there differences between the genders in terms of the number of wins versus losses? Are there differences between the experimental conditions? Are people more or less likely to win towards the last draws as opposed to the beginning of the experiment?\n‚ÄòFinal‚Äô is the column that already has everything needed for the analysis. The only missing column is conditions. To do that, I used mutate to create a new column named as condition where each real condition is named after a number from one to five. I also create an id column in the ex2 dataset so I can then inner_join it with our main column.\n\nex2 &lt;- exp21 %&gt;%\n         mutate(condition = case_when(Control_Msg != \"\" ~ 1,\n                               Norm_Pos_Msg != \"\" ~ 2,\n                               Norm_Neg_Msg != \"\" ~ 3,\n                               Emp_Neg_Msg != \"\" ~ 4,\n                               Emp_Pos_Msg != \"\" ~ 5),\n                id = 1:133,\n                rolnum = 1:133) \n\nAt this point, I create a new dataset with ‚Äòid‚Äô and ‚Äòcondition‚Äô columns so it is easier to inner_join them. Then I inner_join them so I have everything in one dataset named ‚Äòfin‚Äô.\n\nex22 &lt;- ex2 %&gt;% select(id, condition, rolnum)\n\nfin &lt;- final %&gt;% inner_join(ex22, by = \"id\")\n\nThen we group by Gender and summarise by mean. This allows us to see the mean score that each gender got correct. As we can see, females reported a higher correct score than males on average.\n\nfin %&gt;% group_by(Gender) %&gt;% summarise(mean(summm))\n\n# A tibble: 2 √ó 2\n  Gender `mean(summm)`\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Female          4.75\n2 Male            3.68\n\n# mutate(rrr, coun)\n# fin %&gt;% group_by(condition) %&gt;% summarise(mean(summm))\n\nAnalyzing the data regarding the draw number we use the tii dataset. The reason is because this dataset was created in question 1 with a function named seperate. This function separated the column ‚Äòdraw‚Äô into two others (rolsep, rolsum). The first stands for the title of the draw column while the second one stands for the values that a correct answer was reported.\nTherefore, we here use the filter column(rolnum) equal with the number of observation we want piped with summary(answer). This helps us find the stats behind the correct answers in each round. Also, we named a new dataset ‚Äòi‚Äô where it excludes some irrelevant columns for the purpose of analyzing the data based on draws.\nRegarding the draws we can see that the mean for outcome number 1 is almost 20% (0.1955). That means that one out of five people reported a correct answer in that case. As we can see the mean of correct responses ranges from 0.16 to 0.27. This means that the standard deviation is relatively low. In other words all correct response rates are clustered around the mean.\n\ni &lt;- tii %&gt;% select(-Gender, -Age, -SC0, -id, -Risk, -rolsep, -summm, -id)\n\nAdding missing grouping variables: `id`\n\ni %&gt;% filter(rolnum==1) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==2) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==3) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==4) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==5) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2331  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==6) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==7) %&gt;% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==8) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==9) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==10) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2105  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==11) %&gt;% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==12) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==13) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==14) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2707  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==15) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==16) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==17) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2256  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==18) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2105  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==19) %&gt;% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.218  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==20) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2632  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\n\nRegarding the experimental condition of the data, we use the groupby function to call it in the fin dataset, and then use summarise to find the mean. Alongside, as we set above, each number corresponds in one condition. As we can see, the most popular experimental condition is normative positive message (2) while the least popular is empirical positive message (5).\n\nfin %&gt;% group_by(condition) %&gt;% summarise(mean(summm))\n\n# A tibble: 5 √ó 2\n  condition `mean(summm)`\n      &lt;dbl&gt;         &lt;dbl&gt;\n1         1          5.52\n2         2          6.08\n3         3          4.20\n4         4          3.64\n5         5          3.36\n\n\nRedo part 2 after excluding outliers. Do you still see the different in the experimental groups after outliers are eliminated? Do you see differences between the first and the last rolls? Are there any differences by gender?\n\n# trial &lt;- fin %&gt;% select(-Gender, -Age, -SC0, -Risk, -Outcome1, -Outcome2, -Outcome3, -Outcome4, -Outcome5, -Outcome6, -Outcome7, -Outcome8, -Outcome9, -Outcome10, -Outcome11, -Outcome12, -Outcome13, -Outcome14, -Outcome15, -Outcome16, -Outcome17, -Outcome18, -Outcome19, -Outcome20, -rolnum) %&gt;% \n#   boxplot(trial)\n# \n# boxplot.stats(trial$summm)$out \n\n\nboxplot(fin$summm,\n  ylab = \"summm\")\n\n\n\nboxplot.stats(fin$summm)$out\n\n[1] 10 17 20 11 13 10 11 13\n\nout &lt;- boxplot.stats(fin$summm)$out\nout_ind &lt;- which(fin$summm %in% c(out))\nout_ind\n\n[1]  15  25  48  57  89  98 121 130\n\n# ggplot(fin) +\n#   aes(x = summm) +\n#   geom_histogram(bins = 21, fill = \"#0c4c8a\") +\n#   theme_minimal()\n\nAfter, wecreate a new dataset called ‚Äòdesperate‚Äô where we remove the indicative rows with the outliers. This will help us redo exercise 2\n\ndesperate &lt;- fin[-c(15, 25,48,57,89,98,121,130), ]\n\nRedoing exercise 2 with the new dataframe ‚Äòdesperate‚Äô.\nIn comparison with exercise 2, we can see that the mean for both male and female has fallen. This was a natural outcome to follow since the outliers were all above the mean. However, the outcome remained the same in matters of which gender reported the most correct answers. On average females report 4 correct answers out of twenty, as opposed to men (3/20).\n\ndesperate %&gt;% group_by(Gender) %&gt;% summarise(mean(summm))\n\n# A tibble: 2 √ó 2\n  Gender `mean(summm)`\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Female          4.19\n2 Male            3.14\n\n\nFor the draws, we are creating a new column to make the analysis named ‚Äòabcdefg‚Äô. This is to exclude the outlier rows from the the ‚Äòtii‚Äô dataset we‚Äôve used at exercise 2. After doing that though, we can see that the mean correct response reporting ratio remained at the same levels ranging from 16.54% to 27.07%. This indicates that the outliers did not affect the variability of the data. No difference between the first and last rows.\n\nabcdefg &lt;- tii[-c(15, 25,48,57,89,98,121,130), ]\n\ni &lt;- abcdefg %&gt;% select(-Gender, -Age, -SC0, -id, -Risk, -summm, -rolsep, -id)\n\nAdding missing grouping variables: `id`\n\ni %&gt;% filter(rolnum==1) %&gt;% summary(answer)\n\n       id            rolnum              answer     \n Min.   :  1.00   Length:132         Min.   :0.000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.000  \n Median : 67.50   Mode  :character   Median :0.000  \n Mean   : 67.45                      Mean   :0.197  \n 3rd Qu.:100.25                      3rd Qu.:0.000  \n Max.   :133.00                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==2) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==3) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==4) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==5) %&gt;% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.49                      Mean   :0.2348  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==6) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==7) %&gt;% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==8) %&gt;% summary(answer)\n\n       id            rolnum              answer     \n Min.   :  1.00   Length:132         Min.   :0.000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.000  \n Median : 67.50   Mode  :character   Median :0.000  \n Mean   : 67.48                      Mean   :0.197  \n 3rd Qu.:100.25                      3rd Qu.:0.000  \n Max.   :133.00                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==9) %&gt;% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.47                      Mean   :0.2348  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==10) %&gt;% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.45                      Mean   :0.2121  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==11) %&gt;% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==12) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==13) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==14) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2707  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==15) %&gt;% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  2.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.50                      Mean   :0.2576  \n 3rd Qu.:100.25                      3rd Qu.:1.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==16) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==17) %&gt;% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.48                      Mean   :0.2273  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==18) %&gt;% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.47                      Mean   :0.2045  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %&gt;% filter(rolnum==19) %&gt;% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.218  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %&gt;% filter(rolnum==20) %&gt;% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2632  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\n\nIn the case of experimental conditions we can see that there are some significant differences. The range between them became smaller (3.36 to 4.69). In other words the data became more disperse. While there is no difference in the lower end, there is a difference in the higher end of the range. This indicates that the previous means were inflated due to the high outliers. For insance, the most popular experimental condition, normative positive message (2), was 6.08, whereas now is just 4.33.\n\ndesperate %&gt;% group_by(condition) %&gt;% summarise(mean(summm))\n\n# A tibble: 5 √ó 2\n  condition `mean(summm)`\n      &lt;dbl&gt;         &lt;dbl&gt;\n1         1          4.70\n2         2          4.33\n3         3          3.74\n4         4          3.43\n5         5          3.36\n\n\nSplit your sample into the \"younger\" half and the \"older half. Are there differences between the two age groups?\nFirst we sort the date from highest to lowest reported age using the order function.\n\n\nordered&lt;- fin[order(fin$Age, decreasing = TRUE), ]\n\nThen we find the Age median from fin dataset. As we can see, the median is 24 (in the column 67 since is exactly the middle of 133).\n\nmedian(fin$Age)\n\n[1] 24\n\n\nSince I have the ordered dataframe (from the highest to lowest Age), I use it to create two other dataframes, one for the younger Age group ‚Äúorderedyoung‚Äù, and one for the older Age group ‚Äúorderedold‚Äù.\n\norderedyoung &lt;- ordered[-c(1:66), ]\norderedold &lt;- ordered[-c(68:133), ]\n\norderedold%&gt;% ggplot(aes(x= Age, y= summm)) + geom_point()\n\nboxplot(orderedold$Age,\n  ylab = \"Age\")\n\n\n\nboxplot.stats(orderedold$Age)$out\n\n[1] 56 51 46 35\n\nout &lt;- boxplot.stats(orderedold$Age)$out\nout_ind &lt;- which(orderedold$Age %in% c(out))\nout_ind\n\n[1] 1 2 3 4\n\n\n\nboxplot(orderedyoung$Age,\n  ylab = \"Age\")\n\n\n\nboxplot.stats(orderedyoung$Age)$out\n\n[1] 20\n\nout &lt;- boxplot.stats(orderedyoung$Age)$out\nout_ind &lt;- which(orderedyoung$Age %in% c(out))\nout_ind\n\n[1] 67\n\n\n\noo &lt;- orderedold[-c(1,2,3,4), ]\n\noy &lt;- orderedyoung[-c(67), ]\n\nI use the ggplot function to create a graph that indicates the Age of the participants in the x axes, and the number of correct guesses (summm) in the y axes. As we can see, while the Age column now indicates only the oldest people, the majority is still under 30 ‚Äì closer to the median. From the age 24 until 30s, the amount of correct responses seems to decline. We use the median and the mean code in order to see if there is a difference with them. Median indicates that there is no difference among the data as the reported correct outcome. However, if we look at the mean of the two, the younger group tends to report more correct answers on average than the older group. This difference is small though (4.62-3.92)\n\noo%&gt;% ggplot(aes(x= Age, y= summm)) + geom_point()\n\n\n\nmedian(oo$summm)\n\n[1] 4\n\nmean(oo$summm)\n\n[1] 3.920635\n\noy%&gt;% ggplot(aes(x= Age, y= summm)) + geom_point()\n\n\n\nmedian(oy$summm)\n\n[1] 4\n\nmean(oy$summm)\n\n[1] 4.621212\n\n\n\nPART TWO\nLet's study how the number of wins depends on certain factors:\n\nlibrary(readxl)\nexp21 &lt;- read_excel(\"experiment_2021C(1)copy.xlsx\", col_names = FALSE,\n    skip = 2)\n\nNew names:\n‚Ä¢ `` -&gt; `...1`\n‚Ä¢ `` -&gt; `...2`\n‚Ä¢ `` -&gt; `...3`\n‚Ä¢ `` -&gt; `...4`\n‚Ä¢ `` -&gt; `...5`\n‚Ä¢ `` -&gt; `...6`\n‚Ä¢ `` -&gt; `...7`\n‚Ä¢ `` -&gt; `...8`\n‚Ä¢ `` -&gt; `...9`\n‚Ä¢ `` -&gt; `...10`\n‚Ä¢ `` -&gt; `...11`\n‚Ä¢ `` -&gt; `...12`\n‚Ä¢ `` -&gt; `...13`\n‚Ä¢ `` -&gt; `...14`\n‚Ä¢ `` -&gt; `...15`\n‚Ä¢ `` -&gt; `...16`\n‚Ä¢ `` -&gt; `...17`\n‚Ä¢ `` -&gt; `...18`\n‚Ä¢ `` -&gt; `...19`\n‚Ä¢ `` -&gt; `...20`\n‚Ä¢ `` -&gt; `...21`\n‚Ä¢ `` -&gt; `...22`\n‚Ä¢ `` -&gt; `...23`\n‚Ä¢ `` -&gt; `...24`\n‚Ä¢ `` -&gt; `...25`\n‚Ä¢ `` -&gt; `...26`\n‚Ä¢ `` -&gt; `...27`\n‚Ä¢ `` -&gt; `...28`\n‚Ä¢ `` -&gt; `...29`\n‚Ä¢ `` -&gt; `...30`\n‚Ä¢ `` -&gt; `...31`\n‚Ä¢ `` -&gt; `...32`\n‚Ä¢ `` -&gt; `...33`\n‚Ä¢ `` -&gt; `...34`\n‚Ä¢ `` -&gt; `...35`\n‚Ä¢ `` -&gt; `...36`\n‚Ä¢ `` -&gt; `...37`\n‚Ä¢ `` -&gt; `...38`\n‚Ä¢ `` -&gt; `...39`\n‚Ä¢ `` -&gt; `...40`\n‚Ä¢ `` -&gt; `...41`\n‚Ä¢ `` -&gt; `...42`\n‚Ä¢ `` -&gt; `...43`\n‚Ä¢ `` -&gt; `...44`\n‚Ä¢ `` -&gt; `...45`\n‚Ä¢ `` -&gt; `...46`\n‚Ä¢ `` -&gt; `...47`\n‚Ä¢ `` -&gt; `...48`\n‚Ä¢ `` -&gt; `...49`\n‚Ä¢ `` -&gt; `...50`\n‚Ä¢ `` -&gt; `...51`\n‚Ä¢ `` -&gt; `...52`\n‚Ä¢ `` -&gt; `...53`\n‚Ä¢ `` -&gt; `...54`\n‚Ä¢ `` -&gt; `...55`\n‚Ä¢ `` -&gt; `...56`\n‚Ä¢ `` -&gt; `...57`\n‚Ä¢ `` -&gt; `...58`\n‚Ä¢ `` -&gt; `...59`\n‚Ä¢ `` -&gt; `...60`\n‚Ä¢ `` -&gt; `...61`\n‚Ä¢ `` -&gt; `...62`\n‚Ä¢ `` -&gt; `...63`\n‚Ä¢ `` -&gt; `...64`\n‚Ä¢ `` -&gt; `...65`\n‚Ä¢ `` -&gt; `...66`\n‚Ä¢ `` -&gt; `...67`\n‚Ä¢ `` -&gt; `...68`\n‚Ä¢ `` -&gt; `...69`\n‚Ä¢ `` -&gt; `...70`\n‚Ä¢ `` -&gt; `...71`\n‚Ä¢ `` -&gt; `...72`\n‚Ä¢ `` -&gt; `...73`\n‚Ä¢ `` -&gt; `...74`\n‚Ä¢ `` -&gt; `...75`\n‚Ä¢ `` -&gt; `...76`\n‚Ä¢ `` -&gt; `...77`\n‚Ä¢ `` -&gt; `...78`\n‚Ä¢ `` -&gt; `...79`\n‚Ä¢ `` -&gt; `...80`\n‚Ä¢ `` -&gt; `...81`\n‚Ä¢ `` -&gt; `...82`\n‚Ä¢ `` -&gt; `...83`\n‚Ä¢ `` -&gt; `...84`\n‚Ä¢ `` -&gt; `...85`\n‚Ä¢ `` -&gt; `...86`\n‚Ä¢ `` -&gt; `...87`\n‚Ä¢ `` -&gt; `...88`\n‚Ä¢ `` -&gt; `...89`\n‚Ä¢ `` -&gt; `...90`\n‚Ä¢ `` -&gt; `...91`\n‚Ä¢ `` -&gt; `...92`\n‚Ä¢ `` -&gt; `...93`\n‚Ä¢ `` -&gt; `...94`\n‚Ä¢ `` -&gt; `...95`\n‚Ä¢ `` -&gt; `...96`\n‚Ä¢ `` -&gt; `...97`\n‚Ä¢ `` -&gt; `...98`\n‚Ä¢ `` -&gt; `...99`\n‚Ä¢ `` -&gt; `...100`\n‚Ä¢ `` -&gt; `...101`\n‚Ä¢ `` -&gt; `...102`\n‚Ä¢ `` -&gt; `...103`\n‚Ä¢ `` -&gt; `...104`\n‚Ä¢ `` -&gt; `...105`\n‚Ä¢ `` -&gt; `...106`\n‚Ä¢ `` -&gt; `...107`\n‚Ä¢ `` -&gt; `...108`\n‚Ä¢ `` -&gt; `...109`\n‚Ä¢ `` -&gt; `...110`\n‚Ä¢ `` -&gt; `...111`\n‚Ä¢ `` -&gt; `...112`\n‚Ä¢ `` -&gt; `...113`\n‚Ä¢ `` -&gt; `...114`\n‚Ä¢ `` -&gt; `...115`\n‚Ä¢ `` -&gt; `...116`\n‚Ä¢ `` -&gt; `...117`\n‚Ä¢ `` -&gt; `...118`\n‚Ä¢ `` -&gt; `...119`\n‚Ä¢ `` -&gt; `...120`\n‚Ä¢ `` -&gt; `...121`\n‚Ä¢ `` -&gt; `...122`\n‚Ä¢ `` -&gt; `...123`\n‚Ä¢ `` -&gt; `...124`\n‚Ä¢ `` -&gt; `...125`\n‚Ä¢ `` -&gt; `...126`\n‚Ä¢ `` -&gt; `...127`\n‚Ä¢ `` -&gt; `...128`\n‚Ä¢ `` -&gt; `...129`\n‚Ä¢ `` -&gt; `...130`\n‚Ä¢ `` -&gt; `...131`\n‚Ä¢ `` -&gt; `...132`\n‚Ä¢ `` -&gt; `...133`\n‚Ä¢ `` -&gt; `...134`\n‚Ä¢ `` -&gt; `...135`\n‚Ä¢ `` -&gt; `...136`\n‚Ä¢ `` -&gt; `...137`\n‚Ä¢ `` -&gt; `...138`\n‚Ä¢ `` -&gt; `...139`\n‚Ä¢ `` -&gt; `...140`\n‚Ä¢ `` -&gt; `...141`\n‚Ä¢ `` -&gt; `...142`\n‚Ä¢ `` -&gt; `...143`\n‚Ä¢ `` -&gt; `...144`\n‚Ä¢ `` -&gt; `...145`\n‚Ä¢ `` -&gt; `...146`\n‚Ä¢ `` -&gt; `...147`\n‚Ä¢ `` -&gt; `...148`\n‚Ä¢ `` -&gt; `...149`\n‚Ä¢ `` -&gt; `...150`\n‚Ä¢ `` -&gt; `...151`\n‚Ä¢ `` -&gt; `...152`\n‚Ä¢ `` -&gt; `...153`\n‚Ä¢ `` -&gt; `...154`\n‚Ä¢ `` -&gt; `...155`\n‚Ä¢ `` -&gt; `...156`\n\n    var_names &lt;- read_excel(\"experiment_2021C(1)copy.xlsx\", n_max = 1) \n\nNew names:\n‚Ä¢ `Q171_First Click` -&gt; `Q171_First Click...46`\n‚Ä¢ `Q171_Last Click` -&gt; `Q171_Last Click...47`\n‚Ä¢ `Q171_Page Submit` -&gt; `Q171_Page Submit...48`\n‚Ä¢ `Q171_Click Count` -&gt; `Q171_Click Count...49`\n‚Ä¢ `Q171_First Click` -&gt; `Q171_First Click...82`\n‚Ä¢ `Q171_Last Click` -&gt; `Q171_Last Click...83`\n‚Ä¢ `Q171_Page Submit` -&gt; `Q171_Page Submit...84`\n‚Ä¢ `Q171_Click Count` -&gt; `Q171_Click Count...85`\n\n    colnames(exp21) &lt;- colnames(var_names)\n\n    rm(var_names) \n\n    columnss &lt;- exp21 %&gt;% \n      select(starts_with(\"Outcome\"), \"Age\", \"Risk\", \"Gender\", \"SC0\") \n\nnormal numbers, the more * the more signif\n\ndatac &lt;- columnss %&gt;% mutate(score = SC0 *10)\n\ndatacool &lt;- datac %&gt;% select(-Outcome1, -Outcome2, -Outcome3, -Outcome4, -Outcome5, -Outcome6, -Outcome7, -Outcome8, -Outcome9, -Outcome10, -Outcome11, -Outcome12, -Outcome13, -Outcome14, -Outcome15, -Outcome16, -Outcome17, -Outcome18, -Outcome19, -Outcome20, -SC0)\n\ndataex1&lt;- datacool %&gt;% select(-Risk, - Age)\n\n\noptions(scipen = 100)\n\n1. Examine the relationship between gender and the number of wins using regression analysis.\nindependent variable x = the cause explanatory, dependent variable y = outcome\nNumber of wins is referred to as the Score in our data, and it is a continuous dependent variable(=y).\nGender is a categorical variable and is the independent variable(=x).\nOn the first chunk I create one new column name ‚ÄòGendering‚Äô where I assign values 0 and 1 to female and male correspondingly.\nOn the second and third chunks, I do the linear regression between Gender(x) and Score - aka number of wins - (y).\n\ndatacool1 &lt;- datacool %&gt;% mutate(Gendering = case_when(Gender == \"Male\" ~ \"1\",\n                                        Gender== \"Female\" ~ \"0\"))\n\ndataex1 %&gt;% summary(lm(score ~ factor(Gendering), data = datacool1))\n\n    Gender              score       \n Length:133         Min.   : 0.000  \n Class :character   1st Qu.: 3.000  \n Mode  :character   Median : 4.000  \n                    Mean   : 4.398  \n                    3rd Qu.: 5.000  \n                    Max.   :20.000  \n\nmodel&lt;- lm(formula= score~Gendering, data= datacool1)\n(summary(model))\n\n\nCall:\nlm(formula = score ~ Gendering, data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7528 -1.6818 -0.7528  0.3182 16.3182 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   4.7528     0.3088  15.389 &lt;0.0000000000000002 ***\nGendering1   -1.0710     0.5370  -1.995              0.0482 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.914 on 131 degrees of freedom\nMultiple R-squared:  0.02947,   Adjusted R-squared:  0.02206 \nF-statistic: 3.978 on 1 and 131 DF,  p-value: 0.04817\n\n\nLooking at the results from the intercept and the Gendering column (1 = given that is a male), we can see that the regression equation is the following:\ny=-1.07x+4.75\na) What is the effect (the slope) of gender?\nTo see the effect we look to the slope of x. It indicates that on average males win one time less than females.\nb) How strong is the predictive power of gender?\nLooking at the multiple R square we can see that the effect of gender explains 2,947% of the variability of the score.\nc) What are the predicted outcomes for men and women?\nFor this I substitute the values for male and female on the regression equation. For male x=1, for female x=0. After doing that, we will have a result that demonstrates the predicted score for females and males given that our initial assumption holds true.\n\n#for male\nmalepred &lt;- -1.07*1+4.75\n\n#for female\nfemalepred &lt;- -1.07*0+4.75\n\nThe predicted score for male on average is 3.68, while the equivalent predicted score for female is 4.75.\n2. Examine the relationship between age and the number of wins using regression analysis.\n\nmodel1&lt;- lm(formula = score~Age, data= datacool1)\nsummary(model1)\n\n\nCall:\nlm(formula = score ~ Age, data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4383 -1.4232 -0.4080  0.6524 15.4559 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  4.01507    1.36266   2.946  0.00381 **\nAge          0.01511    0.05276   0.287  0.77495   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.957 on 131 degrees of freedom\nMultiple R-squared:  0.0006262, Adjusted R-squared:  -0.007003 \nF-statistic: 0.08208 on 1 and 131 DF,  p-value: 0.7749\n\n\nAfter following the same procedure as we followed in exercise 1, we have the equivalent regression results for Age as independent variable x, and score as dependent variable y. The equation is as follows:\ny=0.015x+4.01\nmultiple R-squared, how much of the score variability is explained assuming that there is an effect of age.\na) What is the effect of age?\nThe effect (aka slope) of age is 0.015. This implies that there is an observed higher score of 0.015 with people that are older. Put differerently, the age factor has a negligible result to the score as is almost zero.\nb) How strong is the predictive power of age?\nLooking at the multiple R square we can see that the effect of age has almost zero explanation for the variability of the score. In other words, the predictive power of age on the time of victories is very weak.\nc) What's the predicted outcome for a 20 year old person? For a 40 year old person?\n\n#for 20 year old\ntwentypred &lt;- 0.015*20+4.01\n\n#for 40 year old\nfourtypred &lt;- 0.015*40+4.01\n\nFor this I substitute the values for 20year old and 40year old on the regression equation. For 20year x=20, for 40year old x=40. After doing that, we will have a result that demonstrates the predicted score for 20year old and 40year old given that our initial assumption holds true.\nThe predicted score for 20year old on average is 4.31, while the equivalent predicted score for 40year old is 4.61.\n3. Examine the relationship between age and gender combined.\nwhy the numbers are changing slightly (=men and women have different ages. when u plug both coefficients together it tries to)\na) What are the effects now? How do they compare with the results in 1 and 2? Explain the differences\n\nmodel1&lt;- lm(formula = score~Age+factor(Gendering), data= datacool1)\nsummary(model1)\n\n\nCall:\nlm(formula = score ~ Age + factor(Gendering), data = datacool1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.859 -1.673 -0.673  0.401 16.082 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)         4.08770    1.34714   3.034  0.00291 **\nAge                 0.02660    0.05244   0.507  0.61280   \nfactor(Gendering)1 -1.10062    0.54165  -2.032  0.04419 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.922 on 130 degrees of freedom\nMultiple R-squared:  0.03139,   Adjusted R-squared:  0.01649 \nF-statistic: 2.107 on 2 and 130 DF,  p-value: 0.1258\n\n\nNow the equation becomes:\ny=0.02x1-1.1x2+4.08\nwith x1 being the age, and x2 being the gender given is a male (if that one is zero then is a female)\nAs we can see here, while the age effect is still close to zero, now it is a bit higher (0.026 as opposed to 0.015); however it still remains a weak factor. With respect to Gender, now is also higher ton the other end than its corresponding previous result (-1.10 as opposed to -1.07).\nConsequently, there is a very slight inter correlation between the two factors as the model tends to become slightly stronger. How ever the differences are minimal. This is indicated also in the adjusted R squared (=0.016) which is even lower than R squared. This means that at least one of the two variables does not explain the dependent variable score. As we can see from the exercises 1 and 2, this variable is Age because its adjusted R squared is negative when we have Age as the only independent variable (see exercise 2).\nThen we proceed to an interaction coefficient just to observe the cross sections between our independent variables. Despite that the numbers are changing a bit though interaction coefficient is not necessary for the purpose of this exercise.\n\ncool3 &lt;- lm(score~ Age * factor(Gendering), data = datacool1)\nsummary(cool3)\n\n\nCall:\nlm(formula = score ~ Age * factor(Gendering), data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8912 -1.6606 -0.6606  0.6527 15.5740 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)             5.90581    1.99661   2.958  0.00369 **\nAge                    -0.04612    0.07890  -0.585  0.55989   \nfactor(Gendering)1     -4.41097    2.74144  -1.609  0.11006   \nAge:factor(Gendering)1  0.12987    0.10544   1.232  0.22030   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.916 on 129 degrees of freedom\nMultiple R-squared:  0.04265,   Adjusted R-squared:  0.02039 \nF-statistic: 1.916 on 3 and 129 DF,  p-value: 0.1303\n\n\nWe observe that the estimations between age and gender are not significant.\nb) How strong is the predictive power of this model?\nLooking at the adjusted R square we can see that the effect of age combined with the effect of gender explains 1,6% the variability of the score - when doing multiple regression, adjusted R squared is a better indication to look at for the predictive power of the models. And in conclusion, this one does not have a strong predictive power.\nc) What is the predicted outcome for a 20 year old man? 20 year old woman? 40 year old man? 40 year old woman?\nFollowing similar procedure as the exercise 1 and 2 c, we substitute the age and gender factors in the equation for the equivalent results we need in the four different instances:\n\n#for 20year old man\ntwentyman &lt;- 0.02*20-1.1*1+4.08\n\n#for 20year old woman\ntwentywoman &lt;- 0.02*20-1.1*0+4.08\n\n#for 40year old man\nfourtyman &lt;- 0.02*40-1.1*1+4.08\n\n#for 40year old woman\nfourtywoman &lt;- 0.02*40-1.1*0+4.08\n\nAs we can see, the predicted scores are the following:\nfor 20year old man = 3.38\nfor 20year old woman = 4.48\nfor 40year old man = 3.78\nfor 40year old woman = 4.88"
  },
  {
    "objectID": "software/sitcoms.html",
    "href": "software/sitcoms.html",
    "title": "Sitcoms",
    "section": "",
    "text": "Let's continue working with the Google Trends data you obtained for Homework 02. Homework 3 starts from the line 180 and onward.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úî ggplot2 3.3.5      ‚úî purrr   0.3.4 \n‚úî tibble  3.1.6      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.0 \n‚úî readr   2.0.1      ‚úî forcats 0.5.1 \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(broom)\nlibrary(ggthemes)\nlibrary(ggsci)\n\n\nCode to load the data into R and prepare it for the analysis. You need to correctly specify data types and choose concise variable names.\n\n#Import data for the US\nuscoms &lt;- read_csv(\"uscoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (United States), South Park: (United States), The Simps...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for the world\nworldcoms &lt;- read_csv(\"worldcoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Worldwide), South Park: (Worldwide), The Simpsons: (Wo...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for Cyprus\ncycoms &lt;- read_csv(\"cycoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Cyprus), South Park: (Cyprus), The Simpsons: (Cyprus)\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nprepare the data for analysis\n\n#NOW WE CLEAN AND PREPARE THE DATA FOR ANALYSIS\n#Clean for the US.\nuscoms_clean &lt;- uscoms %&gt;%\n  rename(month = Month,\n         famguy = `Family Guy: (United States)`,\n         southpark = `South Park: (United States)`,\n         simpsons = `The Simpsons: (United States)`) %&gt;% \n  mutate_if(is.character, str_replace, pattern = \"&lt;\", replacement = \"\") %&gt;% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %&gt;% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %&gt;%\n  mutate(day = 15, .after = month) %&gt;%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Worldwide\nworldcoms_clean &lt;- worldcoms %&gt;%\n  rename(month = Month,\n         famguy = `Family Guy: (Worldwide)`,\n         southpark = `South Park: (Worldwide)`,\n         simpsons = `The Simpsons: (Worldwide)`) %&gt;% \n  mutate_if(is.character, str_replace, pattern = \"&lt;\", replacement = \"\") %&gt;% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %&gt;% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %&gt;%\n  mutate(day = 15, .after = month) %&gt;%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Cyprus.\ncycoms_clean &lt;- cycoms %&gt;%\n  rename(month = Month,\n         famguy = `Family Guy: (Cyprus)`,\n         southpark = `South Park: (Cyprus)`,\n         simpsons = `The Simpsons: (Cyprus)`) %&gt;% \n  mutate_if(is.character, str_replace, pattern = \"&lt;\", replacement = \"\") %&gt;% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %&gt;% \n\n  #I separate the month and  year into two columns. Then I convert the character column to a number.\n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %&gt;%\n\n  #I create a new column that is called day, and I use 15 as it is the middle of the month.\n  mutate(day = 15, .after = month) %&gt;%\n\n  #Here I create a date column using the ymd (=year month day)function.\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\nCode that will calculate the average popularity of the terms by year for each of the search terms in each of the geographies.\n\n#HERE WE SUMMARIZE THE AVERAGE POPULARITY OF THE TERMS BY YEAR OF EACH SEARCH\n#Summarize for the US\nuscoms_summary &lt;- uscoms_clean %&gt;% \n  group_by(year) %&gt;% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Worldwide\nworldcoms_summary &lt;- worldcoms_clean %&gt;% \n  group_by(year) %&gt;% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Cyprus\ncycoms_summary &lt;- cycoms_clean %&gt;% \n  group_by(year) %&gt;% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n\nBefore we analyse the data, we tidy them. This way, we have three rows that correspond for each month from 2004 until 2021. This makes easier for us to specify what goes where.\n\nuscoms_tidy &lt;- uscoms_clean %&gt;%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworldcoms_tidy &lt;- worldcoms_clean %&gt;%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\ncycoms_tidy &lt;- cycoms_clean %&gt;%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\n4. Analyze the data to answer the following questions:\n\nIn what year was each term most popular? In which geography is each of the terms most popular in the year you found in Part A?\n\nuscoms_tidy %&gt;% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\nworldcoms_tidy %&gt;% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line()+ scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\ncycoms_tidy %&gt;% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\nBased on the graphs the most popular year for\n‚ÄìFamily guy was: 2nd half of 2008 (USA)/ 1st half of 2009(Worldwide)/ 1st, 2nd half of 2007 (Cyprus)\n‚ÄìSimpsons was: mid-2007 (USA,Worldwide)/ 2nd half of 2008 (Cyprus)\n‚ÄìSouth Park was: 1st half of 2010 (USA, Worldwide) / first half of 2004 (Cyprus)\nCalculate the ratio of the most popular term to the least popular term. Describe how this ratio changed over time in each of the geographies by relying on yearly data.\n\n\n#I hereby summarize in three different ways the mean search score of each series.\nworldcoms_clean %&gt;% summarize(mean(southpark), mean(simpsons), mean(famguy))\n\n# A tibble: 1 √ó 3\n  `mean(southpark)` `mean(simpsons)` `mean(famguy)`\n              &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1              22.2             36.2           20.3\n\nworldcoms_clean %&gt;% summarize_at(vars(southpark, simpsons, famguy), mean)\n\n# A tibble: 1 √ó 3\n  southpark simpsons famguy\n      &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1      22.2     36.2   20.3\n\nsummary(worldcoms_clean)\n\n      year          month             day         famguy        southpark    \n Min.   :2004   Min.   : 1.000   Min.   :15   Min.   : 5.00   Min.   : 7.00  \n 1st Qu.:2008   1st Qu.: 3.000   1st Qu.:15   1st Qu.:12.00   1st Qu.:11.00  \n Median :2013   Median : 6.000   Median :15   Median :19.00   Median :21.00  \n Mean   :2013   Mean   : 6.475   Mean   :15   Mean   :20.29   Mean   :22.22  \n 3rd Qu.:2017   3rd Qu.: 9.000   3rd Qu.:15   3rd Qu.:29.00   3rd Qu.:29.00  \n Max.   :2022   Max.   :12.000   Max.   :15   Max.   :45.00   Max.   :69.00  \n    simpsons           date           \n Min.   : 13.00   Min.   :2004-01-15  \n 1st Qu.: 23.00   1st Qu.:2008-07-15  \n Median : 37.00   Median :2013-01-15  \n Mean   : 36.18   Mean   :2013-01-13  \n 3rd Qu.: 47.00   3rd Qu.:2017-07-15  \n Max.   :100.00   Max.   :2022-01-15  \n\n#Looking at the results we conclude that the simpsons on average was the most popular by far with 36.18 score. Then southpark follows with 22.22 and then family guy follows with 20.29.\n\nThen we group the averages by year so we can compare the yearly differences.\n\nworld_summary &lt;- worldcoms_clean %&gt;% \n  group_by(year) %&gt;% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\nworld_tidy &lt;- world_summary %&gt;%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworld_tidy %&gt;% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() \n\n\n\n\n\nHOMEWORK 3 STARTS HERE\n\nPrepare the graph showing the trends in popularity of the search terms over time. Make sure to add a descriptive title, label the axes, and modify the look of the graph to be presentable. You can choose your own colors or use one of the palettes we talked about in class. Add a one-sentence explanation for the geometry you selected for the graph.\n\nworld_tidy %&gt;% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\") +\n  scale_color_discrete(labels = c(\"Family Guy\", \"The Simpsons\", \"South Park\")) #rename titles\n\n\n\n\nThe same code as before is used to make the graph. The only change here is that we rename the names of the shows in the graph by using the command ‚Äúscale_color_discrete(labels = c(‚Ä¶)‚Äù.\n\n\nworld_tidy %&gt;% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"Popularity graph\",\n       subtitle = \"Top 3 American sitcoms\"\n       ) +\n  scale_color_discrete(name = \"Shows:\",\n                       labels = c(\"South Park\", \"The Simpsons\", \"Family Guy\")) +\n  theme_wsj()  #this theme makes the graph more presentable\n\n\n\n\nThis is a more presentable version of the graph above.\n\nSmooth the data (using a graph) to eliminate random noise. Explain which smoothing method you chose and why.\n\n\nworld_tidy %&gt;% \n  ggplot(aes(x = year, y = score, color = name)) +\n  #geom_line() + #I remove this so we can see the variability of each sitcom.\n  geom_smooth(method = \"loess\") + #This command creates a moving average that smoothes out all the fluctuations.\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFor this code we use LOESS command to smooth the fluctuations. LOESS (aka locally weighted smoothing), helps us see the relationship between the three variables and foresee trends. A LOESS smoother takes the data, fits a regression with the subset of data, and uses that linear regression model to get a point for the smoothed curve. The points closer to the fitted line are more impact-full.\n\nCreate a chart to show seasonality by month in your data. What does the chart tell you about the seasonality of your chosen search terms? Some examples of seasonality charts can be found here: https://www.r-graph-gallery.com/142-basic-radar-chart.html (Links to an external site.)\n\n\nworldcoms_tidy %&gt;% \n  group_by(month, name) %&gt;% #I group by month to see which season are popular. By name to see the shows.\n  summarize(mean_score = mean(score)) %&gt;% \n  ggplot(aes(x = month, y = mean_score, fill = name)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  coord_polar()\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n\n\n\nAs we can see, the Simpsons seasonality graph looks the same. That is probably we have no limits to the popularity score of the variable and every score is close to each other. i.e.¬†popularity does not vary by a lot.\n\nworldcoms_tidy %&gt;% \n  filter(name == \"simpsons\") %&gt;% #I filter by name \"simpsons\" so I can examine the Simpsons show by itself.\n  group_by(month) %&gt;% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %&gt;% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(30, 40)) + #I include score limits from 30 to 40 in order to see the exact seasons where \"The simpsons\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col).\n\n\n\n\n\n\nworldcoms_tidy %&gt;% \n  filter(name == \"famguy\") %&gt;% #I filter by name \"famguy\" so I can examine the \"Family Guy\" show by itself.\n  group_by(month) %&gt;% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %&gt;% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(15, 25)) + #I include score limits from 15 to 25 in order to see the exact seasons where \"The Family Guy\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col)."
  },
  {
    "objectID": "software/presidents_analysis.html",
    "href": "software/presidents_analysis.html",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "",
    "text": "Refer to the state of the union addresses made by US presidents since WWII. To simplify the task, only looks at the first address for each president. (You can find the archive here: https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union (Links to an external site.))\n# We start as always with importing the necessary libraries\nimport requests #PACKAGE THAT allows us download texts from online (e.g. I request Moby Dick's online book without downloading it)\nimport re\nfrom requests import get\nfrom bs4 import BeautifulSoup #Submodule of bs4 (I don't need the entire package). \nimport nltk #natural language processing (NLP)\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom collections import Counter \nfrom requests import get\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nfrom gensim import corpora, similarities\nfrom gensim.models import TfidfModel\nfrom scipy.cluster import hierarchy\nfrom nltk.stem import PorterStemmer\nimport pickle\nimport warnings\nimport wordninja\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom autocorrect import Speller\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport contractions\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom scipy.cluster.vq import kmeans, vq\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.svm import LinearSVC\n\n#for the code to work\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('vader_lexicon')\n\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /Users/loizoskon/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/loizoskon/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/loizoskon/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\nTrue\nWe ask BeautifulSoup to locate a table on the page, scan through the rows of this table, and then get the first link on each of the rows.\nurl = \"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union\"\n\n#https://www.presidency.ucsb.edu/documents/app-categories/spoken-addresses-and-remarks/presidential/state-the-union-addresses\n#\n# Find the first instance of a table on the page (this will simplify work for us)\nresponse = get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\ntable = soup.find('table')\n\n# Create an empty list to keep all the links\nlinks = []\n\n# Look through each row of the table to identify if there are links,\n# if there are any, then get the first link, if there aren't any, just skip that row\nif table.findAll(\"tr\"):\n    trs = table.findAll('tr') \n    for tr in trs:\n        try:\n            link = tr.find('a')['href'] # Finds the first link in a row\n            links.append(link) # Appends that link to the links list.\n        except:\n            pass\n# We can also find all instances of the links on a page and that's what the code below does\n# You can use the code below but I want to solve a more general version of the homework, \n# so I will use this code here:\n############\ntrs= table.findAll('tr')\nfor tr in trs:\n    try:\n        aas = tr.find_all('a') # Finds the first link in a row\n        for a in aas:\n            link = a['href']\n            links.append(link) # Appends that link to the links list.\n    except:\n        pass\n# #\n# #links\n# #############\n# # Note that we had to use the try: and except:,\n# # because otherwise the code would result in an error as some rows don't have any links    \n# # Uncomment the code below to see this for yourself.\n\n# # Select the links that are needed\nlinks_needed = links[0:22]\n\n# for tr in trs:\n#     link = tr.find('a')['href'] # Finds the first link in a row\n#     links.append(link) # Appends that link to the links list.\n# Links now has all the links we needed. I can save them to an excel file \n# just in case I need them\n\npd.DataFrame(links).to_excel(\"links.xlsx\")\n# We can now go through individual links and extract the information we want.\n# If we examine the pages of each speech, we will notice each speech is stored inside\n# the following tag \"&lt;div class=\"field-docs-content\"&gt; &lt;/div&gt;\". \n# That's what we can grab.\n\n# We can also grab the date stored inside the following tag:\n#  &lt;div class=\"field-docs-start-date-time\"&gt;&lt;/div&gt;\n\n# Finally, to make our life even more easy, let's grab the name of the President\n# It's in the tag \"&lt;div class=\"field-title\"&gt;&lt;/div&gt;\"\n\n# I will also grab the title of the speech just in case I need to use it since the titles differed somewhat\n\n\n# We start with creating empty containers\n\nnames = []\ndates = []\nspeeches = []\ntitles = [] \n\n#for link in links:\n#     try:\n#         response = get(link)\n#    except: # I need this because Nixon's speech in 1973 in done in an annoying way, it's a footnote actually\n#        pass # so the code without it would result in an error\n#     soup = BeautifulSoup(response.text, 'html.parser')\n#     name =  soup.find(\"div\", class_ =\"field-title\").get_text(strip=True)\n#  #   date = soup.find(\"div\", class_ =\"field-docs-start-date-time\").get_text(strip=True)\n#  #   title = soup.find(\"div\", class_ =\"field-ds-doc-title\").get_text(strip=True)\n# #    #speech = soup.find(\"div\", class_ =\"field-docs-content\").get_text(strip=True)\n#     names.append(name)\n#   #  dates.append(date)\n#  #   titles.append(title)\n#    speeches.append(speech)\n    \n#    print(names)\ndf = pd.DataFrame({'name': names, 'date': dates, 'title': titles, \"speech\": speeches})\ndisplay(df)\n\n\n\n\n\n\n\n\nname\ndate\ntitle\nspeech\n#df.to_excel(\"speeches.xlsx\")\n#print(date)\n# #create a list of president names\nPresidentName = ['Joseph R. Biden', 'Donald J. Trump', 'Barack Obama', 'George W. Bush',\n                  'William J. Clinton', 'George Bush', 'Ronald Reagan', 'Jimmy Carter',\n                  'Gerald R. Ford','Richard Nixon', 'Lyndon B. Johnson','Dwight D. Eisenhower',\n                  'Harry S. Truman', 'Franklin D. Roosevelt']\n\n# #manually extract linksof first speech of each president\nlink_index = [1,2,7,14,22,30,34,42,49,52,59,68,79,91]\nlink_index = [1,2,7,14,22,30,34,42,49,52,59,65,68,79]\nnewlink=[links[i] for i in link_index]\n\nprint(PresidentName)\nprint(newlink)\n#COMMAND + SLASH\n\n['Joseph R. Biden', 'Donald J. Trump', 'Barack Obama', 'George W. Bush', 'William J. Clinton', 'George Bush', 'Ronald Reagan', 'Jimmy Carter', 'Gerald R. Ford', 'Richard Nixon', 'Lyndon B. Johnson', 'Dwight D. Eisenhower', 'Harry S. Truman', 'Franklin D. Roosevelt']\n['https://www.presidency.ucsb.edu/ws/index.php?pid=123408', 'https://www.presidency.ucsb.edu/ws/index.php?pid=102826', 'https://www.presidency.ucsb.edu/ws/index.php?pid=47232', 'https://www.presidency.ucsb.edu/ws/index.php?pid=2921', 'https://www.presidency.ucsb.edu/ws/index.php?pid=16603', 'https://www.presidency.ucsb.edu/ws/index.php?pid=29558', 'https://www.presidency.ucsb.edu/ws/index.php?pid=29542', 'https://www.presidency.ucsb.edu/documents/first-annual-message-11', 'https://www.presidency.ucsb.edu/ws/index.php?pid=29486', 'https://www.presidency.ucsb.edu/ws/index.php?pid=29475', 'https://www.presidency.ucsb.edu/ws/index.php?pid=29447', 'https://www.presidency.ucsb.edu/ws/index.php?pid=3996', 'https://www.presidency.ucsb.edu/ws/index.php?pid=4121', 'https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-25']\n# #create a dataframe\ndf = pd.DataFrame()\ndf['PresidentName'] = PresidentName\ndf['Links'] = newlink\ndisplay(df.Links.values)\nprint(df)\n\narray(['https://www.presidency.ucsb.edu/ws/index.php?pid=123408',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=102826',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=47232',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=2921',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=16603',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=29558',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=29542',\n       'https://www.presidency.ucsb.edu/documents/first-annual-message-11',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=29486',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=29475',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=29447',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=3996',\n       'https://www.presidency.ucsb.edu/ws/index.php?pid=4121',\n       'https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-25'],\n      dtype=object)\n\n\n            PresidentName                                              Links\n0         Joseph R. Biden  https://www.presidency.ucsb.edu/ws/index.php?p...\n1         Donald J. Trump  https://www.presidency.ucsb.edu/ws/index.php?p...\n2            Barack Obama  https://www.presidency.ucsb.edu/ws/index.php?p...\n3          George W. Bush  https://www.presidency.ucsb.edu/ws/index.php?p...\n4      William J. Clinton  https://www.presidency.ucsb.edu/ws/index.php?p...\n5             George Bush  https://www.presidency.ucsb.edu/ws/index.php?p...\n6           Ronald Reagan  https://www.presidency.ucsb.edu/ws/index.php?p...\n7            Jimmy Carter  https://www.presidency.ucsb.edu/documents/firs...\n8          Gerald R. Ford  https://www.presidency.ucsb.edu/ws/index.php?p...\n9           Richard Nixon  https://www.presidency.ucsb.edu/ws/index.php?p...\n10      Lyndon B. Johnson  https://www.presidency.ucsb.edu/ws/index.php?p...\n11   Dwight D. Eisenhower  https://www.presidency.ucsb.edu/ws/index.php?p...\n12        Harry S. Truman  https://www.presidency.ucsb.edu/ws/index.php?p...\n13  Franklin D. Roosevelt  https://www.presidency.ucsb.edu/documents/addr...\noutcome = []\nfor lnk in df.Links.values:\n     r = requests.get(lnk)\n     r.encoding = 'utf-8'\n     html = r.text\n     soup = BeautifulSoup(html,'html.parser')\n     text = soup.get_text()\n     outcome.append(text)\n    \n# print(pd.DataFrame(outcome))#all the speeches in tabular format\noutcome1 = [x.replace(\"\\n\", \"\") for x in outcome]#I replace 'n' with nothing\ndf2 = pd.DataFrame(outcome1)#I create a new dataframe\n\ndf2[[0]]\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\nAddress Before a Joint Session of the Congress...\n\n\n1\nAddress Before a Joint Session of Congress on ...\n\n\n2\nAddress Before a Joint Session of Congress on ...\n\n\n3\nAnnual Message to the Congress on the State of...\n\n\n4\nRadio Address Summarizing the State of the Uni...\n\n\n5\nFifth Annual Message | The American Presidency...\n\n\n6\nFirst Annual Message | The American Presidency...\n\n\n7\nFirst Annual Message | The American Presidency...\n\n\n8\nFirst Annual Message | The American Presidency...\n\n\n9\nFifth Annual Message | The American Presidency...\n\n\n10\nFifth Annual Message | The American Presidency...\n\n\n11\nState of the Union Message to the Congress: Ov...\n\n\n12\nState of the Union Message to the Congress on ...\n\n\n13\nAddress Before a Joint Session of the Congress...\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\nspeeches = pd.read_excel('state_of_the_union_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\nspeeches['top ten'] = top_list\n\ndisplay(speeches)\nprint(speeches)\n\n\n\n\n\n\n\n\npresident\nyear\nparty\nhtml\ntop ten\n\n\n\n\n0\nHoover\n1929\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('public', 33),('law', 25),('service', 23),('...\n\n\n1\nFD_Roosvelt\n1934\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('industrial', 9),('work', 8),('recovery', 7)...\n\n\n2\nTruman\n1949\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('prosperity', 12),('production', 12),('power...\n\n\n3\nEisenhower\n1957\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('free', 16),('security', 16),('economy', 12)...\n\n\n4\nKennedy\n1961\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('economic', 16),('development', 10),('peace'...\n\n\n5\nLyndon\n1965\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('freedom', 12),('life', 9),('progress', 8),(...\n\n\n6\nNixon\n1974\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('peace', 27),('energy', 17),('war', 8),('pro...\n\n\n7\nFord\n1975\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('energy', 25),('oil', 20),('tax', 17),('econ...\n\n\n8\nCarter\n1978\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/the-...\n[('inflation', 17),('economic', 14),('tax', 13...\n\n\n9\nReagan\n1985\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('freedom', 20),('tax', 16),('growth', 14),('...\n\n\n10\nBush\n1989\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('budget', 17),('work', 12),('hope', 10),('dr...\n\n\n11\nClinton\n1997\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('children', 24),('work', 21),('budget', 17),...\n\n\n12\nBush\n2005\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('security', 29),('freedom', 20),('social', 1...\n\n\n13\nObama\n2013\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('jobs', 32),('work', 20),('energy', 18),('fa...\n\n\n14\nTrump\n2018\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('tax', 15),('last', 13),('together', 13),('w...\n\n\n15\nJoe\n2022\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('folks', 19),('see', 15),('families', 15),('...\n\n\n\n\n\n\n\n      president  year       party  \\\n0        Hoover  1929  republican   \n1   FD_Roosvelt  1934    democrat   \n2        Truman  1949    democrat   \n3    Eisenhower  1957  republican   \n4       Kennedy  1961    democrat   \n5        Lyndon  1965    democrat   \n6         Nixon  1974  republican   \n7          Ford  1975  republican   \n8        Carter  1978    democrat   \n9        Reagan  1985  republican   \n10         Bush  1989  republican   \n11      Clinton  1997    democrat   \n12         Bush  2005  republican   \n13        Obama  2013    democrat   \n14        Trump  2018  republican   \n15          Joe  2022    democrat   \n\n                                                 html  \\\n0   https://www.presidency.ucsb.edu/documents/annu...   \n1   https://www.presidency.ucsb.edu/documents/annu...   \n2   https://www.presidency.ucsb.edu/documents/annu...   \n3   https://www.presidency.ucsb.edu/documents/annu...   \n4   https://www.presidency.ucsb.edu/documents/annu...   \n5   https://www.presidency.ucsb.edu/documents/annu...   \n6   https://www.presidency.ucsb.edu/documents/addr...   \n7   https://www.presidency.ucsb.edu/documents/addr...   \n8   https://www.presidency.ucsb.edu/documents/the-...   \n9   https://www.presidency.ucsb.edu/documents/addr...   \n10  https://www.presidency.ucsb.edu/documents/addr...   \n11  https://www.presidency.ucsb.edu/documents/addr...   \n12  https://www.presidency.ucsb.edu/documents/addr...   \n13  https://www.presidency.ucsb.edu/documents/addr...   \n14  https://www.presidency.ucsb.edu/documents/addr...   \n15  https://www.presidency.ucsb.edu/documents/addr...   \n\n                                              top ten  \n0   [('public', 33),('law', 25),('service', 23),('...  \n1   [('industrial', 9),('work', 8),('recovery', 7)...  \n2   [('prosperity', 12),('production', 12),('power...  \n3   [('free', 16),('security', 16),('economy', 12)...  \n4   [('economic', 16),('development', 10),('peace'...  \n5   [('freedom', 12),('life', 9),('progress', 8),(...  \n6   [('peace', 27),('energy', 17),('war', 8),('pro...  \n7   [('energy', 25),('oil', 20),('tax', 17),('econ...  \n8   [('inflation', 17),('economic', 14),('tax', 13...  \n9   [('freedom', 20),('tax', 16),('growth', 14),('...  \n10  [('budget', 17),('work', 12),('hope', 10),('dr...  \n11  [('children', 24),('work', 21),('budget', 17),...  \n12  [('security', 29),('freedom', 20),('social', 1...  \n13  [('jobs', 32),('work', 20),('energy', 18),('fa...  \n14  [('tax', 15),('last', 13),('together', 13),('w...  \n15  [('folks', 19),('see', 15),('families', 15),('...\nAmerican priorities shifted over time. As we can see, From Hoover (1929) until Nixon (1974) issues and words related to ‚Äúfreedom‚Äù, and ‚Äúpeace‚Äù were emphasized. This makes sense since during that time, WW2 and the Vietnam War were fought.\n‚ÄúEconomy‚Äù seems like a topic that is popular in almost every presidency.\nDuring Ford‚Äôs and Reagan‚Äôs presidency, ‚Äútax‚Äù, and ‚Äúgrowth‚Äù became really hot buzz words. Especially during Reagan administration, big tax reforms were introduced which they have significantly reduce taxes for businesses.\n‚Äúinflation‚Äù and ‚Äúenergy‚Äù were also popular during Nixon, Ford, and Reagan. It is important because especially during Nixon, the US economy after 14 years of economic development got in a stagflationary state; oil and gas crisis at the 70s was also a part of that.\nDuring George W Bush (2005), ‚Äúsecurity‚Äù was the most popular word used. This is because especially after 911, security became the main focus of his presidency.\nHealthcare gained importance during the Clinton Administration, and two administrations later, the Obama Administration expanded Medicaid. With the Covid-19 pandemic, healthcare again dominated the policy priorities in Biden‚Äôs 2022 address.\nAll the presidents, irrespective of their political affiliation (Democrats vs Republicans), mentioned about strengthening / growing the economy. Only presidents affiliated with the Democratic party seemed to emphasize on ‚Äúhealthcare‚Äù, whereas a common theme among the Republican presidents‚Äô addresses was ‚Äúwar / military spending / terrorism‚Äù.\nTop ten words of all Democrat Presidents\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\ndemocrat_speeches = pd.read_excel('democrat_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in democrat_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\ndemocrat_speeches['top ten'] = top_list\n\ndisplay(democrat_speeches)\n\n\n\n\n\n\n\n\npresident\nyear\nparty\nhtml\ntop ten\n\n\n\n\n0\nFD_Roosvelt\n1934\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('industrial', 9),('work', 8),('recovery', 7)...\n\n\n1\nTruman\n1949\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('prosperity', 12),('production', 12),('power...\n\n\n2\nKennedy\n1961\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('economic', 16),('development', 10),('peace'...\n\n\n3\nLyndon\n1965\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('freedom', 12),('life', 9),('progress', 8),(...\n\n\n4\nCarter\n1978\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/the-...\n[('inflation', 17),('economic', 14),('tax', 13...\n\n\n5\nClinton\n1997\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('children', 24),('work', 21),('budget', 17),...\n\n\n6\nObama\n2013\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('jobs', 32),('work', 20),('energy', 18),('fa...\n\n\n7\nJoe\n2022\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('folks', 19),('see', 15),('families', 15),('...\nTop ten words of all Republican Presidents\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\nrepublican_speeches = pd.read_excel('republican_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in republican_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\nrepublican_speeches['top ten'] = top_list\n\ndisplay(republican_speeches)\n\n\n\n\n\n\n\n\npresident\nyear\nparty\nhtml\ntop ten\n\n\n\n\n0\nHoover\n1929\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('public', 33),('law', 25),('service', 23),('...\n\n\n1\nEisenhower\n1957\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('free', 16),('security', 16),('economy', 12)...\n\n\n2\nNixon\n1974\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('peace', 27),('energy', 17),('war', 8),('pro...\n\n\n3\nFord\n1975\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('energy', 25),('oil', 20),('tax', 17),('econ...\n\n\n4\nReagan\n1985\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('freedom', 20),('tax', 16),('growth', 14),('...\n\n\n5\nBush\n1989\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('budget', 17),('work', 12),('hope', 10),('dr...\n\n\n6\nBush\n2005\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('security', 29),('freedom', 20),('social', 1...\n\n\n7\nTrump\n2018\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('tax', 15),('last', 13),('together', 13),('w...\nLinear Discriminant Analysis (LDA) is like PCA, but it focuses on maximizing the seperatibility among known categories\ntype(df2.iloc[:,0])\n\npandas.core.series.Series\ndataset = pd.read_excel('speeches53463.xlsx')\n\ngrouped = dataset.groupby('name')\ndataset['date'] = pd.to_datetime(dataset['date'])\n\n#print(dataset.head())\ngrouped = dataset.groupby('name')\n\ndataset2 = dataset.loc[dataset.groupby('name').date.idxmin()]\n#.filter(lambda x: x['date'] == min(x['date']))\nimport re\nimport numpy as np\n    \n# Print the titles of the first rows \nprint(df2[[0]].head())\n\n# Remove punctuation\ndataset['title_processed'] = dataset['speech'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\ndataset['title_processed'] = dataset['title_processed'].map(lambda x: x.lower())\n\n# Print the processed titles of the first rows \ndataset['title_processed'].head()\n\n                                                   0\n0  Address Before a Joint Session of the Congress...\n1  Address Before a Joint Session of Congress on ...\n2  Address Before a Joint Session of Congress on ...\n3  Annual Message to the Congress on the State of...\n4  Radio Address Summarizing the State of the Uni...\n\n\n0    the presidentthank you thank you thank you goo...\n1    the presidentthank you all very very much than...\n2    thank you very much mr speaker mr vice preside...\n3    the presidentmr speaker mr vice president memb...\n4    the presidentmadam speaker mr vice president m...\nName: title_processed, dtype: object\n# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(dataset['title_processed'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation",
    "href": "software/presidents_analysis.html#interpretation",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "Interpretation",
    "text": "Interpretation\nWhile the most common words do not seem to reveal something particular, when those generic words are cleaned as we saw above, we get specific topics and buzz words from each president.\n\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 5\nnumber_words = 25\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)\n\nTopics found via LDA:\n\nTopic #0:\nworld war nations peace people united nation free economic great shall congress year military security men national forces new freedom defense time power american government\n\nTopic #1:\nstates government united congress country public great citizens people year time state war treaty foreign shall present subject american general power peace mexico law relations\n\nTopic #2:\ngovernment congress law public federal business national great people country present labor power legislation action conditions shall men year world necessary work nation make time\n\nTopic #3:\nnew america people year years congress american world government make americans help work time federal nation tax economy jobs let programs budget need health program\n\nTopic #4:\nstates united government great congress treaty act year commerce spain public session present state nations france citizens war duties vessels relations minister effect subject commercial"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-1",
    "href": "software/presidents_analysis.html#interpretation-1",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "Interpretation",
    "text": "Interpretation\nTopic 0: aligns with internal state laws and political stability.\nTopic 1: seems to align mostly with internal US issues. It is targeted to citizens stability (jobs) and needs.\nTopic 2: Seems to be more related with tax policy and approach to the US economy.\nTopic 3: It has to do with public policy and foreign affairs.\nTopic 4: Mainly targeted to defence department and peace status maintenance while protecting the interests of the US.\n\nCan you determine the sentiment of each state of the union using nltk‚Äôs Vader module?\n\n\nanalyzer=SentimentIntensityAnalyzer()\ndef polarity_score(text):\n    if len(text)&gt;0:\n        score=analyzer.polarity_scores(text)['compound']\n        return score\n    else:\n        return 0\ndataset['polarityscore'] = dataset['speech'].apply(lambda text : polarity_score(text))\ndataset['polarityscore']\n\n0      0.9999\n1      0.9999\n2      1.0000\n3      0.9999\n4      0.9999\n        ...  \n254    0.9998\n255    0.9994\n256    0.9995\n257    0.9991\n258   -0.9997\nName: polarityscore, Length: 259, dtype: float64\n\n\n\ndef sentianamolybarplot(df):\n    polarity_scale=[0.9991,0.9992,0.9993,0.9994,0.9995,0.9996,0.9997,0.9998,0.9999,1]\n    #'Review_polarity' is column name of sentiment score calculated for whole review.\n    df3=df[(df['polarityscore']&gt;0)]\n    out = pd.cut(df3['polarityscore'],polarity_scale)\n    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n    plt.show()\nsentianamolybarplot(dataset)\n\n\n\n\nInterpretation\nIn the State of Union Speeches, the presidents talk about important issues facing Americans and offers their ideas on solving the nation‚Äôs problems, including suggestions for new laws and policies. As displayed in the plot, the polarity scores for 13 speeches (barring W. Bush) is positive. This is understandable as the State of Union speeches are a PR vehicle, leveraged to display the President‚Äôs power and positive influence.\n\nDo speeches of different presidents cluster in any way that can allow you to determine their political party? How different are Biden and Trump according to this clustering?\n\n\ndef remove_noise(text, stop_words = nltk.corpus.stopwords.words('english')):\n    newsw = ['annual', 'number', 'help', 'thank', 'get', 'going', 'think', 'look', 'said', 'create',\n             'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', 'long', 'better', \n             'plan', 'national', 'ask' '10', 'much', 'good', 'great', 'best', 'cannot', 'still', \n             'know', 'years', '1', 'major', 'want', 'able', 'put', 'capacity', 'programs', 'per', \n             'percent', 'million', 'act', 'provide', 'afford', 'needed', 'may', 'possible', 'full',\n             '2', 'effort', 'meeting', 'address', 'ever', 'measures', 'ago', 'delivered', '5', \n             'program', 'past', 'future', 'need', 'needs', 'house', 'also', 'tonight', 'propose', \n             'toward', 'continue', 'society','country', 'seek', 'period', 'year', 'man', 'men', \n             'one', 'areas', 'begin', 'live', 'make', 'let', 'upon', 'well', 'office', 'meet', \n             'make' 'citizens', 'human', 'self', 'among', 'peoples', 'affairs', 'would', 'field', \n             'first', 'interest', 'today', 'recommendations', 'recomenndation', 'within', 'shall', \n             'administration', 'nation', 'nations', 'us', 'we', 'policy', 'legislation', 'time', \n             'new', 'many', 'several', 'few', 'government', 'world', 'people', 'united', 'states', \n             'system', 'every', 'people', 'must', '626','give', 'categories', '226762', '17608', \n             '24532', '430', '38','statistics', 'analyses', 'miscellaneous', 'congressional', \n             'skip', 'content', 'documents', 'attributes', 'media', 'message', 'congress', 'state',\n             'union', 'america', 'american', 'americans', 'presidency', 'president', 'project', \n             'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', 'main', \n             'take','like','yet','j','000']\n    stop_words = stop_words + newsw\n    tokens = word_tokenize(text)\n    cleaned_tokens = []\n    for token in tokens:\n        token = re.sub('[^A-Za-z0-9]+', '', token)\n        if len(token) &gt; 1 and token.lower() not in stop_words:\n            # Get lowercase\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\n# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.8,\n                                   max_features = 50,\n                                   min_df = 0.1,\n                                   tokenizer = remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(dataset['speech'].values)\n\n\nnum_clusters = 2\n\n# Generate cluster centers through the kmeans function\ncluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n# display(cluster_centers)\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print('Cluster: {}'.format(i+1))\n    # Sort the terms and print top 3 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms [:15])\n\nCluster: 1\n['federal', 'economic', 'budget', 'work', 'tax', 'economy', 'security', 'jobs', 'nt', 'freedom', 'health', 'free', 'life', 'together', 'defense']\nCluster: 2\n['treaty', 'subject', 'commerce', 'treasury', 'general', 'relations', 'powers', 'laws', 'duty', 'session', 'interests', 'rights', 'however', 'service', 'trade']"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-2",
    "href": "software/presidents_analysis.html#interpretation-2",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "Interpretation",
    "text": "Interpretation\nOn clustering the popular words in the speech, it seems like Cluster 1 aligns with speeches by Republican presidents and Cluster 2 with that of speeches by Democartic presidents.\n\n#Converting the datafram into list for further analysis.\nspeech_list = []\nfor i in range(len(dataset2)):\n    speech_list.append(dataset2.iloc[[i]]['speech'].item())\n    \ntitles = []\nfor i in range(len(dataset2)):\n    titles.append(dataset2.iloc[[i]]['name'].item())\n    \ntexts = [txt.split() for txt in speech_list]\n\n# Create an instance of a PorterStemmer object\nporter = PorterStemmer()\n\n# For each token of each text, we generated its stem \ntexts_stem = [[porter.stem(token) for token in text] for text in texts]\n\n# Create a dictionary from the stemmed tokens\ndictionary = corpora.Dictionary(texts_stem)\n\n# Create a bag-of-words model for each speech, using the previously generated dictionary\nbows = [dictionary.doc2bow(text) for text in texts_stem]\n\n# Generate the tf-idf model\nmodel = TfidfModel(bows)\n\n# Compute the similarity matrix (pairwise distance between all speeches)\nsims = similarities.MatrixSimilarity(model[bows])\n\n# Transform the resulting list into a DataFrame\nsim_df = pd.DataFrame(list(sims))\n\n# Add the name of the presidents as columns and index of the DataFrame\nsim_df.columns = titles\nsim_df.index = titles\n\n# Print the resulting matrix\nsim_df\n\n# creat function that adds two numbers\n\n\n\n\n\n\n\n\nAbraham Lincoln\nAndrew Jackson\nAndrew Johnson\nBarack Obama\nBenjamin Harrison\nCalvin Coolidge\nChester A. Arthur\nDonald J. Trump\nDwight D. Eisenhower\nFranklin D. Roosevelt\n...\nRutherford B. Hayes\nTheodore Roosevelt\nThomas Jefferson\nUlysses S. Grant\nWarren G. Harding\nWilliam Howard Taft\nWilliam J. Clinton\nWilliam McKinley\nWoodrow Wilson\nZachary Taylor\n\n\n\n\nAbraham Lincoln\n1.000000\n0.121246\n0.151205\n0.047722\n0.110114\n0.118578\n0.114825\n0.047754\n0.075706\n0.047841\n...\n0.092012\n0.056717\n0.110742\n0.124756\n0.082261\n0.092794\n0.046491\n0.113427\n0.077418\n0.145153\n\n\nAndrew Jackson\n0.121246\n1.000000\n0.121070\n0.053532\n0.131924\n0.086656\n0.106385\n0.054705\n0.077823\n0.047385\n...\n0.112656\n0.070145\n0.117074\n0.146622\n0.092593\n0.113945\n0.050358\n0.102445\n0.086139\n0.155541\n\n\nAndrew Johnson\n0.151205\n0.121070\n1.000000\n0.047996\n0.097058\n0.094947\n0.079968\n0.049593\n0.073742\n0.053389\n...\n0.082619\n0.077785\n0.093668\n0.127001\n0.090346\n0.077516\n0.049155\n0.080710\n0.082443\n0.092401\n\n\nBarack Obama\n0.047722\n0.053532\n0.047996\n1.000000\n0.046357\n0.069860\n0.037358\n0.213163\n0.103428\n0.096561\n...\n0.035344\n0.069914\n0.054143\n0.047495\n0.073191\n0.049368\n0.317297\n0.050867\n0.064416\n0.039508\n\n\nBenjamin Harrison\n0.110114\n0.131924\n0.097058\n0.046357\n1.000000\n0.095429\n0.198889\n0.043727\n0.082837\n0.068579\n...\n0.269502\n0.057292\n0.080109\n0.146124\n0.080189\n0.125491\n0.049069\n0.142834\n0.076850\n0.134368\n\n\nCalvin Coolidge\n0.118578\n0.086656\n0.094947\n0.069860\n0.095429\n1.000000\n0.085849\n0.072170\n0.118543\n0.078246\n...\n0.081089\n0.087503\n0.077970\n0.098388\n0.128878\n0.092294\n0.076209\n0.074285\n0.086982\n0.092193\n\n\nChester A. Arthur\n0.114825\n0.106385\n0.079968\n0.037358\n0.198889\n0.085849\n1.000000\n0.042525\n0.058818\n0.042672\n...\n0.144648\n0.045146\n0.063310\n0.179516\n0.066345\n0.137828\n0.045936\n0.123470\n0.059776\n0.132799\n\n\nDonald J. Trump\n0.047754\n0.054705\n0.049593\n0.213163\n0.043727\n0.072170\n0.042525\n1.000000\n0.077175\n0.062748\n...\n0.040511\n0.077699\n0.046509\n0.056097\n0.065570\n0.040417\n0.183823\n0.042885\n0.056082\n0.040505\n\n\nDwight D. Eisenhower\n0.075706\n0.077823\n0.073742\n0.103428\n0.082837\n0.118543\n0.058818\n0.077175\n1.000000\n0.108428\n...\n0.059357\n0.074973\n0.060950\n0.085503\n0.107023\n0.072974\n0.128444\n0.065945\n0.104088\n0.069061\n\n\nFranklin D. Roosevelt\n0.047841\n0.047385\n0.053389\n0.096561\n0.068579\n0.078246\n0.042672\n0.062748\n0.108428\n1.000000\n...\n0.064508\n0.072999\n0.052945\n0.059127\n0.094722\n0.054787\n0.086663\n0.063699\n0.077462\n0.050414\n\n\nFranklin Pierce\n0.138249\n0.149229\n0.113608\n0.039742\n0.135522\n0.096017\n0.129588\n0.034980\n0.083958\n0.055784\n...\n0.096431\n0.059912\n0.091416\n0.149410\n0.087211\n0.114362\n0.038283\n0.098263\n0.081036\n0.181408\n\n\nGeorge Bush\n0.039750\n0.110718\n0.045218\n0.223350\n0.039387\n0.067593\n0.030361\n0.166730\n0.101130\n0.063581\n...\n0.029770\n0.064276\n0.043393\n0.048644\n0.064347\n0.045680\n0.236761\n0.042725\n0.064541\n0.038623\n\n\nGeorge W. Bush\n0.045682\n0.045953\n0.042901\n0.244457\n0.043902\n0.076660\n0.044497\n0.170425\n0.109930\n0.069602\n...\n0.038201\n0.047932\n0.043533\n0.048213\n0.069961\n0.033635\n0.258270\n0.042687\n0.054859\n0.033777\n\n\nGeorge Washington\n0.075427\n0.080150\n0.054498\n0.022271\n0.046189\n0.048199\n0.043466\n0.021537\n0.033813\n0.023598\n...\n0.056167\n0.028022\n0.075892\n0.047378\n0.038203\n0.039933\n0.022466\n0.030561\n0.040550\n0.058378\n\n\nGerald R. Ford\n0.040651\n0.041807\n0.043266\n0.133494\n0.045676\n0.079921\n0.053701\n0.082173\n0.135036\n0.062518\n...\n0.035063\n0.051462\n0.039757\n0.051069\n0.071961\n0.044571\n0.183497\n0.041057\n0.048935\n0.039580\n\n\nGrover Cleveland\n0.117943\n0.128614\n0.102102\n0.036857\n0.129553\n0.088140\n0.141185\n0.043138\n0.070560\n0.049183\n...\n0.098994\n0.069918\n0.072758\n0.129015\n0.083239\n0.154388\n0.035827\n0.105645\n0.084410\n0.169106\n\n\nHarry S. Truman\n0.060982\n0.071213\n0.065671\n0.087225\n0.070839\n0.099180\n0.058541\n0.080372\n0.165834\n0.092160\n...\n0.055451\n0.064122\n0.054537\n0.079809\n0.106270\n0.081560\n0.105554\n0.059129\n0.081950\n0.079267\n\n\nHerbert Hoover\n0.103231\n0.079289\n0.081658\n0.070459\n0.111400\n0.127897\n0.128176\n0.049934\n0.146132\n0.097870\n...\n0.078349\n0.062296\n0.072092\n0.110752\n0.135878\n0.098500\n0.090548\n0.076114\n0.096796\n0.091948\n\n\nJames Buchanan\n0.074219\n0.117172\n0.085650\n0.053138\n0.213879\n0.067012\n0.108787\n0.033511\n0.050480\n0.062365\n...\n0.181783\n0.050247\n0.071056\n0.138804\n0.063212\n0.083642\n0.036889\n0.143667\n0.058985\n0.117347\n\n\nJames K. Polk\n0.108668\n0.127383\n0.080034\n0.034267\n0.083551\n0.063491\n0.086467\n0.037409\n0.050591\n0.038777\n...\n0.081692\n0.041648\n0.059918\n0.124555\n0.054302\n0.098106\n0.031113\n0.069200\n0.073058\n0.204857\n\n\nJames Madison\n0.071391\n0.094473\n0.062079\n0.024049\n0.066594\n0.058868\n0.067699\n0.027369\n0.047359\n0.028658\n...\n0.071821\n0.037817\n0.098455\n0.088595\n0.049361\n0.056839\n0.027100\n0.063285\n0.035300\n0.113097\n\n\nJames Monroe\n0.121900\n0.130480\n0.097276\n0.034281\n0.096656\n0.073366\n0.089977\n0.034260\n0.062603\n0.045482\n...\n0.076209\n0.049221\n0.112049\n0.125313\n0.073308\n0.077521\n0.033153\n0.085300\n0.060619\n0.129747\n\n\nJimmy Carter\n0.038933\n0.046552\n0.046946\n0.240644\n0.048041\n0.088707\n0.047153\n0.162571\n0.159608\n0.088317\n...\n0.040429\n0.069212\n0.045362\n0.048941\n0.091093\n0.049687\n0.229270\n0.049615\n0.075476\n0.042085\n\n\nJohn Adams\n0.085961\n0.099759\n0.060527\n0.025965\n0.059714\n0.045513\n0.077210\n0.026649\n0.044619\n0.024816\n...\n0.053008\n0.042880\n0.083986\n0.104180\n0.036770\n0.063388\n0.029237\n0.067327\n0.038350\n0.110925\n\n\nJohn F. Kennedy\n0.054274\n0.064872\n0.067020\n0.145161\n0.064716\n0.087427\n0.052211\n0.091397\n0.166903\n0.082637\n...\n0.060584\n0.066535\n0.050081\n0.067712\n0.091555\n0.065329\n0.145564\n0.086820\n0.069619\n0.051925\n\n\nJohn Quincy Adams\n0.134975\n0.163353\n0.091024\n0.045042\n0.101502\n0.086493\n0.104866\n0.043737\n0.062438\n0.040328\n...\n0.084253\n0.063036\n0.126550\n0.123556\n0.071098\n0.098951\n0.040523\n0.082057\n0.072336\n0.149257\n\n\nJohn Tyler\n0.131702\n0.152333\n0.117307\n0.045652\n0.123190\n0.103976\n0.117884\n0.047375\n0.062916\n0.067584\n...\n0.141787\n0.081356\n0.119774\n0.153659\n0.101667\n0.120820\n0.052103\n0.113313\n0.072481\n0.170983\n\n\nJoseph R. Biden\n0.030122\n0.035307\n0.032910\n0.345184\n0.028887\n0.054890\n0.031472\n0.220390\n0.068947\n0.053099\n...\n0.021369\n0.050805\n0.034194\n0.039431\n0.052046\n0.028445\n0.306361\n0.027510\n0.054481\n0.033634\n\n\nLyndon B. Johnson\n0.045479\n0.043621\n0.046318\n0.128715\n0.035904\n0.065005\n0.038097\n0.106125\n0.100885\n0.056544\n...\n0.033196\n0.058649\n0.040729\n0.053333\n0.065493\n0.042870\n0.168042\n0.054452\n0.047279\n0.036023\n\n\nMartin van Buren\n0.123016\n0.189609\n0.095427\n0.050744\n0.148065\n0.078894\n0.110916\n0.030465\n0.075507\n0.051770\n...\n0.114139\n0.069005\n0.121863\n0.127558\n0.084042\n0.114678\n0.043701\n0.112850\n0.086940\n0.182542\n\n\nMillard Fillmore\n0.128602\n0.168584\n0.111184\n0.040514\n0.116195\n0.083215\n0.112446\n0.046339\n0.078221\n0.067761\n...\n0.102227\n0.080306\n0.119655\n0.143147\n0.097297\n0.102903\n0.044584\n0.100449\n0.100466\n0.200081\n\n\nRichard Nixon\n0.039255\n0.049510\n0.062639\n0.139949\n0.042007\n0.068414\n0.038798\n0.105487\n0.136241\n0.066019\n...\n0.036937\n0.061997\n0.043634\n0.054097\n0.077172\n0.050246\n0.174663\n0.044486\n0.073935\n0.037279\n\n\nRonald Reagan\n0.042896\n0.039901\n0.044701\n0.231009\n0.048165\n0.076235\n0.040150\n0.140081\n0.140061\n0.081909\n...\n0.034992\n0.052896\n0.045076\n0.049203\n0.080074\n0.039373\n0.300796\n0.046391\n0.057221\n0.038241\n\n\nRutherford B. Hayes\n0.092012\n0.112656\n0.082619\n0.035344\n0.269502\n0.081089\n0.144648\n0.040511\n0.059357\n0.064508\n...\n1.000000\n0.058833\n0.074037\n0.131257\n0.078165\n0.077687\n0.046635\n0.139910\n0.066988\n0.097658\n\n\nTheodore Roosevelt\n0.056717\n0.070145\n0.077785\n0.069914\n0.057292\n0.087503\n0.045146\n0.077699\n0.074973\n0.072999\n...\n0.058833\n1.000000\n0.050832\n0.071917\n0.103435\n0.060783\n0.066718\n0.069393\n0.067994\n0.059469\n\n\nThomas Jefferson\n0.110742\n0.117074\n0.093668\n0.054143\n0.080109\n0.077970\n0.063310\n0.046509\n0.060950\n0.052945\n...\n0.074037\n0.050832\n1.000000\n0.093839\n0.069619\n0.051927\n0.047307\n0.067414\n0.075135\n0.109134\n\n\nUlysses S. Grant\n0.124756\n0.146622\n0.127001\n0.047495\n0.146124\n0.098388\n0.179516\n0.056097\n0.085503\n0.059127\n...\n0.131257\n0.071917\n0.093839\n1.000000\n0.091914\n0.121181\n0.060122\n0.167308\n0.080233\n0.137702\n\n\nWarren G. Harding\n0.082261\n0.092593\n0.090346\n0.073191\n0.080189\n0.128878\n0.066345\n0.065570\n0.107023\n0.094722\n...\n0.078165\n0.103435\n0.069619\n0.091914\n1.000000\n0.081010\n0.092189\n0.096931\n0.096031\n0.070389\n\n\nWilliam Howard Taft\n0.092794\n0.113945\n0.077516\n0.049368\n0.125491\n0.092294\n0.137828\n0.040417\n0.072974\n0.054787\n...\n0.077687\n0.060783\n0.051927\n0.121181\n0.081010\n1.000000\n0.045340\n0.082594\n0.087100\n0.124097\n\n\nWilliam J. Clinton\n0.046491\n0.050358\n0.049155\n0.317297\n0.049069\n0.076209\n0.045936\n0.183823\n0.128444\n0.086663\n...\n0.046635\n0.066718\n0.047307\n0.060122\n0.092189\n0.045340\n1.000000\n0.051195\n0.075808\n0.044786\n\n\nWilliam McKinley\n0.113427\n0.102445\n0.080710\n0.050867\n0.142834\n0.074285\n0.123470\n0.042885\n0.065945\n0.063699\n...\n0.139910\n0.069393\n0.067414\n0.167308\n0.096931\n0.082594\n0.051195\n1.000000\n0.067011\n0.103954\n\n\nWoodrow Wilson\n0.077418\n0.086139\n0.082443\n0.064416\n0.076850\n0.086982\n0.059776\n0.056082\n0.104088\n0.077462\n...\n0.066988\n0.067994\n0.075135\n0.080233\n0.096031\n0.087100\n0.075808\n0.067011\n1.000000\n0.081701\n\n\nZachary Taylor\n0.145153\n0.155541\n0.092401\n0.039508\n0.134368\n0.092193\n0.132799\n0.040505\n0.069061\n0.050414\n...\n0.097658\n0.059469\n0.109134\n0.137702\n0.070389\n0.124097\n0.044786\n0.103954\n0.081701\n1.000000\n\n\n\n\n43 rows √ó 43 columns\n\n\n\nHere we can see the degree of similarity of each president‚Äôs speech with each other. There is no speech with similarity more than 40%.\n\n# Compute the clusters from the similarity matrix,\n# using the Ward variance minimization algorithm\nZ = hierarchy.linkage(sim_df, 'ward')\nplt.rcParams['figure.figsize'] = [10,20]\n# Display this result as a horizontal dendrogram\na = hierarchy.dendrogram(Z,  leaf_font_size=20, labels=sim_df.index,  orientation=\"left\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-3",
    "href": "software/presidents_analysis.html#interpretation-3",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "Interpretation",
    "text": "Interpretation\nIn the dendrogram above, the presidents seem to be clustered based on the era that they served. Two big clusters are distinct ‚Äì the green one which includes mostly recent presidents 20th and 21st century, and ‚Äì the orange one which includes presidents of the 18th and 19th century.\n\nWho was the president whose speech was the most similar to the speech of Biden in 2022?\n\n\nv = sim_df[['Joseph R. Biden']]\nv\n\n\n\n\n\n\n\n\nJoseph R. Biden\n\n\n\n\nAbraham Lincoln\n0.030122\n\n\nAndrew Jackson\n0.035307\n\n\nAndrew Johnson\n0.032910\n\n\nBarack Obama\n0.345184\n\n\nBenjamin Harrison\n0.028887\n\n\nCalvin Coolidge\n0.054890\n\n\nChester A. Arthur\n0.031472\n\n\nDonald J. Trump\n0.220390\n\n\nDwight D. Eisenhower\n0.068947\n\n\nFranklin D. Roosevelt\n0.053099\n\n\nFranklin Pierce\n0.030974\n\n\nGeorge Bush\n0.227037\n\n\nGeorge W. Bush\n0.205001\n\n\nGeorge Washington\n0.012446\n\n\nGerald R. Ford\n0.094785\n\n\nGrover Cleveland\n0.025062\n\n\nHarry S. Truman\n0.052615\n\n\nHerbert Hoover\n0.045206\n\n\nJames Buchanan\n0.024993\n\n\nJames K. Polk\n0.038190\n\n\nJames Madison\n0.016895\n\n\nJames Monroe\n0.019880\n\n\nJimmy Carter\n0.198534\n\n\nJohn Adams\n0.016608\n\n\nJohn F. Kennedy\n0.094466\n\n\nJohn Quincy Adams\n0.024704\n\n\nJohn Tyler\n0.035437\n\n\nJoseph R. Biden\n1.000000\n\n\nLyndon B. Johnson\n0.107517\n\n\nMartin van Buren\n0.030007\n\n\nMillard Fillmore\n0.024063\n\n\nRichard Nixon\n0.113265\n\n\nRonald Reagan\n0.223184\n\n\nRutherford B. Hayes\n0.021369\n\n\nTheodore Roosevelt\n0.050805\n\n\nThomas Jefferson\n0.034194\n\n\nUlysses S. Grant\n0.039431\n\n\nWarren G. Harding\n0.052046\n\n\nWilliam Howard Taft\n0.028445\n\n\nWilliam J. Clinton\n0.306361\n\n\nWilliam McKinley\n0.027510\n\n\nWoodrow Wilson\n0.054481\n\n\nZachary Taylor\n0.033634\n\n\n\n\n\n\n\n\n# This is needed to display plots in a notebook\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10,10]\n\n# Select the column corresponding to Biden's address and \nv = sim_df['Joseph R. Biden']\n\n# Sort by ascending scores\nv_sorted = v.sort_values(ascending=True)\n\n# Plot this data has a horizontal bar plot\nv_sorted.plot.barh(x='lab', y='val', rot=0).plot()\n\n# Modify the axes labels and plot title for better readability\nplt.xlabel(\"Cosine distance\")\nplt.ylabel(\"\")\nplt.title(\"Most similar to Biden's\")\n\nText(0.5, 1.0, \"Most similar to Biden's\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-4",
    "href": "software/presidents_analysis.html#interpretation-4",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "Interpretation",
    "text": "Interpretation\nBiden‚Äôs address is most similar to that of Obama‚Äôs, follwed by Clinton and George Bush. As we can see, George Washington, John Adams and James Madison are the least similar to Biden. This can be explained by the different eras that each president lived. Speeches in 18th century are different than speeches of today. Biden‚Äôs speech similarity with Obama and Clinton makes sense also because they are all recently elected democrats.\n\nBonus points: (5 points): Develop and algorithm that can allow you to determine if the speech was given by a Democrat or by a republican.\n\nPS2: I will go over this homework on Thursday to help you think through how to solve it. You will be able to recycle a lot of code discussed.\n\nspeeches\n\n\n\n\n\n\n\n\npresident\nyear\nparty\nhtml\ntop ten\n\n\n\n\n0\nHoover\n1929\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('public', 33),('law', 25),('service', 23),('...\n\n\n1\nFD_Roosvelt\n1934\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('industrial', 9),('work', 8),('recovery', 7)...\n\n\n2\nTruman\n1949\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('prosperity', 12),('production', 12),('power...\n\n\n3\nEisenhower\n1957\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('free', 16),('security', 16),('economy', 12)...\n\n\n4\nKennedy\n1961\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('economic', 16),('development', 10),('peace'...\n\n\n5\nLyndon\n1965\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\n[('freedom', 12),('life', 9),('progress', 8),(...\n\n\n6\nNixon\n1974\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('peace', 27),('energy', 17),('war', 8),('pro...\n\n\n7\nFord\n1975\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('energy', 25),('oil', 20),('tax', 17),('econ...\n\n\n8\nCarter\n1978\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/the-...\n[('inflation', 17),('economic', 14),('tax', 13...\n\n\n9\nReagan\n1985\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('freedom', 20),('tax', 16),('growth', 14),('...\n\n\n10\nBush\n1989\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('budget', 17),('work', 12),('hope', 10),('dr...\n\n\n11\nClinton\n1997\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('children', 24),('work', 21),('budget', 17),...\n\n\n12\nBush\n2005\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('security', 29),('freedom', 20),('social', 1...\n\n\n13\nObama\n2013\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('jobs', 32),('work', 20),('energy', 18),('fa...\n\n\n14\nTrump\n2018\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('tax', 15),('last', 13),('together', 13),('w...\n\n\n15\nJoe\n2022\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\n[('folks', 19),('see', 15),('families', 15),('...\n\n\n\n\n\n\n\n\ndf2[[0]]\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\nAddress Before a Joint Session of the Congress...\n\n\n1\nAddress Before a Joint Session of Congress on ...\n\n\n2\nAddress Before a Joint Session of Congress on ...\n\n\n3\nAnnual Message to the Congress on the State of...\n\n\n4\nRadio Address Summarizing the State of the Uni...\n\n\n5\nFifth Annual Message | The American Presidency...\n\n\n6\nFirst Annual Message | The American Presidency...\n\n\n7\nFirst Annual Message | The American Presidency...\n\n\n8\nFirst Annual Message | The American Presidency...\n\n\n9\nFifth Annual Message | The American Presidency...\n\n\n10\nFifth Annual Message | The American Presidency...\n\n\n11\nState of the Union Message to the Congress: Ov...\n\n\n12\nState of the Union Message to the Congress on ...\n\n\n13\nAddress Before a Joint Session of the Congress...\n\n\n\n\n\n\n\n\nstate_speeches = pd.read_excel('state_speeches.xlsx')\nstate_speeches\n\n\n\n\n\n\n\n\npresident\nyear\nparty\nhtml\naddress\n\n\n\n\n0\nHoover\n1929\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\nThe Constitution requires that the President \"...\n\n\n1\nFD_Roosvelt\n1934\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\nI COME before you at the opening of the Regula...\n\n\n2\nTruman\n1949\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\nI am happy to report to this 81st Congress tha...\n\n\n3\nEisenhower\n1957\nrepublican\nhttps://www.presidency.ucsb.edu/documents/annu...\nI appear before the Congress today to report o...\n\n\n4\nKennedy\n1961\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\nIt is a pleasure to return from whence I came....\n\n\n5\nLyndon\n1965\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/annu...\nOn this Hill which was my home, I am stirred b...\n\n\n6\nNixon\n1974\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\nWe meet here tonight at a time of great challe...\n\n\n7\nFord\n1975\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\nTwenty-six years ago, a freshman Congressman, ...\n\n\n8\nCarter\n1978\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/the-...\nTwo years ago today we had the first caucus in...\n\n\n9\nReagan\n1985\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\nI come before you to report on the state of ou...\n\n\n10\nBush\n1989\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\nMr. Speaker, Mr. President, and distinguished ...\n\n\n11\nClinton\n1997\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\nMr. Speaker, Mr. Vice President, Members of th...\n\n\n12\nBush\n2005\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\nAs a new Congress gathers, all of us in the el...\n\n\n13\nObama\n2013\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\nPlease, everybody, have a seat. Mr. Speaker, M...\n\n\n14\nTrump\n2018\nrepublican\nhttps://www.presidency.ucsb.edu/documents/addr...\nThe President. Mr. Speaker, Mr. Vice President...\n\n\n15\nJoe\n2022\ndemocrat\nhttps://www.presidency.ucsb.edu/documents/addr...\nThe President. Thank you all very, very much. ...\n\n\n\n\n\n\n\n\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(state_speeches['address'], state_speeches['party'], test_size=0.3, \n                 random_state=53)\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n# Create count train and test variables\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ntfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train, y_train)\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\ncount_nb = MultinomialNB()\ncount_nb.fit(count_train, y_train)\ncount_nb_pred = count_nb.predict(count_test)\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.2\nNaiveBayes Count Score:  0.2\n\n\n\n%matplotlib inline\nfrom sklearn.metrics import plot_confusion_matrix\n\n\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred, labels=['republican', 'democrat'])\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred, labels=['republican', 'democrat'])\n\n# plot_confusion_matrix(tfidf_nb_cm, classes=['republican', 'democrat'], title=\"TF-IDF NB Confusion Matrix\")\n\n# plot_confusion_matrix(count_nb_cm, classes=['republican', 'democrat'], title=\"Count NB Confusion Matrix\", figure=1)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-5",
    "href": "software/presidents_analysis.html#interpretation-5",
    "title": "Interpretation of all president‚Äôs words since Truman",
    "section": "Interpretation",
    "text": "Interpretation\nIt looks like that algorithm‚Äôs power to identify whether the speech comes from a democrat or a republican is only 20%. In this case, for the algorithm to get stronger, more speeches are necessary from both sides, and maybe more text cleaning.\n\npip install jupyterthemes\n\nRequirement already satisfied: jupyterthemes in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (0.20.0)\nRequirement already satisfied: matplotlib&gt;=1.4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (3.4.3)\nRequirement already satisfied: lesscpy&gt;=0.11.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (0.15.0)\nRequirement already satisfied: notebook&gt;=5.6.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (6.4.5)\nRequirement already satisfied: ipython&gt;=5.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (7.29.0)\nRequirement already satisfied: jupyter-core in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (4.8.1)\nRequirement already satisfied: setuptools&gt;=18.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (58.0.4)\nRequirement already satisfied: appnope in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (0.1.2)\nRequirement already satisfied: traitlets&gt;=4.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (5.1.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (3.0.20)\nRequirement already satisfied: pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (2.10.0)\nRequirement already satisfied: backcall in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (0.2.0)\nRequirement already satisfied: pexpect&gt;4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (4.8.0)\nRequirement already satisfied: matplotlib-inline in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (0.1.2)\nRequirement already satisfied: decorator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (5.1.0)\nRequirement already satisfied: pickleshare in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (0.7.5)\nRequirement already satisfied: jedi&gt;=0.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython&gt;=5.4.1-&gt;jupyterthemes) (0.18.0)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=5.4.1-&gt;jupyterthemes) (0.8.2)\nRequirement already satisfied: six in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy&gt;=0.11.2-&gt;jupyterthemes) (1.16.0)\nRequirement already satisfied: ply in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy&gt;=0.11.2-&gt;jupyterthemes) (3.11)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib&gt;=1.4.3-&gt;jupyterthemes) (2.8.2)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib&gt;=1.4.3-&gt;jupyterthemes) (0.10.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib&gt;=1.4.3-&gt;jupyterthemes) (1.3.1)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib&gt;=1.4.3-&gt;jupyterthemes) (3.0.4)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib&gt;=1.4.3-&gt;jupyterthemes) (8.4.0)\nRequirement already satisfied: numpy&gt;=1.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib&gt;=1.4.3-&gt;jupyterthemes) (1.20.3)\nRequirement already satisfied: jinja2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (2.11.3)\nRequirement already satisfied: pyzmq&gt;=17 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (22.2.1)\nRequirement already satisfied: argon2-cffi in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (20.1.0)\nRequirement already satisfied: jupyter-client&gt;=5.3.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (6.1.12)\nRequirement already satisfied: Send2Trash&gt;=1.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (1.8.0)\nRequirement already satisfied: prometheus-client in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (0.11.0)\nRequirement already satisfied: nbformat in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (5.1.3)\nRequirement already satisfied: ipykernel in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (6.4.1)\nRequirement already satisfied: ipython-genutils in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (0.2.0)\nRequirement already satisfied: tornado&gt;=6.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (6.1)\nRequirement already satisfied: terminado&gt;=0.8.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (0.9.4)\nRequirement already satisfied: nbconvert in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook&gt;=5.6.0-&gt;jupyterthemes) (6.1.0)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=5.4.1-&gt;jupyterthemes) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;ipython&gt;=5.4.1-&gt;jupyterthemes) (0.2.5)\nRequirement already satisfied: cffi&gt;=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (1.14.6)\nRequirement already satisfied: pycparser in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from cffi&gt;=1.0.0-&gt;argon2-cffi-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (2.20)\nRequirement already satisfied: debugpy&lt;2.0,&gt;=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipykernel-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (1.4.1)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jinja2-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (1.1.1)\nRequirement already satisfied: entrypoints&gt;=0.2.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.3)\nRequirement already satisfied: jupyterlab-pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.1.2)\nRequirement already satisfied: pandocfilters&gt;=1.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (1.4.3)\nRequirement already satisfied: nbclient&lt;0.6.0,&gt;=0.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.5.3)\nRequirement already satisfied: bleach in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (4.0.0)\nRequirement already satisfied: testpath in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.5.0)\nRequirement already satisfied: defusedxml in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.7.1)\nRequirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.8.4)\nRequirement already satisfied: async-generator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (1.10)\nRequirement already satisfied: nest-asyncio in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient&lt;0.6.0,&gt;=0.5.0-&gt;nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (1.5.1)\nRequirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbformat-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (3.2.0)\nRequirement already satisfied: attrs&gt;=17.4.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (21.2.0)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,&gt;=2.4-&gt;nbformat-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.18.0)\nRequirement already satisfied: packaging in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (21.0)\nRequirement already satisfied: webencodings in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach-&gt;nbconvert-&gt;notebook&gt;=5.6.0-&gt;jupyterthemes) (0.5.1)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n!jt -l\n\nAvailable Themes: \n   chesterish\n   grade3\n   gruvboxd\n   gruvboxl\n   monokai\n   oceans16\n   onedork\n   solarizedd\n   solarizedl"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "About",
    "section": "",
    "text": "Resources and information you might find fun, interesting, helpful:\n\nWhy a change in mindset is necessary to protect environment: Biking to work in Texas By Andy Eskenazi\nAdvice for married men By Tim Hawkins\nLanding a Job after graduating By Jessica Ramses\nOldest restaurants in the world By Nutty History\nInterractive Website - Fun stuff and interesting dilemmas By Neal Agarwal\nThe Language Nerds Quirky linguistics and language fun facts\nWait But Why Quirky insights on interesting topics\nElection Betting Odds By Maxim Lott - Real time election odds from bets\nBackpacker‚Äôs guide to arriving at UPenn By Orestis Skoutellas\nHelping founders settle in San Francisco O. Skoutellas\nReferral win-win deals (credit/debit cards, phone coverage, etc)\nCost of living comparison in different cities in the world.\nCompound Interest Calculator How much can your savings sweat for you\n\nAbout me\n\nI have a blog that is occasionally updated üìù\nSee my projects for some things I‚Äôve been working on üõ†\nCurrently learning  and  on : LoizosKons\nLet‚Äôs play chess ‚ôû"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some words I wrote\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\n\n\nWhat did I learn from doing Standup Comedy\n\n\nSep 9, 2024\n\n\n\n\n\n\n\nHappy birthday!\n\n\nOct 12, 2023\n\n\n\n\n\n\n\nFour-Day Road Trip: Denver-Grand Canyon-Las Vegas\n\n\nAug 6, 2023\n\n\n\n\n\n\n\nMastering Monotony: 10 Techniques for Uninteresting Presentations\n\n\nJul 15, 2023\n\n\n\n\n\n\n\nComprehensive Activity Guide for Colorado\n\n\nJul 12, 2023\n\n\n\n\n\n\n\nExperiencing Colorado, One Bite at a Time¬†üçΩ\n\n\nJul 5, 2023\n\n\n\n\n\n\n\nTips for Smooth Sailing: Moving to Denver, Colorado\n\n\nJul 5, 2023\n\n\n\n\n\n\n\nHow to Be Unproductive: 9 Tips for Doing Less\n\n\nJun 5, 2023\n\n\n\n\n\n\n\nFood pitstops in Texas: Exploring Waco, Seguin, Mingus, and Strawn\n\n\nMay 31, 2023\n\n\n\n\n\n\n\nNepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp\n\n\nMar 7, 2023\n\n\n\n\n\n\n\nExperience Cyprus Like a Local!\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\nSUSI Entrepreneurship Program: A Free Ticket to an American Dream\n\n\nJan 5, 2023\n\n\n\n\n\n\n\nMaking the Most of Your University Experience at EUC: A Student‚Äôs Perspective\n\n\nOct 2, 2022\n\n\n\n\n\n\n\nCopenhagen: A Great Travel Guide for Your Next Visit\n\n\nJun 9, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-09-09-standup/index.html",
    "href": "posts/2024-09-09-standup/index.html",
    "title": "What did I learn from doing Standup Comedy",
    "section": "",
    "text": "Doing standup comedy can be like jumping out of a plane, except instead of a parachute, you‚Äôre armed with punchlines that may or may not land. At least you are guaranteed one thing: a room full of people staring at you, waiting to either laugh ‚Äî or, in some cases, reconsider their life choices for being there. Bottom line, it would be funny one way or another:)\nIn this article, I‚Äôm going to share the unexpected, fun, and challenging moments of what it‚Äôs like to purposefully try to make people laugh.\n\n\nThe first obstacle you have to face in doing stand-up comedy is the writing part. While it seems difficult to come up with jokes, there are some techniques that could help you come up with plenty of authentic material.\nWrite down topics that you are passionate about‚Ää‚Äî‚Ääthings you love, hate, fear, or feel strongly about. These can range from broad subjects like family, politics, or the universe, to specific ones like shopping for bananas, mean coworkers, or your grandpa‚Äôs new girlfriend. Choose topics that evoke strong emotions, whether it‚Äôs curiosity, anxiety, envy, or amazement. In comedy, the best material comes from topics you‚Äôre truly passionate about, so pick things that really move you. There are no bad topics if you care about them.\n\n\nOnce you have your topics, you can start exploring by using this formula. Being passionate about a subject, helps you develop material. Write your feelings about different things, people, places, and experiences in life. For example, you can say, ‚ÄúI love the American healthcare system because it reminds me to make responsible choices if I don‚Äôt want to become broke.‚Äù or ‚ÄúI hate prison because I look bad in orange‚Äù.\nThat formula will help you establish a basis to then build upon subjects that you truly care about and could go and explore further.\nOnce you have enough material you can start reading it in front of your friends, classmates, or your support group, and get their feedback on where you can elaborate more. For example, you may have 5 topics and 10 lines per topic of how you feel about it and why. Once you collect feedback there, you can dive deeper into these topics.\n\nFor example, if the focus sentence is ‚ÄúI love old people because they have no filters‚Äù. Go deeper into that. Say more. Any stories where you met someone like that? Be specific, compare, and contrast that with maybe other age groups. Etc.\n\n\n\n\nNormal, normal, weird‚Ää‚Äî‚Ääif you make a list of things, the 3rd item can be absurd or something unexpected. The first thing sets a premise, the second reinforces that premise, and the third is the punchline.\nQuick list form example: My favorite comedies are Dumb and Dumber, Mr Bean Holidays, and Titanic. Yes, after that I know what ice-breaker to use to start conversations!\nIn paragraph form: I find visiting a hospice interesting because it allows me to have conversations about how life used to be in the past; just the other day while I was talking to a grandma there, I was learning how people used to communicate without the internet, how complicated was to travel to places, and how their biggest fear was catching polio, instead of a cold from the office air conditioning‚Ä¶\nAuthor Brian Luff talks about the rule of three\n\n\n\nComparing two or more things that have differences you can talk about. Common ways to create contrast are: ‚ÄúWhen I was a kid we did X, nowadays, kids do Y‚Ä¶ When I was single I was doing X, now that I‚Äôm married I am doing Y‚Ä¶ etc. Atheists do this, Religious people do that‚Ä¶ etc.\nComedy stems from observation. Everything can be a joke when you try to understand how we got where we are at. Contradictory feelings about subjects undermine expectations, create surprise, and highlight absurdity. Introducing opposing ideas amplifies the ridiculousness, enhances irony, and delivers a punchline that releases built-up tension, making the humor more impactful and relatable. You can connect it with the formula above.\n\nFor example:\n\n\n‚ÄúI love old people because they have no filters to what they say, they don‚Äôt beat around the bush; they show their feelings immediately, whether they like you or not, you will know.‚Äù\n\n\n‚ÄúI hate old people because they frequently repeat the same stuff and they do things slowly, sometimes even on purpose!‚Äù\n\n‚ÄúEntering Mexico is easier than entering a Costco‚Äù\n\n\n\nIf you hear a comedian saying, ‚ÄúWhat‚Äôs next?‚Äù after complaining about something, they are using this technique. This allows you to view your stories and ideas from a perspective where you can be creative to reach at your own entertaining or even exaggerated conclusions that would ideally make people laugh.\nExamples\n\nBasic Premise: In a world where social media influencers exist, there must be people who follow them religiously.\nStep Up: In a world where social media influencers exist, should we also assume that their followers take life advice from someone whose biggest achievement is perfecting a duck face?\nDouble Step Up: In a world where social media influencers exist, should we expect the next Nobel Prize to be awarded for the best hashtag campaign? Because, you know, #WorldPeace starts with #GoodVibesOnly.\nOther premises: Music celebrities, went from Mozart to Elvis, to Taylor Swift; what‚Äôs next?! Republican party went from Lincoln to Reagan, to Trump; what‚Äôs next?!\n\nThis way of thinking is a useful exploratory tool and can take you and the audience on an unexpected journey. While it might not always bring the funniest joke, it would allow you to explore topics from different perspectives, and entertain your own creativity.\nIf Murder was legal, then what else would be true?\n\n\n\nUsing callbacks in comedy is effective because it rewards the audience for paying attention, creating a sense of connection and inside humor. It builds momentum, enhances the impact of earlier jokes, and brings cohesion to the set, making it feel more polished and satisfying. Callbacks can also make a punchline feel sharper by revisiting familiar territory in an unexpected way.\nExplaining callbacks\n\n\n\n\n\n\nStories that you are the protagonist, always have the potential to be funnier than just describing stories or events as an observer. I found that to be true because it creates a direct connection with the audience. When you are the protagonist, your reactions, emotions, and unique perspective add authenticity and relatability, making the humor more engaging and impactful. The potential for laughter is much higher when the audience can see themselves in your shoes or when they feel like they‚Äôre sharing the experience with you.\nLarry David personalizing the story starting at 0:49\n\n\n\nIf you talk down on something make sure you come across as an observer, and not as a judge. You want to come across as a comedian who delivers a point, not like a comedic version of Gordon Ramsay.\nIf you come across as too harsh or superior, the audience might feel uncomfortable or defensive. By positioning yourself as an observer, you invite the audience to laugh with you, not at someone else, creating a shared experience. This approach also allows the joke to be more relatable. That makes maintaining a positive rapport with the audience easier. The goal is to critique, question, or mock the situation, not ridicule people; unless you have a strong reason to make that your style.\n\n\n\nThe element of surprise can bring laughter. If you are driving a bus, the passengers won‚Äôt appreciate a sharp turn. However, if you are a comedian, people love it!\nIn the first 20 seconds of this video, Louis CK goes from a ‚Äúthank you‚Äù to talking about abortion.\n\n\n\nThis is how you mitigate stress, feel comfortable with your material, and sharpen your delivery. The smoother the delivery, the bigger the laughs you will get. Practice in front of a mirror to see your reactions, and hold something that looks like a microphone (hairbrush, toothbrush, shaving machine, a carrot‚Ä¶ ). You can reach out to friends that you think their feedback would help and practice in front of them too. Take a video of yourself, record, and repeat. This will help you get what you want to say on time, and also view yourself from a different perspective‚Ää‚Äî‚Ääbetter for self-evaluation and feedback. Even if you think you remember everything, think that you might forget when you are in front of a big audience, so do not underestimate the value of practicing.\n\n\n\n\nTo end it with a comparison, being a comedian is like the opposite of being an entrepreneur. Enterpreneurs think forward, and comedians think backward. Entrepreneurs ask themselves ‚ÄúWhat‚Äôs next?‚Äù. Comedians ask ‚ÄúHow did we end up in this situation‚Äù.\nDoing a standup was a great experience. I find that standup comedy helps you practice public speaking, resilience, and creativity. Also, it can push you out of your comfort zone. I found it to be a beautiful journey with lots of laughing, learning, and new friendships formed along the way.\nI am thankful to my classmates Philip, Angie, Melissa, Abraham, HellDog, and Alex, who made this learning experience enjoyable, shared their feedback, and jokes along the way. Special thanks/credit to my teacher Jeff Jurgens for teaching us all these cool theories, supporting us in setting up our first stand-up comedy gig, and offering to pay for therapy after hearing my material‚Ää‚Äî‚ÄäRule of Three:)\n\n\n\nOur cool flyer cover.\n\n\nIf you are in Gainesville and want to give it a try here is more information: https://www.communityimprov.com/classes"
  },
  {
    "objectID": "posts/2024-09-09-standup/index.html#learnings",
    "href": "posts/2024-09-09-standup/index.html#learnings",
    "title": "What did I learn from doing Standup Comedy",
    "section": "",
    "text": "The first obstacle you have to face in doing stand-up comedy is the writing part. While it seems difficult to come up with jokes, there are some techniques that could help you come up with plenty of authentic material.\nWrite down topics that you are passionate about‚Ää‚Äî‚Ääthings you love, hate, fear, or feel strongly about. These can range from broad subjects like family, politics, or the universe, to specific ones like shopping for bananas, mean coworkers, or your grandpa‚Äôs new girlfriend. Choose topics that evoke strong emotions, whether it‚Äôs curiosity, anxiety, envy, or amazement. In comedy, the best material comes from topics you‚Äôre truly passionate about, so pick things that really move you. There are no bad topics if you care about them.\n\n\nOnce you have your topics, you can start exploring by using this formula. Being passionate about a subject, helps you develop material. Write your feelings about different things, people, places, and experiences in life. For example, you can say, ‚ÄúI love the American healthcare system because it reminds me to make responsible choices if I don‚Äôt want to become broke.‚Äù or ‚ÄúI hate prison because I look bad in orange‚Äù.\nThat formula will help you establish a basis to then build upon subjects that you truly care about and could go and explore further.\nOnce you have enough material you can start reading it in front of your friends, classmates, or your support group, and get their feedback on where you can elaborate more. For example, you may have 5 topics and 10 lines per topic of how you feel about it and why. Once you collect feedback there, you can dive deeper into these topics.\n\nFor example, if the focus sentence is ‚ÄúI love old people because they have no filters‚Äù. Go deeper into that. Say more. Any stories where you met someone like that? Be specific, compare, and contrast that with maybe other age groups. Etc.\n\n\n\n\nNormal, normal, weird‚Ää‚Äî‚Ääif you make a list of things, the 3rd item can be absurd or something unexpected. The first thing sets a premise, the second reinforces that premise, and the third is the punchline.\nQuick list form example: My favorite comedies are Dumb and Dumber, Mr Bean Holidays, and Titanic. Yes, after that I know what ice-breaker to use to start conversations!\nIn paragraph form: I find visiting a hospice interesting because it allows me to have conversations about how life used to be in the past; just the other day while I was talking to a grandma there, I was learning how people used to communicate without the internet, how complicated was to travel to places, and how their biggest fear was catching polio, instead of a cold from the office air conditioning‚Ä¶\nAuthor Brian Luff talks about the rule of three\n\n\n\nComparing two or more things that have differences you can talk about. Common ways to create contrast are: ‚ÄúWhen I was a kid we did X, nowadays, kids do Y‚Ä¶ When I was single I was doing X, now that I‚Äôm married I am doing Y‚Ä¶ etc. Atheists do this, Religious people do that‚Ä¶ etc.\nComedy stems from observation. Everything can be a joke when you try to understand how we got where we are at. Contradictory feelings about subjects undermine expectations, create surprise, and highlight absurdity. Introducing opposing ideas amplifies the ridiculousness, enhances irony, and delivers a punchline that releases built-up tension, making the humor more impactful and relatable. You can connect it with the formula above.\n\nFor example:\n\n\n‚ÄúI love old people because they have no filters to what they say, they don‚Äôt beat around the bush; they show their feelings immediately, whether they like you or not, you will know.‚Äù\n\n\n‚ÄúI hate old people because they frequently repeat the same stuff and they do things slowly, sometimes even on purpose!‚Äù\n\n‚ÄúEntering Mexico is easier than entering a Costco‚Äù\n\n\n\nIf you hear a comedian saying, ‚ÄúWhat‚Äôs next?‚Äù after complaining about something, they are using this technique. This allows you to view your stories and ideas from a perspective where you can be creative to reach at your own entertaining or even exaggerated conclusions that would ideally make people laugh.\nExamples\n\nBasic Premise: In a world where social media influencers exist, there must be people who follow them religiously.\nStep Up: In a world where social media influencers exist, should we also assume that their followers take life advice from someone whose biggest achievement is perfecting a duck face?\nDouble Step Up: In a world where social media influencers exist, should we expect the next Nobel Prize to be awarded for the best hashtag campaign? Because, you know, #WorldPeace starts with #GoodVibesOnly.\nOther premises: Music celebrities, went from Mozart to Elvis, to Taylor Swift; what‚Äôs next?! Republican party went from Lincoln to Reagan, to Trump; what‚Äôs next?!\n\nThis way of thinking is a useful exploratory tool and can take you and the audience on an unexpected journey. While it might not always bring the funniest joke, it would allow you to explore topics from different perspectives, and entertain your own creativity.\nIf Murder was legal, then what else would be true?\n\n\n\nUsing callbacks in comedy is effective because it rewards the audience for paying attention, creating a sense of connection and inside humor. It builds momentum, enhances the impact of earlier jokes, and brings cohesion to the set, making it feel more polished and satisfying. Callbacks can also make a punchline feel sharper by revisiting familiar territory in an unexpected way.\nExplaining callbacks"
  },
  {
    "objectID": "posts/2024-09-09-standup/index.html#delivery",
    "href": "posts/2024-09-09-standup/index.html#delivery",
    "title": "What did I learn from doing Standup Comedy",
    "section": "",
    "text": "Stories that you are the protagonist, always have the potential to be funnier than just describing stories or events as an observer. I found that to be true because it creates a direct connection with the audience. When you are the protagonist, your reactions, emotions, and unique perspective add authenticity and relatability, making the humor more engaging and impactful. The potential for laughter is much higher when the audience can see themselves in your shoes or when they feel like they‚Äôre sharing the experience with you.\nLarry David personalizing the story starting at 0:49\n\n\n\nIf you talk down on something make sure you come across as an observer, and not as a judge. You want to come across as a comedian who delivers a point, not like a comedic version of Gordon Ramsay.\nIf you come across as too harsh or superior, the audience might feel uncomfortable or defensive. By positioning yourself as an observer, you invite the audience to laugh with you, not at someone else, creating a shared experience. This approach also allows the joke to be more relatable. That makes maintaining a positive rapport with the audience easier. The goal is to critique, question, or mock the situation, not ridicule people; unless you have a strong reason to make that your style.\n\n\n\nThe element of surprise can bring laughter. If you are driving a bus, the passengers won‚Äôt appreciate a sharp turn. However, if you are a comedian, people love it!\nIn the first 20 seconds of this video, Louis CK goes from a ‚Äúthank you‚Äù to talking about abortion.\n\n\n\nThis is how you mitigate stress, feel comfortable with your material, and sharpen your delivery. The smoother the delivery, the bigger the laughs you will get. Practice in front of a mirror to see your reactions, and hold something that looks like a microphone (hairbrush, toothbrush, shaving machine, a carrot‚Ä¶ ). You can reach out to friends that you think their feedback would help and practice in front of them too. Take a video of yourself, record, and repeat. This will help you get what you want to say on time, and also view yourself from a different perspective‚Ää‚Äî‚Ääbetter for self-evaluation and feedback. Even if you think you remember everything, think that you might forget when you are in front of a big audience, so do not underestimate the value of practicing."
  },
  {
    "objectID": "posts/2024-09-09-standup/index.html#conclusion",
    "href": "posts/2024-09-09-standup/index.html#conclusion",
    "title": "What did I learn from doing Standup Comedy",
    "section": "",
    "text": "To end it with a comparison, being a comedian is like the opposite of being an entrepreneur. Enterpreneurs think forward, and comedians think backward. Entrepreneurs ask themselves ‚ÄúWhat‚Äôs next?‚Äù. Comedians ask ‚ÄúHow did we end up in this situation‚Äù.\nDoing a standup was a great experience. I find that standup comedy helps you practice public speaking, resilience, and creativity. Also, it can push you out of your comfort zone. I found it to be a beautiful journey with lots of laughing, learning, and new friendships formed along the way.\nI am thankful to my classmates Philip, Angie, Melissa, Abraham, HellDog, and Alex, who made this learning experience enjoyable, shared their feedback, and jokes along the way. Special thanks/credit to my teacher Jeff Jurgens for teaching us all these cool theories, supporting us in setting up our first stand-up comedy gig, and offering to pay for therapy after hearing my material‚Ää‚Äî‚ÄäRule of Three:)\n\n\n\nOur cool flyer cover.\n\n\nIf you are in Gainesville and want to give it a try here is more information: https://www.communityimprov.com/classes"
  }
]