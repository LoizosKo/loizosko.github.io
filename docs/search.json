[
  {
    "objectID": "posts/2022-06-09-denmark/index.html",
    "href": "posts/2022-06-09-denmark/index.html",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "",
    "text": "Copenhagen\nAre you visiting Copenhagen anytime soon? If yes, keep reading as you will find some related info and suggestions that might help you plan your visit! As an exchange student in Denmark back in 2018, I had enough time to explore it. While you might be for a few days there, this can help you allocate your time efficiently during your visit."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#currency",
    "href": "posts/2022-06-09-denmark/index.html#currency",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "CURRENCY",
    "text": "CURRENCY\nThe main currency is Danish Krone. Live currency converter. However, they accept card payments everywhere. With Revolut or Venmo you should be fine."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#transportation",
    "href": "posts/2022-06-09-denmark/index.html#transportation",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "TRANSPORTATION",
    "text": "TRANSPORTATION\n\nThe Metro is convenient and it takes you everywhere you want to go within the city:\n\nRejseplanen app - This is the app for the metro/train. It is designed specifically for them and it works like Google maps. Great navigation tool while you are in Denmark. There will be machines (also a person for support) at the airport where you can buy tickets. Also, Denmark is infamous for cycling with great infrastructure. You can rent bikes as well and ride across the city."
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-visits",
    "href": "posts/2022-06-09-denmark/index.html#cool-visits",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "COOL VISITS",
    "text": "COOL VISITS\nNyhavn (The most iconic part of Copenhagen. Highly recommended. the Closest metro station: Kongens Nytorv station.)\n\n\n\nNyhavn\n\n\n\nSomething you can do is to walk through the shopping center of Copenhagen. You can go to Norreport station and walk towards Nyhavn. It is like a 1 to 1.5 mile (1.6‚Äì2.4km) walk. There are many stores and restaurants in between (along the yellow line on the map). While it is not a straightforward walk it is a pleasant one as you are passing through the shopping area of the city.\n\n\nRosenborg Castle & Botanical Garden (Nice places to walk around. Also, Rosenborg castle is a historic place that now operates as a museum ticket info. Have no opinion for the museum though as I haven‚Äôt been. Closest metro station: Norreport station)\nLittle Mermaid (You will often see it in souvenir shops as it is considered as one of the main attraction points in Copenhagen along Nyhavn. While you might not be impressed by it due to its tiny size, you may want to see it simply for its touristic outreach.) Osterport Station.\nAmalienborg (The equivalent ‚ÄòBuckingham palace‚Äô of Denmark. Cool for walking around and taking pictures with the royal guards. Have no opinion for the museum there though as I haven‚Äôt been. tick info)\nCarlsberg Factory (if you like beer this is a nice place to visit. There is a tour in the factory and some beer tasting in the end. Carlsberg Station.)\nTivoli Gardens/Park (Located in front of the Central Station. The most famous park in Copenhagen. Kobenhavn H station)\nFisketorvet (If you like Malls, this can be an option. Most famous mall in the city.)\nBlack Diamond (The largest library in Denmark. It is a modern building. A Nice place to visit. Christianshavn station.)"
  },
  {
    "objectID": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "href": "posts/2022-06-09-denmark/index.html#cool-neighbourhoods",
    "title": "Copenhagen: A Great Travel Guide for Your Next Visit",
    "section": "COOL NEIGHBOURHOODS",
    "text": "COOL NEIGHBOURHOODS\n\nFreetown Christiania (It is considered an independent town within Copenhagen where people go there to chillax. It is a renowned marijuana place since Christiania is the only part of Copenhagen where Danish law is not enforced. However, it is a safe area that is worth a visit mainly due to its particularity. Highly recommended.) Metro station: Christianshavn.\nBeautiful Parks (especially for summer walks):\n\n\nFrederiksberg Have (One of the most beautiful parks in Copenhagen) Frederiksberg station.\nSuperkilen Park (Norrebro station). Fun fact, Norrebro is an area where many immigrants stay. Diverse area.\nVestre Kirkegard (near Carlsberg factory) Metro: Carlsberg Station\n\nFOOD & COOL PLACES (min $ /max $$$$)\n\nUnion Kitchen $$ (Cool place for brunch. Heard great things from friends that have been there.)\nSticks‚Äôn‚ÄôSushi $$$ (Great sushi place. Known rooftop restaurant in the city.)\nDalle Valle $ (variety food buffet - alue for money place with plenty of food) Not fast food, but not a high-class restaurant. Almost always lots of people, decent food, a variety of options. Recommended for lunch.\nConditori La Glace $$$ (Traditional Danish pastries, it is the oldest patisserie in Denmark - founded in 1870, good for dessert)\nEspresso House $$ (coffee place, despite its Swedish origin it is considered as the ‚ÄòStarbucks‚Äô of Denmark)\nBastard cafe $$ (a place full of board games. A nice place to go with friends for coffee/snacks/beer and board games.\nTaphouse $$ (Famous bar in the city. If you like beer then this is the place for you. It has one of the largest beer selections in Europe.)\n\nYou can always map your choices and combine visits that are close to each other. Also, since it is Scandinavia, remember to check the weather while preparing your bags! You are visiting arguably the most beautiful city in Scandinavia so try to enjoy every minute of it!\nHave fun and safe travels!"
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html",
    "href": "posts/2022-10-22-undergrad/index.html",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "",
    "text": "European University Cyprus\nEuropean University Cyprus, mainly has a Cypriot community where students come with their established friend groups from school, so you need to put more effort in socializing. For example, if you are coming from abroad you need to establish some common ground and try to do some activities with your colleagues. Moreover, the class atmosphere as well as in the cafeteria helps the newcomers to adjust socially to life at university. Even if you do something apart from university in the city (e.g.¬†join a gym, attend a student event) there is a high possibility of bumping into people that study at the same university. Therefore, I would say that it is not difficult to cover your social needs in that sense and overcome the social barriers during the first months ‚Äî just get involved!"
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#academics",
    "href": "posts/2022-10-22-undergrad/index.html#academics",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "ACADEMICS",
    "text": "ACADEMICS\nThe academic adjustment was more demanding in comparison with the social one. While I began university after a year and a half gap and I was not confident with my academic abilities, I did very well at school. As long as you are organized and focused and you prioritize, you will set yourself up for good grades and success. One strategy that helped me is that I kept track of the material for each course by filing it all separately. This eliminated unnecessary stress. This method, in conjunction with a systematic study approach by studying each week right after each lecture, allowed me to absorb the learning material in a very effective way. In addition, I was doing a 3 hour focused study sessions without distractions (i.e.¬†phone away, water/drinks next to me, and using the bathroom before so I don‚Äôt have excuse to stop). At the university, you will also have your personal advisor who, in coordination with your department, will support you academically. This is the person whom you will be addressing all the concerns you have regarding the courses. Having a personal advisor helps you to adjust at first. Having him/her throughout your studies gives you guidance on what classess to choose based on your goals, which is also important."
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#set-up-your-career",
    "href": "posts/2022-10-22-undergrad/index.html#set-up-your-career",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "SET-UP YOUR CAREER",
    "text": "SET-UP YOUR CAREER\nFor job opportunities and internships, the EUC Career Center has a lot of great resources. One of these is their own employment platform ‚Äî ‚ÄúCSM Simplicity‚Äù ‚Äî for our students. It is a great tool whether you are looking for a part-time job or to really launch your career after you‚Äôve finished your degree. Their Career Center‚Äôs priority is to help EUC students to achieve their career goals. They organize career workshops and share many opportunities with the students. They also offer personal consultation to students for writing a CV or acing an interview. Moreover, if you make clear that you are interested, they send you personally current job and internship offers tailored to your interests and study field."
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#connect-with-faculty",
    "href": "posts/2022-10-22-undergrad/index.html#connect-with-faculty",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "CONNECT WITH FACULTY",
    "text": "CONNECT WITH FACULTY\nIn my school journey, I have also had the support of the faculty. The professors are friendly and approachable. They have office hours and during that time they are open to discussing anything with the students. Discussions with my professors also gave me insights which then helped me to formulate tailored study methodology in each course. Another positive element that comes along with the interaction with professors is that you get to know about new program opportunities that under normal circumstances you wouldn‚Äôt know about. Something that for me created a snowball effect as it evolved to be something larger. For example, the interaction I have had with one of my professors, led me to get exposure to different programs outside school such as the Study of the US Institutes for Student Leaders; a fully funded program in the US for 6 weeks. I found out about this program as I was participating in a workshop outside of the university in which he organized and invited our class.\n\n\n\nStudy of the US Institutes for Student Leaders - USA"
  },
  {
    "objectID": "posts/2022-10-22-undergrad/index.html#pursue-opportunities-abroad",
    "href": "posts/2022-10-22-undergrad/index.html#pursue-opportunities-abroad",
    "title": "Making the Most of Your University Experience at EUC: A Student‚Äôs Perspective",
    "section": "PURSUE OPPORTUNITIES ABROAD",
    "text": "PURSUE OPPORTUNITIES ABROAD\nThis brings us to the travelling opportunities that students have to explore the world. Another part of the university that I would like to talk about is the Erasmus exchange opportunity that offers. It gives you the ability to do an internship or to study in another EU country, which is the most popular option. Since I entered university, I have tried to get involved with the Erasmus program to experience what I had been hearing about that is so amazing. So, I spent my third semester in Denmark living as an exchange student. This was really a worthwhile experience. First, I made new friends and during that time frame, I had the chance to travel a lot. But even more significant was the effect upon my arrival back home. I came back with more confidence; higher ambitions and I was able to see more clearly what is happening in the international arena. Then I joined the Erasmus Student Network organization in Cyprus which deals with incoming exchange students. So I extended my Erasmus experience as I expanded it locally with all its benefits that come along. This experience also had a snowball effect. That is because then I have had many opportunities to participate in other short-term funded Erasmus exchange programs in Europe. Furthermore, through being a part of active youth in Cyprus I have also found out about, applied to, and been selected to be a part of the Cypriot delegation at Model United Nations at Harvard University in Boston. Something that opened the doors for me for being a part of even more activities beyond Europe.\n\n\n\nHarvard National Model United Nations\n\n\nRegarding the Erasmus experience that EUC offers, there are many countries that the university has an agreement with, and YOU as a student have the chance to attend. On the university‚Äôs website, you can find the Erasmus agreements that EUC has and choose according to your field of study and personal preferences.\nIn conclusion, there is a lot that you can gain from European University as long as you take initiative. It is up to YOU to chase those opportunities that are aligned with your interests, and you believe that have the highest potential. Show interest in the classes, invest in relationships with students and professors and stay up to date with the Erasmus office and the career center for opportunities at home and abroad. My biggest tips? Get involved, stay organized and never be afraid to really go for it! I had such great experiences at EUC by living by these words.\nLoizos Konstantinou, 2021‚Äô\nBusiness Economics Undergrad"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "import pandas as pd\n\n\nmyData = pd.read_csv('Header_data.csv', encoding=\"latin\")\nprint(myData.head())\n\n   √ø√æT\n0  NaN\n1  NaN"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Hi üëã I am Loizos! This is my personal website.\nBehavioral Economist @ UPenn 22‚Äô \nHere you will find travel tips, coding projects and, learn more about me.\nI have a blog that is occasionally updated üìù\nSee my projects for some things I‚Äôve been working on üõ†\nFeel free to get in contact below"
  },
  {
    "objectID": "software/credcardpred.html",
    "href": "software/credcardpred.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "#1. Properly load the data into Jupyter Notebooks\n\nimport matplotlib.pyplot as plt #matplotlib generates graphs.\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC #i.e. Support Vector Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ndata = pd.read_csv(\"crx.data\", header=None)\nprint(data.columns)\n\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], dtype='int64')\n\n\nChange the column names to correspond to the ‚Äúreal‚Äù labels from the second link.\n\ndata.rename(columns={1 : \"age\", \n                     2 : \"debt\",\n                     3 : \"married\",\n                     4 : \"bankcustomer\", \n                     5 : \"educationlevel\", \n                     6 : \"ethnicity\",\n                     7 : \"yearsemployed\", \n                     8 : \"priordefault\",\n                     9 : \"employed\",\n                     10 : \"creditscore\",\n                     11 : \"driverslicense\",\n                     12 : \"citizen\",\n                     13 : \"zipcode\",\n                     14 : \"income\",\n                     15 : \"approved\"}, inplace=True)\nprint(data.head())\n\n   0    age   debt married bankcustomer educationlevel ethnicity  \\\n0  b  30.83  0.000       u            g              w         v   \n1  a  58.67  4.460       u            g              q         h   \n2  a  24.50  0.500       u            g              q         h   \n3  b  27.83  1.540       u            g              w         v   \n4  b  20.17  5.625       u            g              w         v   \n\n   yearsemployed priordefault employed  creditscore driverslicense citizen  \\\n0           1.25            t        t            1              f       g   \n1           3.04            t        t            6              f       g   \n2           1.50            t        f            0              f       g   \n3           3.75            t        t            5              t       g   \n4           1.71            t        f            0              f       s   \n\n  zipcode  income approved  \n0   00202       0        +  \n1   00043     560        +  \n2   00280     824        +  \n3   00100       3        +  \n4   00120       0        +  \n\n\nRemove all question marks from value ‚Äòage‚Äô, and convert it to numerical.\n\ndata.loc[83,'age'] = \"\"\ndata.loc[86,'age'] = \"\"\ndata.loc[92,'age'] = \"\"\ndata.loc[97,'age'] = \"\"\ndata.loc[254,'age'] = \"\"\ndata.loc[286,'age'] = \"\"\ndata.loc[329,'age'] = \"\"\ndata.loc[445,'age'] = \"\"\ndata.loc[450,'age'] = \"\"\ndata.loc[500,'age'] = \"\"\ndata.loc[515,'age'] = \"\"\ndata.loc[608,'age'] = \"\"\n\n\n\ndata['age'] = pd.to_numeric(data['age'])\n\n#2. Summarize the data.Frequency tables for categorical variables and histograms for continuous variables.\n\n%matplotlib inline\nz = data.hist(column=['debt', 'age', 'yearsemployed', 'income', 'creditscore'],bins=5, figsize=(12,7))\nprint(z) #is not adding them i.e. it creates a longer list.\n\nsummary_data = data.describe()\nprint(summary_data)\n\n#Categorical variables:\n#             \"married\", \n#             \"bankcustomer\", \n#             'educationlevel', \n#             'ethnicity',  \n#             'priordefault', \n#             'employed', \n#             'driverslicense',\n#             'citizen',\n#             'approved'\n\nmarried = data.loc[:,\"married\"].value_counts()\nprint(married)\n\nbankcustomer = data.loc[:,\"bankcustomer\"].value_counts()\nprint(bankcustomer)\n\neducationlevel = data.loc[:,\"educationlevel\"].value_counts()\nprint(educationlevel)\n\nethnicity = data.loc[:,\"ethnicity\"].value_counts()\nprint(ethnicity)\n\npriordefault = data.loc[:,\"priordefault\"].value_counts()\nprint(priordefault)\n\nemployed = data.loc[:,\"employed\"].value_counts()\nprint(employed)\n\ndriverslicense = data.loc[:,\"driverslicense\"].value_counts()\nprint(driverslicense)\n\napproved = data.loc[:,\"approved\"].value_counts()\nprint(approved)\n\ncitizen = data.loc[:,\"citizen\"].value_counts()\nprint(citizen)\n\n\n[[<AxesSubplot:title={'center':'debt'}>\n  <AxesSubplot:title={'center':'age'}>]\n [<AxesSubplot:title={'center':'yearsemployed'}>\n  <AxesSubplot:title={'center':'income'}>]\n [<AxesSubplot:title={'center':'creditscore'}> <AxesSubplot:>]]\n              age        debt  yearsemployed  creditscore         income\ncount  678.000000  690.000000     690.000000    690.00000     690.000000\nmean    31.568171    4.758725       2.223406      2.40000    1017.385507\nstd     11.957862    4.978163       3.346513      4.86294    5210.102598\nmin     13.750000    0.000000       0.000000      0.00000       0.000000\n25%     22.602500    1.000000       0.165000      0.00000       0.000000\n50%     28.460000    2.750000       1.000000      0.00000       5.000000\n75%     38.230000    7.207500       2.625000      3.00000     395.500000\nmax     80.250000   28.000000      28.500000     67.00000  100000.000000\nu    519\ny    163\n?      6\nl      2\nName: married, dtype: int64\ng     519\np     163\n?       6\ngg      2\nName: bankcustomer, dtype: int64\nc     137\nq      78\nw      64\ni      59\naa     54\nff     53\nk      51\ncc     41\nm      38\nx      38\nd      30\ne      25\nj      10\n?       9\nr       3\nName: educationlevel, dtype: int64\nv     399\nh     138\nbb     59\nff     57\n?       9\nj       8\nz       8\ndd      6\nn       4\no       2\nName: ethnicity, dtype: int64\nt    361\nf    329\nName: priordefault, dtype: int64\nf    395\nt    295\nName: employed, dtype: int64\nf    374\nt    316\nName: driverslicense, dtype: int64\n-    383\n+    307\nName: approved, dtype: int64\ng    625\ns     57\np      8\nName: citizen, dtype: int64\n\n\n\n\n\nOur data is skewed towards the left of the distribution because there are some outliers in our data, such as customers with a high income and credit score relative to the overall dataset. However, we are working with a limited number of data and it is still in the initial stage of development, we can leave it as-is for now. We can try to remove the outliers when we are tuning our model or when we have more data to work with.\n#3. Split the sample into a test set and a train set with 20% of data being in the test dataset. Your random seed should be 808.\n\n#split dataset into train and test, and set the random_state so it maintain the same result\ndata_features = data[['debt', \"age\", \"married\",\"bankcustomer\", \"educationlevel\", \"ethnicity\", \"yearsemployed\", \"priordefault\",\"employed\",\"creditscore\",\n\"driverslicense\",\"citizen\",\"income\"]]\ndata_target = data['approved']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data_features,\n                                                    data_target,\n                                                    test_size = 0.2,\n                                                    random_state = 808)\n\n#4. Try the following algorithms and choose the one that generates the best accuracy:\n\n#First, we convert categorical variables to numerical.\ndata1 = pd.get_dummies(data).dropna()\nprint(data1)\n\n\nprint(list(data1.columns))\n\n       age    debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n0    30.83   0.000           1.25            1       0    0    0    1   \n1    58.67   4.460           3.04            6     560    0    1    0   \n2    24.50   0.500           1.50            0     824    0    1    0   \n3    27.83   1.540           3.75            5       3    0    0    1   \n4    20.17   5.625           1.71            0       0    0    0    1   \n..     ...     ...            ...          ...     ...  ...  ...  ...   \n685  21.08  10.085           1.25            0       0    0    0    1   \n686  22.67   0.750           2.00            2     394    0    1    0   \n687  25.25  13.500           2.00            1       1    0    1    0   \n688  17.92   0.205           0.04            0     750    0    0    1   \n689  35.00   3.375           8.29            0       0    0    0    1   \n\n     married_?  married_l  ...  zipcode_00720  zipcode_00760  zipcode_00840  \\\n0            0          0  ...              0              0              0   \n1            0          0  ...              0              0              0   \n2            0          0  ...              0              0              0   \n3            0          0  ...              0              0              0   \n4            0          0  ...              0              0              0   \n..         ...        ...  ...            ...            ...            ...   \n685          0          0  ...              0              0              0   \n686          0          0  ...              0              0              0   \n687          0          0  ...              0              0              0   \n688          0          0  ...              0              0              0   \n689          0          0  ...              0              0              0   \n\n     zipcode_00928  zipcode_00980  zipcode_01160  zipcode_02000  zipcode_?  \\\n0                0              0              0              0          0   \n1                0              0              0              0          0   \n2                0              0              0              0          0   \n3                0              0              0              0          0   \n4                0              0              0              0          0   \n..             ...            ...            ...            ...        ...   \n685              0              0              0              0          0   \n686              0              0              0              0          0   \n687              0              0              0              0          0   \n688              0              0              0              0          0   \n689              0              0              0              0          0   \n\n     approved_+  approved_-  \n0             1           0  \n1             1           0  \n2             1           0  \n3             1           0  \n4             1           0  \n..          ...         ...  \n685           0           1  \n686           0           1  \n687           0           1  \n688           0           1  \n689           0           1  \n\n[678 rows x 223 columns]\n['age', 'debt', 'yearsemployed', 'creditscore', 'income', '0_?', '0_a', '0_b', 'married_?', 'married_l', 'married_u', 'married_y', 'bankcustomer_?', 'bankcustomer_g', 'bankcustomer_gg', 'bankcustomer_p', 'educationlevel_?', 'educationlevel_aa', 'educationlevel_c', 'educationlevel_cc', 'educationlevel_d', 'educationlevel_e', 'educationlevel_ff', 'educationlevel_i', 'educationlevel_j', 'educationlevel_k', 'educationlevel_m', 'educationlevel_q', 'educationlevel_r', 'educationlevel_w', 'educationlevel_x', 'ethnicity_?', 'ethnicity_bb', 'ethnicity_dd', 'ethnicity_ff', 'ethnicity_h', 'ethnicity_j', 'ethnicity_n', 'ethnicity_o', 'ethnicity_v', 'ethnicity_z', 'priordefault_f', 'priordefault_t', 'employed_f', 'employed_t', 'driverslicense_f', 'driverslicense_t', 'citizen_g', 'citizen_p', 'citizen_s', 'zipcode_00000', 'zipcode_00017', 'zipcode_00020', 'zipcode_00021', 'zipcode_00022', 'zipcode_00024', 'zipcode_00028', 'zipcode_00029', 'zipcode_00030', 'zipcode_00032', 'zipcode_00040', 'zipcode_00043', 'zipcode_00045', 'zipcode_00049', 'zipcode_00050', 'zipcode_00052', 'zipcode_00056', 'zipcode_00060', 'zipcode_00062', 'zipcode_00070', 'zipcode_00073', 'zipcode_00075', 'zipcode_00076', 'zipcode_00080', 'zipcode_00086', 'zipcode_00088', 'zipcode_00092', 'zipcode_00093', 'zipcode_00094', 'zipcode_00096', 'zipcode_00099', 'zipcode_00100', 'zipcode_00102', 'zipcode_00108', 'zipcode_00110', 'zipcode_00112', 'zipcode_00117', 'zipcode_00120', 'zipcode_00121', 'zipcode_00128', 'zipcode_00129', 'zipcode_00130', 'zipcode_00132', 'zipcode_00136', 'zipcode_00140', 'zipcode_00141', 'zipcode_00144', 'zipcode_00145', 'zipcode_00150', 'zipcode_00152', 'zipcode_00154', 'zipcode_00156', 'zipcode_00160', 'zipcode_00163', 'zipcode_00164', 'zipcode_00167', 'zipcode_00168', 'zipcode_00170', 'zipcode_00171', 'zipcode_00174', 'zipcode_00176', 'zipcode_00178', 'zipcode_00180', 'zipcode_00181', 'zipcode_00186', 'zipcode_00188', 'zipcode_00195', 'zipcode_00200', 'zipcode_00202', 'zipcode_00204', 'zipcode_00208', 'zipcode_00210', 'zipcode_00211', 'zipcode_00212', 'zipcode_00216', 'zipcode_00220', 'zipcode_00221', 'zipcode_00224', 'zipcode_00225', 'zipcode_00228', 'zipcode_00230', 'zipcode_00231', 'zipcode_00232', 'zipcode_00239', 'zipcode_00240', 'zipcode_00250', 'zipcode_00252', 'zipcode_00253', 'zipcode_00254', 'zipcode_00256', 'zipcode_00260', 'zipcode_00263', 'zipcode_00268', 'zipcode_00272', 'zipcode_00274', 'zipcode_00276', 'zipcode_00280', 'zipcode_00288', 'zipcode_00290', 'zipcode_00292', 'zipcode_00300', 'zipcode_00303', 'zipcode_00309', 'zipcode_00311', 'zipcode_00312', 'zipcode_00320', 'zipcode_00329', 'zipcode_00330', 'zipcode_00333', 'zipcode_00340', 'zipcode_00348', 'zipcode_00349', 'zipcode_00350', 'zipcode_00352', 'zipcode_00356', 'zipcode_00360', 'zipcode_00368', 'zipcode_00369', 'zipcode_00370', 'zipcode_00371', 'zipcode_00372', 'zipcode_00375', 'zipcode_00380', 'zipcode_00381', 'zipcode_00383', 'zipcode_00393', 'zipcode_00395', 'zipcode_00396', 'zipcode_00399', 'zipcode_00400', 'zipcode_00408', 'zipcode_00410', 'zipcode_00411', 'zipcode_00416', 'zipcode_00420', 'zipcode_00422', 'zipcode_00431', 'zipcode_00432', 'zipcode_00434', 'zipcode_00440', 'zipcode_00443', 'zipcode_00450', 'zipcode_00454', 'zipcode_00455', 'zipcode_00460', 'zipcode_00465', 'zipcode_00470', 'zipcode_00480', 'zipcode_00487', 'zipcode_00491', 'zipcode_00500', 'zipcode_00510', 'zipcode_00515', 'zipcode_00519', 'zipcode_00520', 'zipcode_00523', 'zipcode_00550', 'zipcode_00560', 'zipcode_00583', 'zipcode_00600', 'zipcode_00640', 'zipcode_00680', 'zipcode_00711', 'zipcode_00720', 'zipcode_00760', 'zipcode_00840', 'zipcode_00928', 'zipcode_00980', 'zipcode_01160', 'zipcode_02000', 'zipcode_?', 'approved_+', 'approved_-']\n\n\nAs before, we split the sample into a test set and a train set with 20% of data being in the test dataset. Your random seed should be 808. This needs to be done after the ‚Äúget_dummies()‚Äù command for decision tree to work.\n\ndata_features = data1.loc[:,\"age\":\"citizen_s\"]\ndata_target = data1['approved_+']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data_features,\n                                                    data_target,\n                                                    test_size = 0.2,\n                                                    random_state = 808)\n\n\nprint(x_train, x_test, y_train, y_test)\n\n       age    debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n27   56.58  18.500         15.000           17       0    0    0    1   \n100  37.50   1.750          0.250            0     400    0    0    1   \n132  47.42   8.000          6.500            6   51100    0    1    0   \n404  34.00   5.085          1.085            0       0    0    0    1   \n401  28.92   0.375          0.290            0     140    0    0    1   \n..     ...     ...            ...          ...     ...  ...  ...  ...   \n480  16.92   0.500          0.165            6      35    0    1    0   \n384  22.08  11.460          1.585            0    1212    0    0    1   \n300  57.58   2.000          6.500            1      10    0    1    0   \n249  21.83  11.000          0.290            6       0    0    0    1   \n471  21.08   4.125          0.040            0     100    0    0    1   \n\n     married_?  married_l  ...  ethnicity_z  priordefault_f  priordefault_t  \\\n27           0          0  ...            0               0               1   \n100          0          0  ...            0               0               1   \n132          0          0  ...            0               0               1   \n404          0          0  ...            0               1               0   \n401          0          0  ...            0               1               0   \n..         ...        ...  ...          ...             ...             ...   \n480          0          0  ...            0               1               0   \n384          0          0  ...            0               1               0   \n300          0          0  ...            0               1               0   \n249          0          0  ...            0               0               1   \n471          0          0  ...            0               1               0   \n\n     employed_f  employed_t  driverslicense_f  driverslicense_t  citizen_g  \\\n27            0           1                 0                 1          1   \n100           1           0                 0                 1          1   \n132           0           1                 1                 0          1   \n404           1           0                 0                 1          1   \n401           1           0                 1                 0          1   \n..          ...         ...               ...               ...        ...   \n480           0           1                 0                 1          1   \n384           1           0                 0                 1          1   \n300           0           1                 1                 0          1   \n249           0           1                 1                 0          1   \n471           1           0                 1                 0          1   \n\n     citizen_p  citizen_s  \n27           0          0  \n100          0          0  \n132          0          0  \n404          0          0  \n401          0          0  \n..         ...        ...  \n480          0          0  \n384          0          0  \n300          0          0  \n249          0          0  \n471          0          0  \n\n[542 rows x 50 columns]        age   debt  yearsemployed  creditscore  income  0_?  0_a  0_b  \\\n207  28.67  9.335          5.665            6     168    0    0    1   \n406  40.33  8.125          0.165            2      18    0    1    0   \n231  47.42  3.000         13.875            2    1704    0    1    0   \n452  36.50  4.250          3.500            0      50    0    0    1   \n567  25.17  2.875          0.875            0       0    0    1    0   \n..     ...    ...            ...          ...     ...  ...  ...  ...   \n457  29.67  0.750          0.040            0       0    0    0    1   \n544  30.08  1.040          0.500           10      28    0    0    1   \n145  32.83  2.500          2.750            6    2072    0    0    1   \n342  26.92  2.250          0.500            0    4000    0    0    1   \n323  48.58  0.205          0.250           11    2732    0    0    1   \n\n     married_?  married_l  ...  ethnicity_z  priordefault_f  priordefault_t  \\\n207          0          0  ...            0               0               1   \n406          0          0  ...            0               1               0   \n231          0          0  ...            0               0               1   \n452          0          0  ...            0               1               0   \n567          0          0  ...            0               0               1   \n..         ...        ...  ...          ...             ...             ...   \n457          0          0  ...            0               1               0   \n544          0          0  ...            0               0               1   \n145          0          0  ...            0               0               1   \n342          0          0  ...            0               1               0   \n323          0          0  ...            0               0               1   \n\n     employed_f  employed_t  driverslicense_f  driverslicense_t  citizen_g  \\\n207           0           1                 1                 0          1   \n406           0           1                 1                 0          1   \n231           0           1                 0                 1          1   \n452           1           0                 1                 0          1   \n567           1           0                 1                 0          1   \n..          ...         ...               ...               ...        ...   \n457           1           0                 1                 0          1   \n544           0           1                 0                 1          1   \n145           0           1                 1                 0          1   \n342           1           0                 0                 1          1   \n323           0           1                 1                 0          1   \n\n     citizen_p  citizen_s  \n207          0          0  \n406          0          0  \n231          0          0  \n452          0          0  \n567          0          0  \n..         ...        ...  \n457          0          0  \n544          0          0  \n145          0          0  \n342          0          0  \n323          0          0  \n\n[136 rows x 50 columns] 27     1\n100    0\n132    1\n404    0\n401    0\n      ..\n480    0\n384    0\n300    0\n249    1\n471    0\nName: approved_+, Length: 542, dtype: uint8 207    1\n406    0\n231    1\n452    0\n567    1\n      ..\n457    0\n544    0\n145    1\n342    0\n323    1\nName: approved_+, Length: 136, dtype: uint8\n\n\n\ndata_features.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 678 entries, 0 to 689\nData columns (total 50 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   age                678 non-null    float64\n 1   debt               678 non-null    float64\n 2   yearsemployed      678 non-null    float64\n 3   creditscore        678 non-null    int64  \n 4   income             678 non-null    int64  \n 5   0_?                678 non-null    uint8  \n 6   0_a                678 non-null    uint8  \n 7   0_b                678 non-null    uint8  \n 8   married_?          678 non-null    uint8  \n 9   married_l          678 non-null    uint8  \n 10  married_u          678 non-null    uint8  \n 11  married_y          678 non-null    uint8  \n 12  bankcustomer_?     678 non-null    uint8  \n 13  bankcustomer_g     678 non-null    uint8  \n 14  bankcustomer_gg    678 non-null    uint8  \n 15  bankcustomer_p     678 non-null    uint8  \n 16  educationlevel_?   678 non-null    uint8  \n 17  educationlevel_aa  678 non-null    uint8  \n 18  educationlevel_c   678 non-null    uint8  \n 19  educationlevel_cc  678 non-null    uint8  \n 20  educationlevel_d   678 non-null    uint8  \n 21  educationlevel_e   678 non-null    uint8  \n 22  educationlevel_ff  678 non-null    uint8  \n 23  educationlevel_i   678 non-null    uint8  \n 24  educationlevel_j   678 non-null    uint8  \n 25  educationlevel_k   678 non-null    uint8  \n 26  educationlevel_m   678 non-null    uint8  \n 27  educationlevel_q   678 non-null    uint8  \n 28  educationlevel_r   678 non-null    uint8  \n 29  educationlevel_w   678 non-null    uint8  \n 30  educationlevel_x   678 non-null    uint8  \n 31  ethnicity_?        678 non-null    uint8  \n 32  ethnicity_bb       678 non-null    uint8  \n 33  ethnicity_dd       678 non-null    uint8  \n 34  ethnicity_ff       678 non-null    uint8  \n 35  ethnicity_h        678 non-null    uint8  \n 36  ethnicity_j        678 non-null    uint8  \n 37  ethnicity_n        678 non-null    uint8  \n 38  ethnicity_o        678 non-null    uint8  \n 39  ethnicity_v        678 non-null    uint8  \n 40  ethnicity_z        678 non-null    uint8  \n 41  priordefault_f     678 non-null    uint8  \n 42  priordefault_t     678 non-null    uint8  \n 43  employed_f         678 non-null    uint8  \n 44  employed_t         678 non-null    uint8  \n 45  driverslicense_f   678 non-null    uint8  \n 46  driverslicense_t   678 non-null    uint8  \n 47  citizen_g          678 non-null    uint8  \n 48  citizen_p          678 non-null    uint8  \n 49  citizen_s          678 non-null    uint8  \ndtypes: float64(3), int64(2), uint8(45)\nmemory usage: 61.6 KB\n\n\n\ndata1.head()\n\n\n\n\n\n  \n    \n      \n      age\n      debt\n      yearsemployed\n      creditscore\n      income\n      0_?\n      0_a\n      0_b\n      married_?\n      married_l\n      ...\n      zipcode_00720\n      zipcode_00760\n      zipcode_00840\n      zipcode_00928\n      zipcode_00980\n      zipcode_01160\n      zipcode_02000\n      zipcode_?\n      approved_+\n      approved_-\n    \n  \n  \n    \n      0\n      30.83\n      0.000\n      1.25\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      58.67\n      4.460\n      3.04\n      6\n      560\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      2\n      24.50\n      0.500\n      1.50\n      0\n      824\n      0\n      1\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      3\n      27.83\n      1.540\n      3.75\n      5\n      3\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      4\n      20.17\n      5.625\n      1.71\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n  \n\n5 rows √ó 223 columns\n\n\n\n#a. Decision Trees\n\nfrom sklearn.tree import DecisionTreeClassifier\ninit_model = DecisionTreeClassifier ()\nfitted_model = init_model.fit(x_train ,y_train)\ntest_predictions = fitted_model.predict(x_test)\naccuracy_score = fitted_model.score(x_test,y_test)\nprint(accuracy_score)\n\n#overall how many of the predictions are correct\n\n\n#overall out of those defaults how many of those 0.17 are correct and not correct?\n\n0.8455882352941176\n\n\n#b. Logistic Regression\n\n#Logistic regression\nmodel_lr = LogisticRegression()\nfitted_model_lr = model_lr.fit(x_train, y_train)\ntest_predictions_lr = fitted_model_lr.predict(x_test)\naccuracy_lr = fitted_model_lr.score(x_test,y_test)\nprint(fitted_model_lr.coef_)\nprint(accuracy_lr)\n\n#overall how many of the predictions are correct\n\n[[ 4.84069824e-03 -2.46678805e-02  1.20267584e-01  1.64279116e-01\n   5.13339863e-04 -7.02796069e-03 -1.76746656e-01 -2.76489739e-02\n   1.08863388e-01  3.10703871e-02 -8.26650419e-02 -2.68692324e-01\n   1.08863388e-01 -8.26650419e-02  3.10703871e-02 -2.68692324e-01\n   1.08863388e-01 -9.05194758e-02 -9.94068100e-02  1.87520056e-01\n  -2.93554456e-02  3.49157435e-02 -3.61569082e-01 -2.52229191e-01\n  -9.00501443e-04 -2.10370142e-01 -8.76721486e-02  1.76003560e-01\n  -5.84801526e-03  1.76604192e-01  2.42540281e-01  1.08863388e-01\n  -1.29883594e-01 -2.29978369e-02 -3.51268838e-01  1.80746065e-01\n   5.79890564e-02  1.89933137e-02 -1.73749301e-08 -9.58931264e-02\n   2.20279985e-02 -1.72796178e+00  1.51653819e+00 -3.04359173e-01\n   9.29355818e-02 -1.62785055e-03 -2.09795740e-01 -2.90778257e-01\n   9.97707085e-02 -2.04160423e-02]]\n0.8602941176470589\n\n\n/Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n#c.¬†Bagging\n\n#bagging, need to explore the concept and what each parameter does\nfrom sklearn.ensemble import BaggingClassifier\n\n#n_estimators is a number of randomly sampled datasets, similar to cv\nmodel_bag = BaggingClassifier(\n    base_estimator = tree.DecisionTreeClassifier(),\n    n_estimators = 400,\n    max_samples = 0.8,\n    oob_score = True,\n    random_state = 808)\n\nfitted_model_bag = model_bag.fit(x_train,y_train)\ntest_predictions_bag = fitted_model_bag.predict(x_test)\naccuracy_bag = fitted_model_bag.score(x_test,y_test)#validation accuracy, because I want to see if the model works generalize in new data\nprint(accuracy_bag)\nprint(model_bag.oob_score_)\n\n0.8823529411764706\n0.8616236162361623\n\n\n#d.¬†Boosting\n\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\nmodel_boost = GradientBoostingClassifier()\n\nfitted_model_boost = model_boost.fit(x_train,y_train)\ntest_predictions_boost = fitted_model_boost.predict(x_test)\naccuracy_boost = fitted_model_boost.score(x_test,y_test)\nprint(accuracy_boost)\nprint(model_boost)\n\n0.8529411764705882\nGradientBoostingClassifier()\n\n\n#e. Random Forest\n\nrandomforest = RandomForestClassifier (random_state = 808)\nmodel_randforest = RandomForestClassifier()\n\nfitted_model_randforest = model_randforest.fit(x_train,y_train)\ntest_predictions_randforest = fitted_model_randforest.predict(x_test)\naccuracy_randforest = fitted_model_randforest.score(x_test,y_test)\nprint(accuracy_randforest)\n\n0.8382352941176471\n\n\n#f.¬†SVM\n\nfrom sklearn.svm import SVC #i.e. Support Vector Classifier\nsvm=svm.SVC(random_state = 808)\n\nprint(svm.fit(x_train, y_train).score(x_test,y_test))\n\n0.6985294117647058\n\n\n#g. Passive Aggressive Classifier (Links to an external site.)\n\npa=PassiveAggressiveClassifier(random_state = 808)\n\nprint(pa.fit(x_train, y_train).score(x_test,y_test))\n\n0.7573529411764706\n\n\n#h. Radius Neighbors Classifier (Links to an external site.)\n\nmodel_rn = RadiusNeighborsClassifier(radius=11700)\nfitted_model_rn = model_rn.fit(x_train,y_train)\ntest_predictions_rn = fitted_model_rn.predict(x_test)\naccuracy_rn = fitted_model_rn.score(x_test,y_test)\nprint(accuracy_rn)\n\n0.6470588235294118\n\n\n#5. After completing Step 4, explain: # Which algorithm you recommend? What accuracy it has? Why you measured accuracy the way you did? The accuracy of the algorithms above indicate the percentage of predictions that are actually correct. I would reccommend the bagging algorithm because it has the highest accuracy percentage (88,23%).\nThe confusion matrix below indicates that Bagging is the most suitable approach in reaching to the accuracy of the algorithms. In particular, the confusion matrix above shows us that the Bagging models have a higher number of true positives and true negatives than rest accuracy models. In this example above, we can see that bagging is compared with logistic regression and random forest classifiers, the 2nd and the 3rd highest accuracy models.\n\nfrom sklearn.metrics import confusion_matrix\n\nprint('Confusion matrix: Bagging Classifiers')\nprint(confusion_matrix(y_test, test_predictions_bag))\n\nprint('Confusion matrix: Logistic Regression')\nprint(confusion_matrix(y_test, test_predictions_lr))\n\nprint('Confusion matrix: Random Forest Classifiers')\nprint(confusion_matrix(y_test, test_predictions_randforest))\n\nConfusion matrix: Bagging Classifiers\n[[75 12]\n [ 4 45]]\nConfusion matrix: Logistic Regression\n[[70 17]\n [ 2 47]]\nConfusion matrix: Random Forest Classifiers\n[[70 17]\n [ 5 44]]\n\n\n\n6. Brief overview of the last two classifiers (Passive Agressive & Radius Neighbors).\nPassive Agressive classifier is an online learning predictor best suited for systems that receive data in a continuous stream. The calculation passively corrects for the classifications and penalizes ‚Äòaggressive‚Äô for any miscalculation. While the model can perfectly predict all data, it will not change the algorithm; hence it is called passive. The term aggressive referst to the fact that when the model fails to predict the outcome variable just in the slightest, it will change the algorithm to compensate for the failed prediction for every set of a new sample of data.\nRadius Neighbors classifer is similar to the KNN (k-nearest-neighbours) concept and makes predictions based on the data within a radius. Instead of locating the k-neighbors, the Radius Neighbors Classifier locates all examples in the dataset that are within a given radius of the new example. The radius neighbors are then used to make a prediction for the new example. The radius is defined in the feature space and generally assumes that the input variables are numeric and scaled to the range 0-1, e.g.¬†normalized. The radius-based approach to locating neighbors is appropriate for those datasets where it is desirable for the contribution of neighbors to be proportional to the density of examples in the feature space."
  },
  {
    "objectID": "software/titanic.html",
    "href": "software/titanic.html",
    "title": "Titanic Dataset",
    "section": "",
    "text": "Let's work with the titanic dataset.\n1. Construct a table showing the distribution of passengers by class and survival.\n\ntitanic %$% table(survived, pclass)\n\n        pclass\nsurvived   1   2   3\n       0 123 158 528\n       1 200 119 181\n\n# magrittr does this --> table(titanic$survived, titanic$pclass)\n\n2. Construct a logistic regression model that links survival to the passenger class. Write out the equation first without Running it in R. HINT: Class is a factor variable\n\\[\nlog(odds(y)) = Œ≤0 + Œ≤1*pclass2 + Œ≤2*plcass3\n\\]\n\\[\nodds(y) = e^{Œ≤_0} * e^{Œ≤_1 * pclass2} * e^{Œ≤_2 * pclass3}\n\\]\nŒ≤0 = intercept\ny = survival\n3. Using hand-calculations, determine the coefficients in the model and interpret them (HINT: all you need to do is to use the table, calculate odds for the default category and the odds-ratios for the other categories versus the default)\ne.g.¬†not survival is the default exercise (i.e.¬†=0). prob of survival of:\nfirst class => 200/(200+123) = 62% of people survived\nsecond class => 119/(119+158) = 43% of people survived\nthird class => 181/(181+528) = 26% of people survived\nProbabilities vary between zero and one. Instead, for the purpose of the logistical regression we can calculate odds of survival of:\nfirst class =>200/123=1.62 => for each first class person who died, 1.62 first class people survived.\nsecond class =>119/158 = 0.75 => for each second class person who died, 0.75 second class person survived.\nthird class =>181/528 = 0.34 => for each third class person who died, 0.34 third class person survived.\nWhen we take the log of odds, the result varies from - infinity to + infinity. log of survival of:\nfirst class people: log(200/123)=0.486\nsecond class people: log(119/158)=-0.28\nthird class people: log(181/528)=-1.07\n4. Now Run the model in R. Confirm that you got the same results as in part c). Interpret the results and talk about significance (both statistical and substantive).\n\ntitanic %$% summary(glm(survived ~ factor(pclass), family = \"binomial\"))\n\n\nCall:\nglm(formula = survived ~ factor(pclass), family = \"binomial\")\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3896  -0.7678  -0.7678   0.9791   1.6525  \n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)       0.4861     0.1146   4.242 2.21e-05 ***\nfactor(pclass)2  -0.7696     0.1669  -4.611 4.02e-06 ***\nfactor(pclass)3  -1.5567     0.1433 -10.860  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1741.0  on 1308  degrees of freedom\nResidual deviance: 1613.3  on 1306  degrees of freedom\nAIC: 1619.3\n\nNumber of Fisher Scoring iterations: 4\n\nquestion2 <- titanic %$% glm(survived ~ factor(pclass), family = \"binomial\")\nexp(question2$coefficients)\n\n    (Intercept) factor(pclass)2 factor(pclass)3 \n      1.6260163       0.4631962       0.2108239 \n\n\nThis code leaves us with the following equation of log odds:\nSurvived = 0.486 -0.77class2 -1.56class3\nThe significant p-values are telling us that the intercept is not zero. It means that the beta one (Œ≤1) is different from zero. This implies that there is a difference between classes and survival. We do not know yet what is the magnitude of the difference because this is a difference in log odds.\nSince is negative it means that classes 2 and 3 have lower survival chances from the first class. The 3rd class has the lowest chances of survival since its Œ≤1 coefficient has the highest negative number.\n5. What's the probability of survival for each class of passengers?\nFirst class had the odds of survival of 1.62. We can calculate the probability of survival as (1.62)/(1 + 1.62) = 0.6183206 or about 61.8%.\nSecond class had the odds of survival of 0.75. We can calculate the probability of survival as (0.75)/(1 + 0.75) = 0.4285714 or about 42.8%\nThird class had the odds of survival of 0.34. We can calculate the probability of survival as (0.34)/(1 + 0.34) = 0.2537313 or about 25.3%\n6. Construct a model that interacts class of passenger and his/her gender. Interpret the results the same way you did before.\nFirst we conduct the model just like as before but this time we add gender (aka ‚Äòsex‚Äô) as a factor variable. This time though we either add or multiply the one variable with the other. This model would help us make predictions for a passenger.\nBecause here the model assumes interaction between class and gender we construct the model with a ‚Äò*‚Äô sign. If we add a ‚Äò*‚Äô sign we assume that there are interactions; meaning that the effect of class depends on gender OR the effect of gender depends on class.\n‚ÄîThe alternative was the ‚Äò+‚Äô sign. If we add a ‚Äò+‚Äô sign we assume that there are no interactions; meaning that the effect of class does not depend on gender. This was our assumption when we were doing linear models.‚Äî\n\nsummary(glm(survived~factor(pclass) * factor(sex), family = \"binomial\", data = titanic))\n\n\nCall:\nglm(formula = survived ~ factor(pclass) * factor(sex), family = \"binomial\", \n    data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5924  -0.5745  -0.5745   0.4902   1.9610  \n\nCoefficients:\n                                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                       3.3250     0.4549   7.309 2.68e-13 ***\nfactor(pclass)2                  -1.2666     0.5485  -2.309   0.0209 *  \nfactor(pclass)3                  -3.3621     0.4748  -7.081 1.43e-12 ***\nfactor(sex)male                  -3.9848     0.4815  -8.277  < 2e-16 ***\nfactor(pclass)2:factor(sex)male   0.1617     0.6104   0.265   0.7911    \nfactor(pclass)3:factor(sex)male   2.3039     0.5158   4.467 7.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1741  on 1308  degrees of freedom\nResidual deviance: 1210  on 1303  degrees of freedom\nAIC: 1222\n\nNumber of Fisher Scoring iterations: 5\n\n\nLet‚Äôs make an assumption for a male sitting in first class.\nlog odds of survival => 3.325 - 3.9848 = -0.6598\nodds of survival => exp(-0.6598) = 0.5169547\nprobability of survival => 0.5169547/(1+0.5169547) = 0.3407845 or 34%\nLet‚Äôs make an assumption for a male sitting in second class.\nlog odds of survival => 3.325 -1.2666 - 3.9848+0.1617 = -1.7647\nodds of survival => exp(-1.7647) = 0.1712382\nprobability of survival => 0.1712382/(1+0.1712382) = 0.1462027 or 14.6%\nLet‚Äôs make an assumption for a male sitting in third class.\nlog odds of survival => 3.325 -3.3621 - 3.9848 +2.3039 = -1.718\nodds of survival => exp(-1.718) = 0.1794246\nprobability of survival => 0.1794246/(1+0.1794246) = 0.1521289 or 15.21%\nLet‚Äôs make an assumption for a female sitting in first class.\nlog odds of survival => 3.325\nodds of survival => exp(3.325) = 27.799\nprobability of survival => 27.799/(1+27.799) = 0.9652766 or 96.5%\nLet‚Äôs make an assumption for a female sitting in second class.\nlog odds of survival => 3.325 -1.2666 = 2.0584\nodds of survival => exp(2.0584) = 7.833426\nprobability of survival => 7.833426/(1+7.833426) = 0.8867936 or 88.6%\nLet‚Äôs make an assumption for a female sitting in third class.\nlog odds of survival => 3.325 -3.3621 = -0.0371\nodds of survival => exp(-0.0371) = 0.9635798\nprobability of survival => 0.9635798/(1+0.9635798) = 0.4907261 or 49%\nInterpretation of results\nIt looks like the closer someone is in the first class, the better the chance to survive. However, there is an interesting result between men of third and second category, where the men in third category had slightly higher chances of survival than the men in second class. In general, females had higher chances of survival. Even the females in the third class had 15% higher chance of survival than the males in the first class (49% vs 34%). Around 9 out of 10 women survived in classes 1 and 2. One out of two women survived out of the third class. Only one out of three men in the first class survived while in classes 2 and 3 men had the lowest chances of survival (14.6% and 15.2%). This slight difference though can also be due to random noise.\nHow we arrived to those results:\nAfter running the logistical regression model we come up with the intercept and coefficient number odds which where in log form; hence we had to convert those in actual odds. Before converting them to odds, we sum up the related variables that we want to find in order to also have the log odds of survival (e.g.¬†for female in second class we used the intercept 3.325 and the pclass2 value -1.2666).\nThen, we exponentiate the log odds of survival to find the actual odds of survival (e.g.¬†female in second class = 7.83 ‚Äî which means each female who died, 7.83 survived).\nBecause human brain is not well designed enough to perceive odds as a measurement, we converted those to probability using the formula:\n\\[\nprobability = odds/1+odds\n\\]\nWe did that for each one of the six outcomes\n\nmale 1st class = 34%\nmale 2nd class = 14.6%\nmale 3rd class = 15.2%\nfemale 1st class = 96.5%\nfemale 2nd class = 88.6%\nfemale 3rd class = 49%"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html",
    "href": "software/Analysing_Spotify_Data_in_SQL.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "This assignment focuses on helping you get to grips with a new tool: SQL.\nThrough this assignment, we will be working with SQL (specifically pandasql) by exploring a Spotify dataset containing song reviews and statistics. At parts 3 and 4 you can see some text analysis of song reviews.\n\n\n\n\n%pip install pandasql\n%pip3 install pandas\n%pip3 install pandasql\n%pip3 install nltk\n%pip3 install wordcloud\n%pip3 install seaborn\n\n1870.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nRequirement already satisfied: pandasql in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.7.3)\nRequirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandasql) (1.5.1)\nRequirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandasql) (1.4.42)\nRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandasql) (1.23.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->pandasql) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pandas->pandasql) (2022.5)\nRequirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->pandasql) (1.16.0)\nRequirement already satisfied: greenlet!=0.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sqlalchemy->pandasql) (1.1.3.post0)\nWARNING: You are using pip version 21.2.3; however, version 22.3 is available.\nYou should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n\n\nUsageError: Line magic function `%pip3` not found.\n\n\n\nimport pandas as pd\nimport datetime as dt\nimport pandasql as ps \nimport nltk\nnltk.download('punkt')\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt \n\n[nltk_data] Downloading package punkt to /Users/loizoskon/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n# Three datasets that I am using (you can download them from here if the code does not work for you put them in the working directory)\n! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_features.csv\n! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_songs.csv\n! wget -nc https://storage.googleapis.com/penn-cis5450/spotify_rankings.csv\n\n1877.21s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nzsh:1: command not found: wget\n\n\n1882.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nzsh:1: command not found: wget\n\n\n1888.20s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n\n\nzsh:1: command not found: wget\n\n\n\nprint(pd.__version__ )\n\n1.5.1"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-1-loading-processing-our-datasets",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-1-loading-processing-our-datasets",
    "title": "Loizos Konstantinou",
    "section": "Part 1: Loading & Processing our Datasets",
    "text": "Part 1: Loading & Processing our Datasets\nBefore we get into the data, we first need to load and clean our datasets."
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-2-exploring-the-data-with-pandassql-and-pandas",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-2-exploring-the-data-with-pandassql-and-pandas",
    "title": "Loizos Konstantinou",
    "section": "Part 2: Exploring the Data with PandasSQL (and Pandas)",
    "text": "Part 2: Exploring the Data with PandasSQL (and Pandas)\nNow that we‚Äôre more familiar with the dataset, we‚Äôll now go in SQL language. Specifically, I‚Äôll be using pandasql\nThe typical flow of using pandasql (shortened to ps) is as follows: 1. Write a SQL query in the form of a string (Tip: use triple quotes ‚Äú‚Äú‚Äúx‚Äù‚Äú‚Äù to write multi-line strings) 2. Run the query using ps.sqldf(your_query, locals())\nPandaSQL is convenient as it allows you to reference the dataframes that are currently defined in your notebook, so you will be able to fully utilize the dataframes songs_df, rankings_df and features_df that you have created above!\n###2.1 Bruno Mars songs\n\n2.1.1 How many Bruno Mars‚Äô songs were popular in 2017?\nThe dataframe songs_df contains all top songs in 2017. We want to know whether Bruno Mars was a part of it (he obviously was) - but which of his songs made it to the top?\nUsing pandas, let‚Äôs filter out the songs and reviews from songs_df that were by Bruno Mars. Then, let‚Äôs save this data to a DataFrame called bruno_df that has the following schema:\n\n\n\n\nname\nreviews\n\n\n\n\n\n\n\n#Using pandas to obtain songs by `Bruno Mars`\n# bruno_df = songs_df.groupby('artists' == 'Bruno Mars')\nbruno_df = songs_df[songs_df['artists'] == 'Bruno Mars']\n# bruno_df = songs_df.groupby() #how to get b mars\nbruno_df = bruno_df[['name', 'reviews']]\nbruno_df\n\n\n\n\n\n  \n    \n      \n      name\n      reviews\n    \n  \n  \n    \n      8\n      That's What I Like\n      You have a really cool voice! I like the way y...\n    \n    \n      59\n      24K Magic\n      Lyrics are very good. Backing sounds nice.\n    \n  \n\n\n\n\n\n\n2.1.2 How many of Bruno Mars‚Äô songs were deemed ‚Äúgood‚Äù?\nWe now want to see which of these songs contained the word ‚Äúgood‚Äù in the reviews column.\nHere we update bruno_df so that it only contains songs that have the word ‚Äògood‚Äô in the reviews column.\n\n#Using pandasql to obtain only \"good\" songs of bruno mars\ngood_song_query = \"\"\"SELECT * FROM bruno_df WHERE reviews LIKE '%good%'\"\"\" # syntax after like == wildcards - look at the string and ignore everything before% and after% 'good'\nbruno_df = ps.sqldf(good_song_query, locals())\nbruno_df\n\n\n\n\n\n  \n    \n      \n      name\n      reviews\n    \n  \n  \n    \n      0\n      24K Magic\n      Lyrics are very good. Backing sounds nice.\n    \n  \n\n\n\n\n###2.2 You can seeing the hit songs\n\n\n2.2.1 Extract the total no. of streams\nWe now want to see what songs formed the top 75% of the year 2017 from rankings_df. We can measure the popularity of the songs using the total number of streams the song received.\n\nYou can see the total number of streams per song and save it into a dataframe called streams_df\nUnderstand the quartile ranges in streams_df\n\n\nrankings_df.describe()\n# we use `.describe()` to understand quartiles. It would be helpful to save the necessary quartile value to use in the querying section that follows.\n\n\n\n\n\n  \n    \n      \n      Position\n      Streams\n    \n  \n  \n    \n      count\n      3.440540e+06\n      3.440540e+06\n    \n    \n      mean\n      9.465220e+01\n      5.188452e+04\n    \n    \n      std\n      5.739412e+01\n      2.017733e+05\n    \n    \n      min\n      1.000000e+00\n      1.001000e+03\n    \n    \n      25%\n      4.500000e+01\n      3.321000e+03\n    \n    \n      50%\n      9.200000e+01\n      9.226000e+03\n    \n    \n      75%\n      1.430000e+02\n      2.965600e+04\n    \n    \n      max\n      2.000000e+02\n      1.138152e+07\n    \n  \n\n\n\n\n\nrankings_df\n\n\n\n\n\n  \n    \n      \n      Position\n      Track Name\n      Artist\n      Streams\n      Date\n      Region\n      ID\n    \n  \n  \n    \n      0\n      1\n      Reggaet√≥n Lento (Bailemos)\n      CNCO\n      19272\n      2017-01-01\n      ec\n      3AEZUABDXNtecAOSC1qTf\n    \n    \n      2658317\n      118\n      Steady 1234 (feat. Jasmine Thompson & Skizzy M...\n      Vice\n      15142\n      2017-01-01\n      nl\n      40UroIGvsMPLPBYwH8rMN\n    \n    \n      2658316\n      117\n      Dynamite (feat. Pretty Sister)\n      Nause\n      15152\n      2017-01-01\n      nl\n      2Ae5awwKvQpTBKQHr1TYC\n    \n    \n      2658315\n      116\n      Hello\n      Adele\n      15170\n      2017-01-01\n      nl\n      4sPmO7WMQUAf45kwMOtON\n    \n    \n      2658314\n      115\n      One Night Stand\n      B-Brave\n      15510\n      2017-01-01\n      nl\n      2no9x9FRytP9PnB3CQPYS\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1498830\n      109\n      Sky Walker\n      Miguel\n      1581\n      2018-01-09\n      hu\n      5WoaF1B5XIEnWfmb5NZik\n    \n    \n      1498829\n      108\n      Back to You (feat. Bebe Rexha & Digital Farm A...\n      Louis Tomlinson\n      1582\n      2018-01-09\n      hu\n      7F9vK8hNFMml4GtHsaXui\n    \n    \n      1498828\n      107\n      Betrayed\n      Lil Xan\n      1582\n      2018-01-09\n      hu\n      6NWl2m8asvH83xjuXVNsu\n    \n    \n      1498841\n      120\n      Rockabye (feat. Sean Paul & Anne-Marie)\n      Clean Bandit\n      1490\n      2018-01-09\n      hu\n      5knuzwU65gJK7IF5yJsua\n    \n    \n      3441196\n      200\n      Let Her Go\n      Passenger\n      2088\n      2018-01-09\n      hk\n      2jyjhRf6DVbMPU5zxagN2\n    \n  \n\n3440540 rows √ó 7 columns\n\n\n\n\nstreams_df = rankings_df.groupby(by = 'Track Name').sum()\nstreams_df\n# Using pandas extract the total number of streams per song from rankings_df\n# streams_df = rankings_df['Streams'>2.965800e+04]\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/1744104746.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  streams_df = rankings_df.groupby(by = 'Track Name').sum()\n\n\n\n\n\n\n  \n    \n      \n      Position\n      Streams\n    \n    \n      Track Name\n      \n      \n    \n  \n  \n    \n      \"All That Is or Ever Was or Ever Will Be\"\n      383\n      7311\n    \n    \n      \"Read All About It, Pt. III\"\n      2306\n      57025\n    \n    \n      #99\n      1126\n      31826\n    \n    \n      #Askip\n      3184\n      296862\n    \n    \n      #Biziz - feat. Lil Bege\n      4488\n      403591\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Ï†ÑÏïº ÂâçÂ§ú The Eve\n      17063\n      976392\n    \n    \n      ÏßÄÎÇòÍ∞à ÌÖåÎãà Been Through\n      1180\n      87456\n    \n    \n      Ï©îÏñ¥ DOPE\n      7545\n      209493\n    \n    \n      Ìîº ÎïÄ ÎààÎ¨º\n      2757\n      68673\n    \n    \n      ÌîºÏπ¥Î∂Ä Peek-A-Boo\n      24895\n      2078217\n    \n  \n\n18597 rows √ó 2 columns\n\n\n\n\nstreams_df.describe()\n\n\n\n\n\n  \n    \n      \n      Position\n      Streams\n    \n  \n  \n    \n      count\n      1.859700e+04\n      1.859700e+04\n    \n    \n      mean\n      1.751114e+04\n      9.598901e+06\n    \n    \n      std\n      7.505320e+04\n      6.903221e+07\n    \n    \n      min\n      1.000000e+01\n      1.001000e+03\n    \n    \n      25%\n      3.200000e+02\n      2.133400e+04\n    \n    \n      50%\n      1.362000e+03\n      1.225160e+05\n    \n    \n      75%\n      7.620000e+03\n      1.072171e+06\n    \n    \n      max\n      1.604987e+06\n      2.993989e+09\n    \n  \n\n\n\n\n\n\n2.2.2 Top 75% of streams\nNow that we‚Äôve seen the distribution of the streams, we‚Äôd like to extract songs with streams within the top 75%.\nWe filter out songs from streams_df whose stream count is in the top 75%, then save this data as the Pandas dataframe pd_top_streams.\n\nstreams_df\n\n\n\n\n\n  \n    \n      \n      Position\n      Streams\n    \n    \n      Track Name\n      \n      \n    \n  \n  \n    \n      \"All That Is or Ever Was or Ever Will Be\"\n      383\n      7311\n    \n    \n      \"Read All About It, Pt. III\"\n      2306\n      57025\n    \n    \n      #99\n      1126\n      31826\n    \n    \n      #Askip\n      3184\n      296862\n    \n    \n      #Biziz - feat. Lil Bege\n      4488\n      403591\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Ï†ÑÏïº ÂâçÂ§ú The Eve\n      17063\n      976392\n    \n    \n      ÏßÄÎÇòÍ∞à ÌÖåÎãà Been Through\n      1180\n      87456\n    \n    \n      Ï©îÏñ¥ DOPE\n      7545\n      209493\n    \n    \n      Ìîº ÎïÄ ÎààÎ¨º\n      2757\n      68673\n    \n    \n      ÌîºÏπ¥Î∂Ä Peek-A-Boo\n      24895\n      2078217\n    \n  \n\n18597 rows √ó 2 columns\n\n\n\n\n#Using pandasql extract the top 75% based on number of streams\ntop_query = \"\"\"\nSELECT * \nFROM streams_df\nWHERE Streams > 21334\n\"\"\"\n\nsql_top_streams = ps.sqldf(top_query, locals())\nsql_top_streams\n\n\n\n\n\n  \n    \n      \n      Track Name\n      Position\n      Streams\n    \n  \n  \n    \n      0\n      \"Read All About It, Pt. III\"\n      2306\n      57025\n    \n    \n      1\n      #99\n      1126\n      31826\n    \n    \n      2\n      #Askip\n      3184\n      296862\n    \n    \n      3\n      #Biziz - feat. Lil Bege\n      4488\n      403591\n    \n    \n      4\n      #CTZK\n      20515\n      669563\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      13941\n      Ï†ÑÏïº ÂâçÂ§ú The Eve\n      17063\n      976392\n    \n    \n      13942\n      ÏßÄÎÇòÍ∞à ÌÖåÎãà Been Through\n      1180\n      87456\n    \n    \n      13943\n      Ï©îÏñ¥ DOPE\n      7545\n      209493\n    \n    \n      13944\n      Ìîº ÎïÄ ÎààÎ¨º\n      2757\n      68673\n    \n    \n      13945\n      ÌîºÏπ¥Î∂Ä Peek-A-Boo\n      24895\n      2078217\n    \n  \n\n13946 rows √ó 3 columns\n\n\n\n\n\n2.3 Duration of songs\nNow that we know which songs are hits, we‚Äôd like to listen to songs that are not too short nor too long.\nUsing pandas, we filter out songs from songs_df whose duration is between 3 and 5 minutes.\n\nCreating a new column in songs_df called ‚Äúduration_min‚Äù that converts the duration in ‚Äúduration_ms‚Äù from milliseconds to minutes\nExtracting only songs whose duration is at least 3 minutes and at most 5 minutes. Then, saving the output to ideal_songs_df.\n\n\nsongs_df\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      artists\n      duration_ms\n      reviews\n    \n  \n  \n    \n      0\n      7qiZfU4dY1lWllzX7mPBI\n      Shape of You\n      Ed Sheeran\n      233713\n      vocal has a nice warm quality.\n    \n    \n      1\n      5CtI0qwDJkDQGwXD1H1cL\n      Despacito - Remix\n      Luis Fonsi\n      228827\n      Very European feeling. I like that the singer ...\n    \n    \n      2\n      4aWmUDTfIPGksMNLV2rQP\n      Despacito (Featuring Daddy Yankee)\n      Luis Fonsi\n      228200\n      Unique and quirky. Arrangement was good and ni...\n    \n    \n      3\n      6RUKPb4LETWmmr3iAEQkt\n      Something Just Like This\n      The Chainsmokers\n      247160\n      Tastefully put together song. Good instrumenta...\n    \n    \n      4\n      3DXncPQOG4VBw3QHh3S81\n      I'm the One\n      DJ Khaled\n      288600\n      Your voice is awesome\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      1PSBzsahR2AKwLJgx8ehB\n      Bad Things (with Camila Cabello)\n      Machine Gun Kelly\n      239293\n      Good strong voice\n    \n    \n      96\n      0QsvXIfqM0zZoerQfsI9l\n      Don't Let Me Down\n      The Chainsmokers\n      208053\n      good movie sound track\n    \n    \n      97\n      7mldq42yDuxiUNn08nvzH\n      Body Like A Back Road\n      Sam Hunt\n      165387\n      Good melody. A little different\n    \n    \n      98\n      7i2DJ88J7jQ8K7zqFX2fW\n      Now Or Never\n      Halsey\n      214802\n      Good lyrics...\n    \n    \n      99\n      1j4kHkkpqZRBwE0A4CN4Y\n      Dusk Till Dawn - Radio Edit\n      ZAYN\n      239000\n      Good hook - unique vocal presentation - I like...\n    \n  \n\n100 rows √ó 5 columns\n\n\n\n\nsongs_df #convert ms to min\nsongs_df['duration_min'] =  songs_df['duration_ms'] / 60000\nideal_songs_df = songs_df[(songs_df['duration_min'] >= 3) & (songs_df['duration_min'] <=5)]\nideal_songs_df\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      artists\n      duration_ms\n      reviews\n      duration_min\n    \n  \n  \n    \n      0\n      7qiZfU4dY1lWllzX7mPBI\n      Shape of You\n      Ed Sheeran\n      233713\n      vocal has a nice warm quality.\n      3.895217\n    \n    \n      1\n      5CtI0qwDJkDQGwXD1H1cL\n      Despacito - Remix\n      Luis Fonsi\n      228827\n      Very European feeling. I like that the singer ...\n      3.813783\n    \n    \n      2\n      4aWmUDTfIPGksMNLV2rQP\n      Despacito (Featuring Daddy Yankee)\n      Luis Fonsi\n      228200\n      Unique and quirky. Arrangement was good and ni...\n      3.803333\n    \n    \n      3\n      6RUKPb4LETWmmr3iAEQkt\n      Something Just Like This\n      The Chainsmokers\n      247160\n      Tastefully put together song. Good instrumenta...\n      4.119333\n    \n    \n      4\n      3DXncPQOG4VBw3QHh3S81\n      I'm the One\n      DJ Khaled\n      288600\n      Your voice is awesome\n      4.810000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      94\n      2fQrGHiQOvpL9UgPvtYy6\n      Bank Account\n      21 Savage\n      220307\n      Good vocal, Good arrangement, Good subject mat...\n      3.671783\n    \n    \n      95\n      1PSBzsahR2AKwLJgx8ehB\n      Bad Things (with Camila Cabello)\n      Machine Gun Kelly\n      239293\n      Good strong voice\n      3.988217\n    \n    \n      96\n      0QsvXIfqM0zZoerQfsI9l\n      Don't Let Me Down\n      The Chainsmokers\n      208053\n      good movie sound track\n      3.467550\n    \n    \n      98\n      7i2DJ88J7jQ8K7zqFX2fW\n      Now Or Never\n      Halsey\n      214802\n      Good lyrics...\n      3.580033\n    \n    \n      99\n      1j4kHkkpqZRBwE0A4CN4Y\n      Dusk Till Dawn - Radio Edit\n      ZAYN\n      239000\n      Good hook - unique vocal presentation - I like...\n      3.983333\n    \n  \n\n90 rows √ó 6 columns\n\n\n\n\n\n2.4 Who are the highest ranked artists?\nWhich artists have been ranked #1 the most times in 2017?\nUsing the dataframe rankings_df, we perform the following tasks on pandasql: - We extract the names of artist that have position as 1, and store this data in pd_pos_df - Using pd_pos_df, You can see the number of times each artist was ranked #1 - Then we get the Top 10 artists, i.e.¬†the 10 artists which have been ranked #1 the most times.\nThe dataframe pd_pos_df should have the following schema:\n\n\n\n\nArtist\nPosition\n\n\n\n\n\n\n\n# pandasql\npos_query = \"\"\" \nSELECT Artist, COUNT (Position) AS Position\nFROM rankings_df\nWHERE Position = 1\nGROUP BY Artist\nORDER BY Position DESC\nLIMIT 10\n\"\"\"\n\nsql_pos_df = ps.sqldf(pos_query, locals())\nsql_pos_df\n\n\n\n\n\n  \n    \n      \n      Artist\n      Position\n    \n  \n  \n    \n      0\n      Luis Fonsi\n      4085\n    \n    \n      1\n      Ed Sheeran\n      3780\n    \n    \n      2\n      Post Malone\n      1737\n    \n    \n      3\n      J Balvin\n      1195\n    \n    \n      4\n      Maluma\n      900\n    \n    \n      5\n      Natti Natasha\n      416\n    \n    \n      6\n      Bad Bunny\n      311\n    \n    \n      7\n      Taylor Swift\n      304\n    \n    \n      8\n      Danny Ocean\n      291\n    \n    \n      9\n      Camila Cabello\n      272\n    \n  \n\n\n\n\n\n\n2.5 Popular Artists!\nAre there artists whose songs are streamed more often than others? Let‚Äôs see!\n\nWe consider rows in rankings_df that are during Summer 2017\n\nI assume that the duration of summer is from 15th June 2017 to 16th September 2017 (both dates inclusive)\n\nThen I am finding the total number of streams corresponding to each artist, then storing this data in a new column called Number.\nThen I am sorting this dataframe on the Number column so that the most popular artists appear first (i.e.¬†sort according to the Number column in descending order).\n\nCalling the output dataframe sql_summer_df.\n\n#pandasql\nsummer_query = \"\"\"\nSELECT Artist, SUM(Streams) AS Number\nFROM rankings_df\nWHERE strftime ('%Y-%m-%d', DATE) BETWEEN '2017-06-15' AND '2017-09-16'\nGROUP BY Artist\nORDER BY Number DESC\n\"\"\"\n\nsql_summer_df = ps.sqldf(summer_query, locals())\nsql_summer_df\n\n\n\n\n\n  \n    \n      \n      Artist\n      Number\n    \n  \n  \n    \n      0\n      Ed Sheeran\n      1331033447\n    \n    \n      1\n      DJ Khaled\n      1298364289\n    \n    \n      2\n      Luis Fonsi\n      1186212514\n    \n    \n      3\n      Calvin Harris\n      1033045563\n    \n    \n      4\n      J Balvin\n      1021849673\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      3096\n      √Å M√≥ti S√≥l\n      1052\n    \n    \n      3097\n      Huntar\n      1049\n    \n    \n      3098\n      The Panas\n      1032\n    \n    \n      3099\n      Stef√°n Hilmarsson\n      1022\n    \n    \n      3100\n      Delano\n      1010\n    \n  \n\n3101 rows √ó 2 columns\n\n\n\n\n\n2.6 Which songs are danceable but also mellow?\nNow let us switch gears and examine songs_df and features_df. In particular, we want to You can see the songs with high danceability and low tempo.\nSTEPS to to solution: - Round the danceability column to one decimal place, and calling the resultant column r_danceability. This will allow us to conduct a more general (coarser) analysis of the data. - Merge songs_df and features_df, then sort the songs with danceability in descending order and tempo in ascending order. (When sorting, make sure to use the r_danceability column.) - Call the output dataframe sql_songs_features_df.\n\n#pandasql\nsong_feature_query = \"\"\"\nSELECT *, ROUND (danceability, 1) AS r_danceability\nFROM features_df\nJOIN songs_df ON features_df.id = songs_df.id\nORDER BY ROUND (danceability, 1) DESC, tempo ASC\n\"\"\"\n\n\nsql_song_features_df = ps.sqldf(song_feature_query, locals())\nsql_song_features_df\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      id\n      danceability\n      energy\n      key\n      loudness\n      mode\n      speechiness\n      acousticness\n      instrumentalness\n      liveness\n      valence\n      tempo\n      id\n      name\n      artists\n      duration_ms\n      reviews\n      duration_min\n      r_danceability\n    \n  \n  \n    \n      0\n      94\n      2fQrGHiQOvpL9UgPvtYy6\n      0.884\n      0.346\n      8.0\n      -8.228\n      0.0\n      0.3510\n      0.0151\n      0.000007\n      0.0871\n      0.3760\n      75.016\n      2fQrGHiQOvpL9UgPvtYy6\n      Bank Account\n      21 Savage\n      220307\n      Good vocal, Good arrangement, Good subject mat...\n      3.671783\n      0.9\n    \n    \n      1\n      42\n      5bcTCxgc7xVfSaMV3RuVk\n      0.893\n      0.745\n      11.0\n      -3.105\n      0.0\n      0.0571\n      0.0642\n      0.000000\n      0.0943\n      0.8720\n      101.018\n      5bcTCxgc7xVfSaMV3RuVk\n      Feels\n      Calvin Harris\n      223413\n      Overall quite a nice sound.\n      3.723550\n      0.9\n    \n    \n      2\n      47\n      6mICuAdrwEjh6Y6lroV2K\n      0.852\n      0.773\n      8.0\n      -2.921\n      0.0\n      0.0776\n      0.1870\n      0.000030\n      0.1590\n      0.9070\n      102.034\n      6mICuAdrwEjh6Y6lroV2K\n      Chantaje\n      Shakira\n      195840\n      NICE TUNE WITH SOME NEAT CHORD CHANGES....VERY...\n      3.264000\n      0.9\n    \n    \n      3\n      38\n      6EpRaXYhGOB3fj4V2uDkM\n      0.869\n      0.485\n      6.0\n      -5.595\n      1.0\n      0.0545\n      0.2460\n      0.000000\n      0.0765\n      0.5270\n      106.028\n      6EpRaXYhGOB3fj4V2uDkM\n      Strip That Down\n      Liam Payne\n      204502\n      Rich clean vocals. Nice incidental instrumenta...\n      3.408367\n      0.9\n    \n    \n      4\n      91\n      4c2W3VKsOFoIg2SFaO6DY\n      0.855\n      0.624\n      1.0\n      -4.093\n      1.0\n      0.0488\n      0.1580\n      0.000000\n      0.0513\n      0.9620\n      117.959\n      4c2W3VKsOFoIg2SFaO6DY\n      Your Song\n      Rita Ora\n      180757\n      Great music ,Great voice ,arrangement, etc.\n      3.012617\n      0.9\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      52\n      4pdPtRcBmOSQDlJ3Fk945\n      0.476\n      0.718\n      8.0\n      -5.309\n      1.0\n      0.0576\n      0.0784\n      0.000010\n      0.1220\n      0.1420\n      199.864\n      4pdPtRcBmOSQDlJ3Fk945\n      Let Me Love You\n      DJ Snake\n      205947\n      Nice melodies, especially in the beginning. Ha...\n      3.432450\n      0.5\n    \n    \n      96\n      22\n      5uCax9HTNlzGybIStD3vD\n      0.358\n      0.557\n      10.0\n      -7.398\n      1.0\n      0.0590\n      0.6950\n      0.000000\n      0.0902\n      0.4940\n      85.043\n      5uCax9HTNlzGybIStD3vD\n      Say You Won't Let Go\n      James Arthur\n      211467\n      truely unique, i like it\n      3.524450\n      0.4\n    \n    \n      97\n      63\n      6520aj0B4FSKGVuKNsOCO\n      0.448\n      0.801\n      0.0\n      -5.363\n      1.0\n      0.1650\n      0.0733\n      0.000000\n      0.1460\n      0.4620\n      189.798\n      6520aj0B4FSKGVuKNsOCO\n      Chained To The Rhythm\n      Katy Perry\n      237734\n      Like the recording .nice and clear,smooth\n      3.962233\n      0.4\n    \n    \n      98\n      66\n      5hYTyyh2odQKphUbMqc5g\n      0.314\n      0.555\n      9.0\n      -9.601\n      1.0\n      0.3700\n      0.1570\n      0.000108\n      0.0670\n      0.1590\n      179.666\n      5hYTyyh2odQKphUbMqc5g\n      How Far I'll Go - From \"Moana\"\n      Alessia Cara\n      175517\n      Kind of reminds me of new music.\n      2.925283\n      0.3\n    \n    \n      99\n      99\n      1j4kHkkpqZRBwE0A4CN4Y\n      0.258\n      0.437\n      11.0\n      -6.593\n      0.0\n      0.0390\n      0.1010\n      0.000001\n      0.1060\n      0.0967\n      180.043\n      1j4kHkkpqZRBwE0A4CN4Y\n      Dusk Till Dawn - Radio Edit\n      ZAYN\n      239000\n      Good hook - unique vocal presentation - I like...\n      3.983333\n      0.3\n    \n  \n\n100 rows √ó 20 columns\n\n\n\n\n\n2.7 Do we like the same songs?\n\n2.7.1 Which regions have the most streams?\n\nExtracted rows belonging to the top 2 regions that have the most streams.\nStoreed output in a new dataframe called sql_top_regions_df. The schema is as rankings_df.\n\nNote: Since we want to focus on specific regions, we should disregard rows where the Region column has the value \"global\".\n\n#pandas\npd_top_regions_df = rankings_df[rankings_df['Region'] != 'global'] #remove global from column region\npd_top_regions_df = pd_top_regions_df.groupby(by = 'Region').sum().reset_index() #group by Region\npd_top_regions_df = pd_top_regions_df.sort_values(by = ['Streams'], ascending = [False]).head(2) #We want the ones with most Streams\npd_top_regions_df = rankings_df[rankings_df['Region'].isin(['us', 'gb'])] #checking for us and gb\npd_top_regions_df #we use that in pandasql below in our subquery\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/905598242.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  pd_top_regions_df = pd_top_regions_df.groupby(by = 'Region').sum().reset_index() #group by Region\n\n\n\n\n\n\n  \n    \n      \n      Position\n      Track Name\n      Artist\n      Streams\n      Date\n      Region\n      ID\n    \n  \n  \n    \n      3042585\n      143\n      The Sound\n      The 1975\n      30799\n      2017-01-01\n      gb\n      316r1KLN0bcmpr7TZcMCX\n    \n    \n      3042584\n      142\n      Is This Love - Remix\n      Bob Marley & The Wailers\n      31000\n      2017-01-01\n      gb\n      1w5sLDYzYAGI0AkLc6FPl\n    \n    \n      3042583\n      141\n      Sidewalks\n      The Weeknd\n      31453\n      2017-01-01\n      gb\n      4h90qkbnW1Qq6pBhoPvwk\n    \n    \n      3042582\n      140\n      False Alarm\n      Matoma\n      31527\n      2017-01-01\n      gb\n      7gZQfdEQpmwAoPHSbEHzm\n    \n    \n      3042581\n      139\n      Jumpman\n      Drake\n      31716\n      2017-01-01\n      gb\n      27GmP9AWRs744SzKcpJsT\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      845636\n      15\n      Pick It Up (feat. A$AP Rocky)\n      Famous Dex\n      687929\n      2018-01-09\n      us\n      3ncgNpxLoBQ65ABk4djDy\n    \n    \n      845635\n      14\n      River (feat. Ed Sheeran)\n      Eminem\n      724892\n      2018-01-09\n      us\n      5UEnHoDYpsxlfzWLZIc7L\n    \n    \n      845634\n      13\n      Candy Paint\n      Post Malone\n      735421\n      2018-01-09\n      us\n      42CeaId2XNlxugDvyqHfD\n    \n    \n      845632\n      11\n      Codeine Dreaming (feat. Lil Wayne)\n      Kodak Black\n      839826\n      2018-01-09\n      us\n      4DTpngLjoHj5gFxEZFeD3\n    \n    \n      845662\n      41\n      Bad At Love\n      Halsey\n      474255\n      2018-01-09\n      us\n      7y9iMe8SOB6z3NoHE2OfX\n    \n  \n\n148374 rows √ó 7 columns\n\n\n\n\n#pandasql\ntop_regions_query = \"\"\"\nSELECT *\nFROM rankings_df\nWHERE Region IN (\n  SELECT Region FROM pd_top_regions_df\n  WHERE Region<>'global'\n  LIMIT 2\n)\nORDER BY Streams DESC\n\"\"\"\n\n\nsql_top_regions_df = ps.sqldf(top_regions_query, locals())\nsql_top_regions_df\n\n\n\n\n\n  \n    \n      \n      Position\n      Track Name\n      Artist\n      Streams\n      Date\n      Region\n      ID\n    \n  \n  \n    \n      0\n      1\n      Last Christmas\n      Wham!\n      1357938\n      2017-12-25 00:00:00.000000\n      gb\n      2FRnf9qhLbvw8fu4IBXx7\n    \n    \n      1\n      1\n      Shape of You\n      Ed Sheeran\n      1323982\n      2017-01-09 00:00:00.000000\n      gb\n      7qiZfU4dY1lWllzX7mPBI\n    \n    \n      2\n      2\n      All I Want for Christmas Is You\n      Mariah Carey\n      1299377\n      2017-12-25 00:00:00.000000\n      gb\n      0bYg9bo50gSsH3LtXe2SQ\n    \n    \n      3\n      1\n      Shape of You\n      Ed Sheeran\n      1280898\n      2017-01-06 00:00:00.000000\n      gb\n      7qiZfU4dY1lWllzX7mPBI\n    \n    \n      4\n      1\n      Shape of You\n      Ed Sheeran\n      1258743\n      2017-01-10 00:00:00.000000\n      gb\n      7qiZfU4dY1lWllzX7mPBI\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      74185\n      196\n      Famous\n      Kanye West\n      25248\n      2017-01-01 00:00:00.000000\n      gb\n      19a3JfW8BQwqHWUMbcqSx\n    \n    \n      74186\n      197\n      Girls Just Want to Have Fun\n      Cyndi Lauper\n      25176\n      2017-01-01 00:00:00.000000\n      gb\n      4y1LsJpmMti1PfRQV9AWW\n    \n    \n      74187\n      198\n      Dog Days Are Over\n      Florence + The Machine\n      24904\n      2017-01-01 00:00:00.000000\n      gb\n      1YLJVmuzeM2YSUkCCaTNU\n    \n    \n      74188\n      199\n      Half the World Away - Remastered\n      Oasis\n      24806\n      2017-01-01 00:00:00.000000\n      gb\n      6z1Xz89avh1KA7d3Ek7DQ\n    \n    \n      74189\n      200\n      Shut Up\n      Stormzy\n      24727\n      2017-01-01 00:00:00.000000\n      gb\n      2LPUvD5DDOO4UYGkWgjI2\n    \n  \n\n74190 rows √ó 7 columns\n\n\n\n\n\n2.7.2 Do the regions with the most streams like different songs?\n\nFinding the songs that the two regions (found in 2.7.1) DO NOT have in common\nStoring the result in a new dataframe called sql_diff_tracks_df.\n\n\n#pandasql\n\ndiff_tracks_query = \"\"\"\nWITH USdf AS(\n   SELECT DISTINCT `Track Name`, Artist, ID\n   FROM pd_top_regions_df\n   WHERE Region='us'),\n\nGBdf AS (\n  SELECT DISTINCT `Track Name`, Artist, ID\n  FROM pd_top_regions_df\n  WHERE Region='gb'),\n\nUSonly AS (\n  SELECT USdf.`Track Name`, USdf.Artist, USdf.ID as ID \n  FROM USdf\n  LEFT JOIN GBdf ON USdf.id = GBdf.id\n  WHERE GBdf.id IS NULL),\n\n GBonly AS (\n   SELECT USdf.`Track Name`, USdf.Artist, GBdf.ID as ID\n   FROM GBdf\n   LEFT JOIN USdf ON USdf.id = GBdf.id\n   WHERE USdf.id IS NULL)\n\n SELECT * FROM USonly\n UNION\n SELECT * FROM GBonly\n\"\"\"\nsql_diff_tracks_df = ps.sqldf(diff_tracks_query, locals())\nsql_diff_tracks_df\n\n\n\n\n\n  \n    \n      \n      Track Name\n      Artist\n      ID\n    \n  \n  \n    \n      0\n      None\n      None\n      00l1uBtEO4WwmsfxqbeTW\n    \n    \n      1\n      None\n      None\n      0181HMomm7xM3Ks5YNlA9\n    \n    \n      2\n      None\n      None\n      01VXGDL8Ox3SWmvM8ZyvS\n    \n    \n      3\n      None\n      None\n      023lag1AgeOf7YChojecR\n    \n    \n      4\n      None\n      None\n      02KgB1Qyk4PrFweUMGl9N\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2447\n      playboy shit (feat. lil aaron)\n      blackbear\n      031QO44Ql8D7D3ePdI7fk\n    \n    \n      2448\n      santa monica & la brea\n      blackbear\n      3Tvs5NbIqszxl7ctruIqa\n    \n    \n      2449\n      top priority (with Ne-Yo)\n      blackbear\n      2yjwPmZ3XKKoVVPzKQ9d0\n    \n    \n      2450\n      up in this (with Tinashe)\n      blackbear\n      7xmaCmiSOCtQ6nFENK22b\n    \n    \n      2451\n      wokeuplikethis*\n      Playboi Carti\n      59J5nzL1KniFHnU120dQz\n    \n  \n\n2452 rows √ó 3 columns\n\n\n\n\n\n\n2.8 New Years Eve Party!\nWho doesn‚Äôt love to dance? Let‚Äôs You can see some songs to groove to!\n\nYou can see the songs that made it to the charts in December 2017 (2017-12-01 to 2017-12-31) and whose duration is longer than 3 minutes\nYou can see the artist and the danceability of these songs. Be sure to only include songs with danceability > 0.5.\nThe result is stored in a new dataframe called sql_dance_df that has the following schema:\n\n\n\n\n\nArtist\nTrack Name\ndanceability\n\n\n\n\n\n\nHint: Think about which data resides in which table!\n\n#SQL SKELETON\n\n# FROM\n# WHERE\n# _______\n# GROUP BY\n# HAVING\n# ________\n# SELECT\n# _______\n# ORDER BY\n# LIMIT\n\n\nrankings_df.rename(columns = {'ID':'id'}, inplace = True)\nrankings_df\n#making ID work as id\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/2501768206.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  rankings_df.rename(columns = {'ID':'id'}, inplace = True)\n\n\n\n\n\n\n  \n    \n      \n      Position\n      Track Name\n      Artist\n      Streams\n      Date\n      Region\n      id\n    \n  \n  \n    \n      0\n      1\n      Reggaet√≥n Lento (Bailemos)\n      CNCO\n      19272\n      2017-01-01\n      ec\n      3AEZUABDXNtecAOSC1qTf\n    \n    \n      2658317\n      118\n      Steady 1234 (feat. Jasmine Thompson & Skizzy M...\n      Vice\n      15142\n      2017-01-01\n      nl\n      40UroIGvsMPLPBYwH8rMN\n    \n    \n      2658316\n      117\n      Dynamite (feat. Pretty Sister)\n      Nause\n      15152\n      2017-01-01\n      nl\n      2Ae5awwKvQpTBKQHr1TYC\n    \n    \n      2658315\n      116\n      Hello\n      Adele\n      15170\n      2017-01-01\n      nl\n      4sPmO7WMQUAf45kwMOtON\n    \n    \n      2658314\n      115\n      One Night Stand\n      B-Brave\n      15510\n      2017-01-01\n      nl\n      2no9x9FRytP9PnB3CQPYS\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1498830\n      109\n      Sky Walker\n      Miguel\n      1581\n      2018-01-09\n      hu\n      5WoaF1B5XIEnWfmb5NZik\n    \n    \n      1498829\n      108\n      Back to You (feat. Bebe Rexha & Digital Farm A...\n      Louis Tomlinson\n      1582\n      2018-01-09\n      hu\n      7F9vK8hNFMml4GtHsaXui\n    \n    \n      1498828\n      107\n      Betrayed\n      Lil Xan\n      1582\n      2018-01-09\n      hu\n      6NWl2m8asvH83xjuXVNsu\n    \n    \n      1498841\n      120\n      Rockabye (feat. Sean Paul & Anne-Marie)\n      Clean Bandit\n      1490\n      2018-01-09\n      hu\n      5knuzwU65gJK7IF5yJsua\n    \n    \n      3441196\n      200\n      Let Her Go\n      Passenger\n      2088\n      2018-01-09\n      hk\n      2jyjhRf6DVbMPU5zxagN2\n    \n  \n\n3440540 rows √ó 7 columns\n\n\n\n\n#pandasql\ndance_query = \"\"\"\nSELECT DISTINCT Artist, `Track Name`,   danceability\nFROM songs_df AS s \n  JOIN rankings_df AS r ON r.id= s.id\n  JOIN features_df AS f ON f.id=r.id\nWHERE duration_min > 3\n  AND strftime('%Y-%m-%d', DATE) BETWEEN '2017-12-01' AND '2017-12-31'\n  AND danceability  > 0.5\n\"\"\"\n\nsql_dance_df = ps.sqldf(dance_query, locals())\nsql_dance_df\n\n\n\n\n\n  \n    \n      \n      Artist\n      Track Name\n      danceability\n    \n  \n  \n    \n      0\n      Ed Sheeran\n      Shape of You\n      0.825\n    \n    \n      1\n      Luis Fonsi\n      Despacito - Remix\n      0.694\n    \n    \n      2\n      Luis Fonsi\n      Despacito (Featuring Daddy Yankee)\n      0.660\n    \n    \n      3\n      The Chainsmokers\n      Something Just Like This\n      0.617\n    \n    \n      4\n      DJ Khaled\n      I'm the One\n      0.609\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      80\n      Justin Bieber\n      Friends (with BloodPop¬Æ)\n      0.744\n    \n    \n      81\n      21 Savage\n      Bank Account\n      0.884\n    \n    \n      82\n      Machine Gun Kelly\n      Bad Things (with Camila Cabello)\n      0.675\n    \n    \n      83\n      The Chainsmokers\n      Don't Let Me Down\n      0.542\n    \n    \n      84\n      Halsey\n      Now Or Never\n      0.658\n    \n  \n\n85 rows √ó 3 columns"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-3-data-visualization",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-3-data-visualization",
    "title": "Loizos Konstantinou",
    "section": "Part 3: Data Visualization",
    "text": "Part 3: Data Visualization\nThe popularity of songs fluctuates as time progresses. We want to create a graph that illustrates the no. of streams for the most and least popular songs during each month in 2017.\nPerform the following tasks: - You can see the song that had the most streams on 2017-01-01 - You can see the song that had the least streams on 2017-01-01 - You can see the no. of streams that these two songs received on the first day of each month in 2017 (eg. 2017-01-01, 2017-02-01, 2017-03-01 ‚Ä¶ 2017-12-01)\nPlot a line graph which shows the trend you found! Make sure you use the ID of the two songs when creating this graph.\nuseful resource: https://seaborn.pydata.org/generated/seaborn.lineplot.html\nThe line graph has the following features: 1. The X-axis should be labelled ‚ÄúDate‚Äù, and the Y-axis should be labelled ‚ÄúStreams‚Äù. 2. There should be markers on the plot to specify the no. of streams each song received ia particular month. 3. The lines corresponding to the two songs should have different colors.\n\n#first we take a look which songs are streamed the most\nfilter_rankings = rankings_df[(rankings_df['Date'] == '2017-01-01')] #setting the date asked\nfilter_rankings = filter_rankings.groupby('id').sum().reset_index()\nsorted_rankings = filter_rankings.sort_values(by=['Streams'], ascending=False).reset_index() #sorting from most streams to least\ntop_bottom_rankings = pd.concat([sorted_rankings.head(1), sorted_rankings.tail(1)]) #getting the most and least streamed song \ndisplay(top_bottom_rankings)\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/745237006.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  filter_rankings = filter_rankings.groupby('id').sum().reset_index()\n\n\n\n\n\n\n  \n    \n      \n      index\n      id\n      Position\n      Streams\n    \n  \n  \n    \n      0\n      1690\n      5aAx2yezTd8zXrkmtKl66\n      422\n      6266206\n    \n    \n      2360\n      2302\n      7qxgfIAuWUY9VHLn35Sqw\n      199\n      1001\n    \n  \n\n\n\n\n\n#You can see the no. of streams that these two songs received on the first day of each month in 2017\nlist_of_dates = ['2017-01-01', '2017-02-01', '2017-03-01', '2017-04-01', '2017-05-01', '2017-06-01', '2017-07-01', '2017-08-01', '2017-09-01',\n                 '2017-10-01', '2017-11-01', '2017-12-01']\n\nall_months_rankings = rankings_df[rankings_df['Date'].isin(list_of_dates)] #filter for list_of_dates asked\nall_months_rankings = all_months_rankings[all_months_rankings['id'].isin(top_bottom_rankings['id'])]\nall_months_rankings\n\n\n\n\n\n\n  \n    \n      \n      Position\n      Track Name\n      Artist\n      Streams\n      Date\n      Region\n      id\n    \n  \n  \n    \n      2755014\n      18\n      Starboy\n      The Weeknd\n      16013\n      2017-01-01\n      co\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      2732400\n      1\n      Starboy\n      The Weeknd\n      2197\n      2017-01-01\n      sk\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      462008\n      10\n      Starboy\n      The Weeknd\n      104708\n      2017-01-01\n      ph\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      2658204\n      5\n      Starboy\n      The Weeknd\n      76076\n      2017-01-01\n      nl\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      445200\n      1\n      Starboy\n      The Weeknd\n      2583\n      2017-01-01\n      lt\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2272744\n      185\n      Starboy\n      The Weeknd\n      44109\n      2017-12-01\n      mx\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      2124327\n      168\n      Starboy\n      The Weeknd\n      2765\n      2017-12-01\n      pt\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      837998\n      177\n      Starboy\n      The Weeknd\n      219100\n      2017-12-01\n      us\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      763686\n      65\n      Starboy\n      The Weeknd\n      13676\n      2017-12-01\n      tr\n      5aAx2yezTd8zXrkmtKl66\n    \n    \n      951109\n      195\n      Starboy\n      The Weeknd\n      2393\n      2017-12-01\n      cr\n      5aAx2yezTd8zXrkmtKl66\n    \n  \n\n456 rows √ó 7 columns\n\n\n\n\nIDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\nIDdateGroup\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/1523896782.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  IDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\n\n\n\n\n\n\n  \n    \n      \n      id\n      Date\n      Position\n      Streams\n    \n  \n  \n    \n      0\n      5aAx2yezTd8zXrkmtKl66\n      2017-01-01\n      422\n      6266206\n    \n    \n      1\n      5aAx2yezTd8zXrkmtKl66\n      2017-02-01\n      939\n      5697420\n    \n    \n      2\n      5aAx2yezTd8zXrkmtKl66\n      2017-03-01\n      1803\n      4259480\n    \n    \n      3\n      5aAx2yezTd8zXrkmtKl66\n      2017-04-01\n      2609\n      3263163\n    \n    \n      4\n      5aAx2yezTd8zXrkmtKl66\n      2017-05-01\n      4094\n      2179513\n    \n    \n      5\n      5aAx2yezTd8zXrkmtKl66\n      2017-06-01\n      4306\n      1906610\n    \n    \n      6\n      5aAx2yezTd8zXrkmtKl66\n      2017-07-01\n      4027\n      1641376\n    \n    \n      7\n      5aAx2yezTd8zXrkmtKl66\n      2017-08-01\n      3973\n      1348133\n    \n    \n      8\n      5aAx2yezTd8zXrkmtKl66\n      2017-09-01\n      4190\n      1380447\n    \n    \n      9\n      5aAx2yezTd8zXrkmtKl66\n      2017-10-01\n      3558\n      1061926\n    \n    \n      10\n      5aAx2yezTd8zXrkmtKl66\n      2017-11-01\n      2525\n      1040093\n    \n    \n      11\n      5aAx2yezTd8zXrkmtKl66\n      2017-12-01\n      1521\n      999344\n    \n    \n      12\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-01-01\n      199\n      1001\n    \n    \n      13\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-02-01\n      122\n      1440\n    \n    \n      14\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-03-01\n      144\n      1245\n    \n    \n      15\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-04-01\n      147\n      1198\n    \n    \n      16\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-07-01\n      160\n      1112\n    \n  \n\n\n\n\n\nIDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\nIDdateGroup\n\n/var/folders/71/m9n1tm292gg25xn0m44ccvxh0000gn/T/ipykernel_13596/1523896782.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  IDdateGroup = all_months_rankings.groupby(['id', 'Date']).sum().reset_index()\n\n\n\n\n\n\n  \n    \n      \n      id\n      Date\n      Position\n      Streams\n    \n  \n  \n    \n      0\n      5aAx2yezTd8zXrkmtKl66\n      2017-01-01\n      422\n      6266206\n    \n    \n      1\n      5aAx2yezTd8zXrkmtKl66\n      2017-02-01\n      939\n      5697420\n    \n    \n      2\n      5aAx2yezTd8zXrkmtKl66\n      2017-03-01\n      1803\n      4259480\n    \n    \n      3\n      5aAx2yezTd8zXrkmtKl66\n      2017-04-01\n      2609\n      3263163\n    \n    \n      4\n      5aAx2yezTd8zXrkmtKl66\n      2017-05-01\n      4094\n      2179513\n    \n    \n      5\n      5aAx2yezTd8zXrkmtKl66\n      2017-06-01\n      4306\n      1906610\n    \n    \n      6\n      5aAx2yezTd8zXrkmtKl66\n      2017-07-01\n      4027\n      1641376\n    \n    \n      7\n      5aAx2yezTd8zXrkmtKl66\n      2017-08-01\n      3973\n      1348133\n    \n    \n      8\n      5aAx2yezTd8zXrkmtKl66\n      2017-09-01\n      4190\n      1380447\n    \n    \n      9\n      5aAx2yezTd8zXrkmtKl66\n      2017-10-01\n      3558\n      1061926\n    \n    \n      10\n      5aAx2yezTd8zXrkmtKl66\n      2017-11-01\n      2525\n      1040093\n    \n    \n      11\n      5aAx2yezTd8zXrkmtKl66\n      2017-12-01\n      1521\n      999344\n    \n    \n      12\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-01-01\n      199\n      1001\n    \n    \n      13\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-02-01\n      122\n      1440\n    \n    \n      14\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-03-01\n      144\n      1245\n    \n    \n      15\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-04-01\n      147\n      1198\n    \n    \n      16\n      7qxgfIAuWUY9VHLn35Sqw\n      2017-07-01\n      160\n      1112\n    \n  \n\n\n\n\n\nimport seaborn\nseaborn.lineplot(data=IDdateGroup, x='Date', y='Streams', markers=True, hue='id')\n\n<AxesSubplot: xlabel='Date', ylabel='Streams'>"
  },
  {
    "objectID": "software/Analysing_Spotify_Data_in_SQL.html#part-4-working-with-text-data",
    "href": "software/Analysing_Spotify_Data_in_SQL.html#part-4-working-with-text-data",
    "title": "Loizos Konstantinou",
    "section": "Part 4: Working with Text Data",
    "text": "Part 4: Working with Text Data\nNow, let‚Äôs switch gears and try to text-based analysis. Textual data is complex, but can also be used to generate extremely interpretable results, making it both valuable and interesting.\nThroughout this section, we will attempt to answer the following question:\nAccording to the songs_df dataframe, what do the reviews for the Top Tracks of 2017 look like?\n###4.1 Tokenizing the text\nWe are going to split the contents of in the Reviews column into a list of words. We will use the nltk library, which contains an extensive set of tools for text processing. We are only going to use the following components of the library: - nltk.word_tokenize(): a function used to tokenize text - nltk.corpus.stopwords: a list of commonly used words such as ‚Äúa‚Äù, ‚Äúan‚Äù,‚Äúin‚Äù that are often ignored in text analysis\nNote that for this question, we didn‚Äôt have to clean the text data first as our original dataset was well-formatted. However, in practice, we would typically clean the text first using regular expressions (regex). Keep this in mind as you work on the project later on in the semester.\n\nUsing nltk.corpus.stopwords to create a set containing the most common English stopwords.\nImplementing the function tokenized_content(content), which takes in a string and does the following:\n\n\nTokenizing the text\nKeeping tokens that only contain alphabetic characters (i.e.¬†tokens with no punctuation)\nConverting each token to lowercase\nRemoving stopwords (commonly used words such as ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúin‚Äù)\n\n\nimport nltk\nnltk.__version__\n\n'3.7'\n\n\n\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('stopwords')\nstopwords = set(stopwords.words('english'))\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/loizoskon/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\nfrom nltk.tokenize import word_tokenize\n\ndef tokenize_content(content):\n  content = word_tokenize(content)\n  content = [word for word in content if word.isalpha()]\n  content = [word.lower() for word in content]\n  content = [word for word in content if word not in stopwords]\n  return content\n\nWhat is happening in the following columns - Extracting the reviews column of songs_df as a list called reviews. - Applying your tokenize_content() function to each item in the list reviews. Calling the resultant list top_tokens_list. - Flattening the list top_tokens_list, and call the resultant list top_tokens.\n\nreviews = songs_df['reviews'].tolist() #Extract the reviews column of songs_df as a list called reviews.\n\n\n# tokenize and flatten\n\ntop_tokens_list = []\nfor i in reviews:\n     top_tokens_list.append(tokenize_content(i))\n\ntop_tokens = [item for sublist in top_tokens_list for item in sublist]\n\n\n4.2 Most Frequent Words\nHere you can see the 20 most common words in the list top_tokens. Saving the result as a list of (word, count) tuples, in descending order of count.\nFor this question, Counter from the Python collections library is used: https://docs.python.org/2/library/collections.html#counter-objects\n\nfrom collections import Counter\ncount = Counter(top_tokens)\ntop_most_common = count.most_common(20)\nprint(top_most_common)\n\n[('good', 35), ('nice', 31), ('like', 26), ('song', 21), ('voice', 18), ('great', 14), ('really', 13), ('unique', 12), ('lyrics', 12), ('sound', 11), ('love', 9), ('interesting', 9), ('vocal', 8), ('tune', 8), ('vocals', 8), ('instrumentation', 7), ('melody', 7), ('music', 7), ('feel', 6), ('overall', 6)]\n\n\n\n\n4.3 Word Clouds\nBefore we move on from this dataset, let‚Äôs visualize our results using a word cloud.\nHere I create a word cloud containing all the words in the list top_tokens (created in question 4.1). The WordCloud documentation contains instructions on how to do this.\n\nimport re #Load the regular expression library\nimport wordcloud # Import the wordcloud library\n\n\n# Print the titles of the first rows \n#Here I create a word cloud for top tokens\nplt.subplots(figsize = (20,10))\n\nwordcloud = WordCloud (\n                    background_color = 'white',\n                    width = 510,\n                    height = 380\n                        ).generate_from_frequencies(count)\nplt.imshow(wordcloud) # image show\nplt.axis('off') # to off the axis of x and y\nplt.show()"
  },
  {
    "objectID": "software/index.html",
    "href": "software/index.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some things I made\n\nRStudio projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      Google Trends\n      Top 3 American Animated Sitcoms\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Class Die Experiment\n      Correct guesses Vs cheats ~ summary statistics\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Titanic Dataset\n      Probabilities of surviving in terms of class and gender\n   \n  \n\nPython projects\n\n  \n    \n      \n    \n    \n    \n        \n      \n      State of The Union Addresses\n      US presidents speech, keyword similarity analysis\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      Credit Card Application\n      Predicting a  customer's credit card application outcome\n   \n  \n  \n    \n      \n    \n    \n    \n        \n      \n      SQL - Sorting and Analysing Spotify Data\n      Performing SQL functions on a dataset containing Top 2017 Spotify Tracks, along with their reviews and rankings.\n   \n  \n\n\nNo matching items"
  },
  {
    "objectID": "software/expoftruth.html",
    "href": "software/expoftruth.html",
    "title": "Class Die Experiment",
    "section": "",
    "text": "library(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úî ggplot2 3.3.5      ‚úî purrr   0.3.4 \n‚úî tibble  3.1.6      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.0 \n‚úî readr   2.0.1      ‚úî forcats 0.5.1 \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(descr)\nlibrary(infer)\n\nHere we will have to work with the experiment data we collected for this class.\n\nWe are interested in whether some people were more likely to make correct guesses. What are the summary statistics for all draws? Plot a histogram. Are there any outliers in the data (i.e. individuals who guess 0 times correctly and individuals who guessed all 20 times correctly?)\nwe draw a histogram based on the summary statistics. To find\n\nlibrary(readxl)\nexp21 <- read_excel(\"experiment_2021C(1)copy.xlsx\", col_names = FALSE,\n    skip = 2)\n\nNew names:\n‚Ä¢ `` -> `...1`\n‚Ä¢ `` -> `...2`\n‚Ä¢ `` -> `...3`\n‚Ä¢ `` -> `...4`\n‚Ä¢ `` -> `...5`\n‚Ä¢ `` -> `...6`\n‚Ä¢ `` -> `...7`\n‚Ä¢ `` -> `...8`\n‚Ä¢ `` -> `...9`\n‚Ä¢ `` -> `...10`\n‚Ä¢ `` -> `...11`\n‚Ä¢ `` -> `...12`\n‚Ä¢ `` -> `...13`\n‚Ä¢ `` -> `...14`\n‚Ä¢ `` -> `...15`\n‚Ä¢ `` -> `...16`\n‚Ä¢ `` -> `...17`\n‚Ä¢ `` -> `...18`\n‚Ä¢ `` -> `...19`\n‚Ä¢ `` -> `...20`\n‚Ä¢ `` -> `...21`\n‚Ä¢ `` -> `...22`\n‚Ä¢ `` -> `...23`\n‚Ä¢ `` -> `...24`\n‚Ä¢ `` -> `...25`\n‚Ä¢ `` -> `...26`\n‚Ä¢ `` -> `...27`\n‚Ä¢ `` -> `...28`\n‚Ä¢ `` -> `...29`\n‚Ä¢ `` -> `...30`\n‚Ä¢ `` -> `...31`\n‚Ä¢ `` -> `...32`\n‚Ä¢ `` -> `...33`\n‚Ä¢ `` -> `...34`\n‚Ä¢ `` -> `...35`\n‚Ä¢ `` -> `...36`\n‚Ä¢ `` -> `...37`\n‚Ä¢ `` -> `...38`\n‚Ä¢ `` -> `...39`\n‚Ä¢ `` -> `...40`\n‚Ä¢ `` -> `...41`\n‚Ä¢ `` -> `...42`\n‚Ä¢ `` -> `...43`\n‚Ä¢ `` -> `...44`\n‚Ä¢ `` -> `...45`\n‚Ä¢ `` -> `...46`\n‚Ä¢ `` -> `...47`\n‚Ä¢ `` -> `...48`\n‚Ä¢ `` -> `...49`\n‚Ä¢ `` -> `...50`\n‚Ä¢ `` -> `...51`\n‚Ä¢ `` -> `...52`\n‚Ä¢ `` -> `...53`\n‚Ä¢ `` -> `...54`\n‚Ä¢ `` -> `...55`\n‚Ä¢ `` -> `...56`\n‚Ä¢ `` -> `...57`\n‚Ä¢ `` -> `...58`\n‚Ä¢ `` -> `...59`\n‚Ä¢ `` -> `...60`\n‚Ä¢ `` -> `...61`\n‚Ä¢ `` -> `...62`\n‚Ä¢ `` -> `...63`\n‚Ä¢ `` -> `...64`\n‚Ä¢ `` -> `...65`\n‚Ä¢ `` -> `...66`\n‚Ä¢ `` -> `...67`\n‚Ä¢ `` -> `...68`\n‚Ä¢ `` -> `...69`\n‚Ä¢ `` -> `...70`\n‚Ä¢ `` -> `...71`\n‚Ä¢ `` -> `...72`\n‚Ä¢ `` -> `...73`\n‚Ä¢ `` -> `...74`\n‚Ä¢ `` -> `...75`\n‚Ä¢ `` -> `...76`\n‚Ä¢ `` -> `...77`\n‚Ä¢ `` -> `...78`\n‚Ä¢ `` -> `...79`\n‚Ä¢ `` -> `...80`\n‚Ä¢ `` -> `...81`\n‚Ä¢ `` -> `...82`\n‚Ä¢ `` -> `...83`\n‚Ä¢ `` -> `...84`\n‚Ä¢ `` -> `...85`\n‚Ä¢ `` -> `...86`\n‚Ä¢ `` -> `...87`\n‚Ä¢ `` -> `...88`\n‚Ä¢ `` -> `...89`\n‚Ä¢ `` -> `...90`\n‚Ä¢ `` -> `...91`\n‚Ä¢ `` -> `...92`\n‚Ä¢ `` -> `...93`\n‚Ä¢ `` -> `...94`\n‚Ä¢ `` -> `...95`\n‚Ä¢ `` -> `...96`\n‚Ä¢ `` -> `...97`\n‚Ä¢ `` -> `...98`\n‚Ä¢ `` -> `...99`\n‚Ä¢ `` -> `...100`\n‚Ä¢ `` -> `...101`\n‚Ä¢ `` -> `...102`\n‚Ä¢ `` -> `...103`\n‚Ä¢ `` -> `...104`\n‚Ä¢ `` -> `...105`\n‚Ä¢ `` -> `...106`\n‚Ä¢ `` -> `...107`\n‚Ä¢ `` -> `...108`\n‚Ä¢ `` -> `...109`\n‚Ä¢ `` -> `...110`\n‚Ä¢ `` -> `...111`\n‚Ä¢ `` -> `...112`\n‚Ä¢ `` -> `...113`\n‚Ä¢ `` -> `...114`\n‚Ä¢ `` -> `...115`\n‚Ä¢ `` -> `...116`\n‚Ä¢ `` -> `...117`\n‚Ä¢ `` -> `...118`\n‚Ä¢ `` -> `...119`\n‚Ä¢ `` -> `...120`\n‚Ä¢ `` -> `...121`\n‚Ä¢ `` -> `...122`\n‚Ä¢ `` -> `...123`\n‚Ä¢ `` -> `...124`\n‚Ä¢ `` -> `...125`\n‚Ä¢ `` -> `...126`\n‚Ä¢ `` -> `...127`\n‚Ä¢ `` -> `...128`\n‚Ä¢ `` -> `...129`\n‚Ä¢ `` -> `...130`\n‚Ä¢ `` -> `...131`\n‚Ä¢ `` -> `...132`\n‚Ä¢ `` -> `...133`\n‚Ä¢ `` -> `...134`\n‚Ä¢ `` -> `...135`\n‚Ä¢ `` -> `...136`\n‚Ä¢ `` -> `...137`\n‚Ä¢ `` -> `...138`\n‚Ä¢ `` -> `...139`\n‚Ä¢ `` -> `...140`\n‚Ä¢ `` -> `...141`\n‚Ä¢ `` -> `...142`\n‚Ä¢ `` -> `...143`\n‚Ä¢ `` -> `...144`\n‚Ä¢ `` -> `...145`\n‚Ä¢ `` -> `...146`\n‚Ä¢ `` -> `...147`\n‚Ä¢ `` -> `...148`\n‚Ä¢ `` -> `...149`\n‚Ä¢ `` -> `...150`\n‚Ä¢ `` -> `...151`\n‚Ä¢ `` -> `...152`\n‚Ä¢ `` -> `...153`\n‚Ä¢ `` -> `...154`\n‚Ä¢ `` -> `...155`\n‚Ä¢ `` -> `...156`\n\n\nI find all the outcome columns and I name them after the variable names of our dataset.\n\nvar_names <- read_excel(\"experiment_2021C(1)copy.xlsx\", n_max = 1) \n\nNew names:\n‚Ä¢ `Q171_First Click` -> `Q171_First Click...46`\n‚Ä¢ `Q171_Last Click` -> `Q171_Last Click...47`\n‚Ä¢ `Q171_Page Submit` -> `Q171_Page Submit...48`\n‚Ä¢ `Q171_Click Count` -> `Q171_Click Count...49`\n‚Ä¢ `Q171_First Click` -> `Q171_First Click...82`\n‚Ä¢ `Q171_Last Click` -> `Q171_Last Click...83`\n‚Ä¢ `Q171_Page Submit` -> `Q171_Page Submit...84`\n‚Ä¢ `Q171_Click Count` -> `Q171_Click Count...85`\n\ncolnames(exp21) <- colnames(var_names)\n\nrm(var_names) \n\ncolumnss <- exp21 %>% \n  select(starts_with(\"Outcome\"), \"Age\", \"Risk\", \"Gender\", \"SC0\") \n\nNow we pivot longer to have all the outcomes in line. This helps to name the correct answers as ‚Äú1‚Äù and the wrong answers as ‚Äú0‚Äù.\n\ncountcol <- columnss %>% \n  pivot_longer(cols = starts_with(\"Outcome\"), names_to= \"draw\", values_to = \"answer\") %>%\n  mutate(count= case_when(answer == \"Yes (bonus payment of $0.10)\" ~ 1,\n         answer == \"No (no bonus payment)\" ~ 0))\n\nAfter we pivot wider back in its first format. This helps us to have the data\n\npilpi <- countcol %>% select(-answer) %>% mutate(id = rep(1:133, each = 20)) %>%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender), names_from = draw, values_from = count)\n\nWe use pivot longer once more in order to have all the observations in one column. Having the observations in one column, is easier for us to group by the id column we created before. So we groupby id. This helps us identify what each person done. In addition, we add a column using mutate in order to have the clear result of each person.\n\npi <- pilpi %>% pivot_longer(cols = starts_with(\"Outcome\"), names_to= \"draw\", values_to = \"answer\") %>% group_by(id) \n\nti<- pi %>% mutate(summm=sum(answer))\n\ntii<- pi %>% mutate(summm=sum(answer)) %>% separate(draw, into = c(\"rolsep\", \"rolnum\"), sep = \"e\") %>% group_by(id)\n\n# boxplot.stats(data$score)$out\n# \n# rr <- ti %>% separate(draw, into = c(\"rolsep\", \"rolnum\"), sep = \"e\") %>% group_by(id, rolnum)\n\nNow we pivot wider once more so the data can be presented in the form that shows how people are doing from the 1st one until the last (133rd).\n\nfinal <- ti %>%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender, summm), names_from = draw, values_from = answer)\n\n# finally <- tii %>%  pivot_wider(id_cols = c(id, Age, SC0, Risk, Gender, summm), names_from = rolnum, values_from = answer)\n# \n#   fin <- final %>% inner_join(ex22, by = \"id\")\n\nWe use the code below to do our Histogram.\n\nggplot(final, aes(summm)) + geom_histogram(bins = 21) + geom_rug()\n\n\n\n\nAnalyze the data by gender, the draw number, and by experimental condition. Are there differences between the genders in terms of the number of wins versus losses? Are there differences between the experimental conditions? Are people more or less likely to win towards the last draws as opposed to the beginning of the experiment?\n‚ÄòFinal‚Äô is the column that already has everything needed for the analysis. The only missing column is conditions. To do that, I used mutate to create a new column named as condition where each real condition is named after a number from one to five. I also create an id column in the ex2 dataset so I can then inner_join it with our main column.\n\nex2 <- exp21 %>%\n         mutate(condition = case_when(Control_Msg != \"\" ~ 1,\n                               Norm_Pos_Msg != \"\" ~ 2,\n                               Norm_Neg_Msg != \"\" ~ 3,\n                               Emp_Neg_Msg != \"\" ~ 4,\n                               Emp_Pos_Msg != \"\" ~ 5),\n                id = 1:133,\n                rolnum = 1:133) \n\nAt this point, I create a new dataset with ‚Äòid‚Äô and ‚Äòcondition‚Äô columns so it is easier to inner_join them. Then I inner_join them so I have everything in one dataset named ‚Äòfin‚Äô.\n\nex22 <- ex2 %>% select(id, condition, rolnum)\n\nfin <- final %>% inner_join(ex22, by = \"id\")\n\nThen we group by Gender and summarise by mean. This allows us to see the mean score that each gender got correct. As we can see, females reported a higher correct score than males on average.\n\nfin %>% group_by(Gender) %>% summarise(mean(summm))\n\n# A tibble: 2 √ó 2\n  Gender `mean(summm)`\n  <chr>          <dbl>\n1 Female          4.75\n2 Male            3.68\n\n# mutate(rrr, coun)\n# fin %>% group_by(condition) %>% summarise(mean(summm))\n\nAnalyzing the data regarding the draw number we use the tii dataset. The reason is because this dataset was created in question 1 with a function named seperate. This function separated the column ‚Äòdraw‚Äô into two others (rolsep, rolsum). The first stands for the title of the draw column while the second one stands for the values that a correct answer was reported.\nTherefore, we here use the filter column(rolnum) equal with the number of observation we want piped with summary(answer). This helps us find the stats behind the correct answers in each round. Also, we named a new dataset ‚Äòi‚Äô where it excludes some irrelevant columns for the purpose of analyzing the data based on draws.\nRegarding the draws we can see that the mean for outcome number 1 is almost 20% (0.1955). That means that one out of five people reported a correct answer in that case. As we can see the mean of correct responses ranges from 0.16 to 0.27. This means that the standard deviation is relatively low. In other words all correct response rates are clustered around the mean.\n\ni <- tii %>% select(-Gender, -Age, -SC0, -id, -Risk, -rolsep, -summm, -id)\n\nAdding missing grouping variables: `id`\n\ni %>% filter(rolnum==1) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==2) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==3) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==4) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==5) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2331  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==6) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==7) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==8) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==9) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==10) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2105  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==11) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==12) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==13) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==14) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2707  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==15) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==16) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==17) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2256  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==18) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2105  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==19) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.218  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==20) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2632  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\n\nRegarding the experimental condition of the data, we use the groupby function to call it in the fin dataset, and then use summarise to find the mean. Alongside, as we set above, each number corresponds in one condition. As we can see, the most popular experimental condition is normative positive message (2) while the least popular is empirical positive message (5).\n\nfin %>% group_by(condition) %>% summarise(mean(summm))\n\n# A tibble: 5 √ó 2\n  condition `mean(summm)`\n      <dbl>         <dbl>\n1         1          5.52\n2         2          6.08\n3         3          4.20\n4         4          3.64\n5         5          3.36\n\n\nRedo part 2 after excluding outliers. Do you still see the different in the experimental groups after outliers are eliminated? Do you see differences between the first and the last rolls? Are there any differences by gender?\n\n# trial <- fin %>% select(-Gender, -Age, -SC0, -Risk, -Outcome1, -Outcome2, -Outcome3, -Outcome4, -Outcome5, -Outcome6, -Outcome7, -Outcome8, -Outcome9, -Outcome10, -Outcome11, -Outcome12, -Outcome13, -Outcome14, -Outcome15, -Outcome16, -Outcome17, -Outcome18, -Outcome19, -Outcome20, -rolnum) %>% \n#   boxplot(trial)\n# \n# boxplot.stats(trial$summm)$out \n\n\nboxplot(fin$summm,\n  ylab = \"summm\")\n\n\n\nboxplot.stats(fin$summm)$out\n\n[1] 10 17 20 11 13 10 11 13\n\nout <- boxplot.stats(fin$summm)$out\nout_ind <- which(fin$summm %in% c(out))\nout_ind\n\n[1]  15  25  48  57  89  98 121 130\n\n# ggplot(fin) +\n#   aes(x = summm) +\n#   geom_histogram(bins = 21, fill = \"#0c4c8a\") +\n#   theme_minimal()\n\nAfter, wecreate a new dataset called ‚Äòdesperate‚Äô where we remove the indicative rows with the outliers. This will help us redo exercise 2\n\ndesperate <- fin[-c(15, 25,48,57,89,98,121,130), ]\n\nRedoing exercise 2 with the new dataframe ‚Äòdesperate‚Äô.\nIn comparison with exercise 2, we can see that the mean for both male and female has fallen. This was a natural outcome to follow since the outliers were all above the mean. However, the outcome remained the same in matters of which gender reported the most correct answers. On average females report 4 correct answers out of twenty, as opposed to men (3/20).\n\ndesperate %>% group_by(Gender) %>% summarise(mean(summm))\n\n# A tibble: 2 √ó 2\n  Gender `mean(summm)`\n  <chr>          <dbl>\n1 Female          4.19\n2 Male            3.14\n\n\nFor the draws, we are creating a new column to make the analysis named ‚Äòabcdefg‚Äô. This is to exclude the outlier rows from the the ‚Äòtii‚Äô dataset we‚Äôve used at exercise 2. After doing that though, we can see that the mean correct response reporting ratio remained at the same levels ranging from 16.54% to 27.07%. This indicates that the outliers did not affect the variability of the data. No difference between the first and last rows.\n\nabcdefg <- tii[-c(15, 25,48,57,89,98,121,130), ]\n\ni <- abcdefg %>% select(-Gender, -Age, -SC0, -id, -Risk, -summm, -rolsep, -id)\n\nAdding missing grouping variables: `id`\n\ni %>% filter(rolnum==1) %>% summary(answer)\n\n       id            rolnum              answer     \n Min.   :  1.00   Length:132         Min.   :0.000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.000  \n Median : 67.50   Mode  :character   Median :0.000  \n Mean   : 67.45                      Mean   :0.197  \n 3rd Qu.:100.25                      3rd Qu.:0.000  \n Max.   :133.00                      Max.   :1.000  \n\ni %>% filter(rolnum==2) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2406  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==3) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==4) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1654  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==5) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.49                      Mean   :0.2348  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==6) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==7) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==8) %>% summary(answer)\n\n       id            rolnum              answer     \n Min.   :  1.00   Length:132         Min.   :0.000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.000  \n Median : 67.50   Mode  :character   Median :0.000  \n Mean   : 67.48                      Mean   :0.197  \n 3rd Qu.:100.25                      3rd Qu.:0.000  \n Max.   :133.00                      Max.   :1.000  \n\ni %>% filter(rolnum==9) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.47                      Mean   :0.2348  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==10) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.45                      Mean   :0.2121  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==11) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.203  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==12) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==13) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.1955  \n 3rd Qu.:100                      3rd Qu.:0.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==14) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2707  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==15) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  2.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.50                      Mean   :0.2576  \n 3rd Qu.:100.25                      3rd Qu.:1.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==16) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2556  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\ni %>% filter(rolnum==17) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.48                      Mean   :0.2273  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==18) %>% summary(answer)\n\n       id            rolnum              answer      \n Min.   :  1.00   Length:132         Min.   :0.0000  \n 1st Qu.: 34.75   Class :character   1st Qu.:0.0000  \n Median : 67.50   Mode  :character   Median :0.0000  \n Mean   : 67.47                      Mean   :0.2045  \n 3rd Qu.:100.25                      3rd Qu.:0.0000  \n Max.   :133.00                      Max.   :1.0000  \n\ni %>% filter(rolnum==19) %>% summary(answer)\n\n       id         rolnum              answer     \n Min.   :  1   Length:133         Min.   :0.000  \n 1st Qu.: 34   Class :character   1st Qu.:0.000  \n Median : 67   Mode  :character   Median :0.000  \n Mean   : 67                      Mean   :0.218  \n 3rd Qu.:100                      3rd Qu.:0.000  \n Max.   :133                      Max.   :1.000  \n\ni %>% filter(rolnum==20) %>% summary(answer)\n\n       id         rolnum              answer      \n Min.   :  1   Length:133         Min.   :0.0000  \n 1st Qu.: 34   Class :character   1st Qu.:0.0000  \n Median : 67   Mode  :character   Median :0.0000  \n Mean   : 67                      Mean   :0.2632  \n 3rd Qu.:100                      3rd Qu.:1.0000  \n Max.   :133                      Max.   :1.0000  \n\n\nIn the case of experimental conditions we can see that there are some significant differences. The range between them became smaller (3.36 to 4.69). In other words the data became more disperse. While there is no difference in the lower end, there is a difference in the higher end of the range. This indicates that the previous means were inflated due to the high outliers. For insance, the most popular experimental condition, normative positive message (2), was 6.08, whereas now is just 4.33.\n\ndesperate %>% group_by(condition) %>% summarise(mean(summm))\n\n# A tibble: 5 √ó 2\n  condition `mean(summm)`\n      <dbl>         <dbl>\n1         1          4.70\n2         2          4.33\n3         3          3.74\n4         4          3.43\n5         5          3.36\n\n\nSplit your sample into the \"younger\" half and the \"older half. Are there differences between the two age groups?\nFirst we sort the date from highest to lowest reported age using the order function.\n\n\nordered<- fin[order(fin$Age, decreasing = TRUE), ]\n\nThen we find the Age median from fin dataset. As we can see, the median is 24 (in the column 67 since is exactly the middle of 133).\n\nmedian(fin$Age)\n\n[1] 24\n\n\nSince I have the ordered dataframe (from the highest to lowest Age), I use it to create two other dataframes, one for the younger Age group ‚Äúorderedyoung‚Äù, and one for the older Age group ‚Äúorderedold‚Äù.\n\norderedyoung <- ordered[-c(1:66), ]\norderedold <- ordered[-c(68:133), ]\n\norderedold%>% ggplot(aes(x= Age, y= summm)) + geom_point()\n\nboxplot(orderedold$Age,\n  ylab = \"Age\")\n\n\n\nboxplot.stats(orderedold$Age)$out\n\n[1] 56 51 46 35\n\nout <- boxplot.stats(orderedold$Age)$out\nout_ind <- which(orderedold$Age %in% c(out))\nout_ind\n\n[1] 1 2 3 4\n\n\n\nboxplot(orderedyoung$Age,\n  ylab = \"Age\")\n\n\n\nboxplot.stats(orderedyoung$Age)$out\n\n[1] 20\n\nout <- boxplot.stats(orderedyoung$Age)$out\nout_ind <- which(orderedyoung$Age %in% c(out))\nout_ind\n\n[1] 67\n\n\n\noo <- orderedold[-c(1,2,3,4), ]\n\noy <- orderedyoung[-c(67), ]\n\nI use the ggplot function to create a graph that indicates the Age of the participants in the x axes, and the number of correct guesses (summm) in the y axes. As we can see, while the Age column now indicates only the oldest people, the majority is still under 30 ‚Äì closer to the median. From the age 24 until 30s, the amount of correct responses seems to decline. We use the median and the mean code in order to see if there is a difference with them. Median indicates that there is no difference among the data as the reported correct outcome. However, if we look at the mean of the two, the younger group tends to report more correct answers on average than the older group. This difference is small though (4.62-3.92)\n\noo%>% ggplot(aes(x= Age, y= summm)) + geom_point()\n\n\n\nmedian(oo$summm)\n\n[1] 4\n\nmean(oo$summm)\n\n[1] 3.920635\n\noy%>% ggplot(aes(x= Age, y= summm)) + geom_point()\n\n\n\nmedian(oy$summm)\n\n[1] 4\n\nmean(oy$summm)\n\n[1] 4.621212\n\n\n\nPART TWO\nLet's study how the number of wins depends on certain factors:\n\nlibrary(readxl)\nexp21 <- read_excel(\"experiment_2021C(1)copy.xlsx\", col_names = FALSE,\n    skip = 2)\n\nNew names:\n‚Ä¢ `` -> `...1`\n‚Ä¢ `` -> `...2`\n‚Ä¢ `` -> `...3`\n‚Ä¢ `` -> `...4`\n‚Ä¢ `` -> `...5`\n‚Ä¢ `` -> `...6`\n‚Ä¢ `` -> `...7`\n‚Ä¢ `` -> `...8`\n‚Ä¢ `` -> `...9`\n‚Ä¢ `` -> `...10`\n‚Ä¢ `` -> `...11`\n‚Ä¢ `` -> `...12`\n‚Ä¢ `` -> `...13`\n‚Ä¢ `` -> `...14`\n‚Ä¢ `` -> `...15`\n‚Ä¢ `` -> `...16`\n‚Ä¢ `` -> `...17`\n‚Ä¢ `` -> `...18`\n‚Ä¢ `` -> `...19`\n‚Ä¢ `` -> `...20`\n‚Ä¢ `` -> `...21`\n‚Ä¢ `` -> `...22`\n‚Ä¢ `` -> `...23`\n‚Ä¢ `` -> `...24`\n‚Ä¢ `` -> `...25`\n‚Ä¢ `` -> `...26`\n‚Ä¢ `` -> `...27`\n‚Ä¢ `` -> `...28`\n‚Ä¢ `` -> `...29`\n‚Ä¢ `` -> `...30`\n‚Ä¢ `` -> `...31`\n‚Ä¢ `` -> `...32`\n‚Ä¢ `` -> `...33`\n‚Ä¢ `` -> `...34`\n‚Ä¢ `` -> `...35`\n‚Ä¢ `` -> `...36`\n‚Ä¢ `` -> `...37`\n‚Ä¢ `` -> `...38`\n‚Ä¢ `` -> `...39`\n‚Ä¢ `` -> `...40`\n‚Ä¢ `` -> `...41`\n‚Ä¢ `` -> `...42`\n‚Ä¢ `` -> `...43`\n‚Ä¢ `` -> `...44`\n‚Ä¢ `` -> `...45`\n‚Ä¢ `` -> `...46`\n‚Ä¢ `` -> `...47`\n‚Ä¢ `` -> `...48`\n‚Ä¢ `` -> `...49`\n‚Ä¢ `` -> `...50`\n‚Ä¢ `` -> `...51`\n‚Ä¢ `` -> `...52`\n‚Ä¢ `` -> `...53`\n‚Ä¢ `` -> `...54`\n‚Ä¢ `` -> `...55`\n‚Ä¢ `` -> `...56`\n‚Ä¢ `` -> `...57`\n‚Ä¢ `` -> `...58`\n‚Ä¢ `` -> `...59`\n‚Ä¢ `` -> `...60`\n‚Ä¢ `` -> `...61`\n‚Ä¢ `` -> `...62`\n‚Ä¢ `` -> `...63`\n‚Ä¢ `` -> `...64`\n‚Ä¢ `` -> `...65`\n‚Ä¢ `` -> `...66`\n‚Ä¢ `` -> `...67`\n‚Ä¢ `` -> `...68`\n‚Ä¢ `` -> `...69`\n‚Ä¢ `` -> `...70`\n‚Ä¢ `` -> `...71`\n‚Ä¢ `` -> `...72`\n‚Ä¢ `` -> `...73`\n‚Ä¢ `` -> `...74`\n‚Ä¢ `` -> `...75`\n‚Ä¢ `` -> `...76`\n‚Ä¢ `` -> `...77`\n‚Ä¢ `` -> `...78`\n‚Ä¢ `` -> `...79`\n‚Ä¢ `` -> `...80`\n‚Ä¢ `` -> `...81`\n‚Ä¢ `` -> `...82`\n‚Ä¢ `` -> `...83`\n‚Ä¢ `` -> `...84`\n‚Ä¢ `` -> `...85`\n‚Ä¢ `` -> `...86`\n‚Ä¢ `` -> `...87`\n‚Ä¢ `` -> `...88`\n‚Ä¢ `` -> `...89`\n‚Ä¢ `` -> `...90`\n‚Ä¢ `` -> `...91`\n‚Ä¢ `` -> `...92`\n‚Ä¢ `` -> `...93`\n‚Ä¢ `` -> `...94`\n‚Ä¢ `` -> `...95`\n‚Ä¢ `` -> `...96`\n‚Ä¢ `` -> `...97`\n‚Ä¢ `` -> `...98`\n‚Ä¢ `` -> `...99`\n‚Ä¢ `` -> `...100`\n‚Ä¢ `` -> `...101`\n‚Ä¢ `` -> `...102`\n‚Ä¢ `` -> `...103`\n‚Ä¢ `` -> `...104`\n‚Ä¢ `` -> `...105`\n‚Ä¢ `` -> `...106`\n‚Ä¢ `` -> `...107`\n‚Ä¢ `` -> `...108`\n‚Ä¢ `` -> `...109`\n‚Ä¢ `` -> `...110`\n‚Ä¢ `` -> `...111`\n‚Ä¢ `` -> `...112`\n‚Ä¢ `` -> `...113`\n‚Ä¢ `` -> `...114`\n‚Ä¢ `` -> `...115`\n‚Ä¢ `` -> `...116`\n‚Ä¢ `` -> `...117`\n‚Ä¢ `` -> `...118`\n‚Ä¢ `` -> `...119`\n‚Ä¢ `` -> `...120`\n‚Ä¢ `` -> `...121`\n‚Ä¢ `` -> `...122`\n‚Ä¢ `` -> `...123`\n‚Ä¢ `` -> `...124`\n‚Ä¢ `` -> `...125`\n‚Ä¢ `` -> `...126`\n‚Ä¢ `` -> `...127`\n‚Ä¢ `` -> `...128`\n‚Ä¢ `` -> `...129`\n‚Ä¢ `` -> `...130`\n‚Ä¢ `` -> `...131`\n‚Ä¢ `` -> `...132`\n‚Ä¢ `` -> `...133`\n‚Ä¢ `` -> `...134`\n‚Ä¢ `` -> `...135`\n‚Ä¢ `` -> `...136`\n‚Ä¢ `` -> `...137`\n‚Ä¢ `` -> `...138`\n‚Ä¢ `` -> `...139`\n‚Ä¢ `` -> `...140`\n‚Ä¢ `` -> `...141`\n‚Ä¢ `` -> `...142`\n‚Ä¢ `` -> `...143`\n‚Ä¢ `` -> `...144`\n‚Ä¢ `` -> `...145`\n‚Ä¢ `` -> `...146`\n‚Ä¢ `` -> `...147`\n‚Ä¢ `` -> `...148`\n‚Ä¢ `` -> `...149`\n‚Ä¢ `` -> `...150`\n‚Ä¢ `` -> `...151`\n‚Ä¢ `` -> `...152`\n‚Ä¢ `` -> `...153`\n‚Ä¢ `` -> `...154`\n‚Ä¢ `` -> `...155`\n‚Ä¢ `` -> `...156`\n\n    var_names <- read_excel(\"experiment_2021C(1)copy.xlsx\", n_max = 1) \n\nNew names:\n‚Ä¢ `Q171_First Click` -> `Q171_First Click...46`\n‚Ä¢ `Q171_Last Click` -> `Q171_Last Click...47`\n‚Ä¢ `Q171_Page Submit` -> `Q171_Page Submit...48`\n‚Ä¢ `Q171_Click Count` -> `Q171_Click Count...49`\n‚Ä¢ `Q171_First Click` -> `Q171_First Click...82`\n‚Ä¢ `Q171_Last Click` -> `Q171_Last Click...83`\n‚Ä¢ `Q171_Page Submit` -> `Q171_Page Submit...84`\n‚Ä¢ `Q171_Click Count` -> `Q171_Click Count...85`\n\n    colnames(exp21) <- colnames(var_names)\n\n    rm(var_names) \n\n    columnss <- exp21 %>% \n      select(starts_with(\"Outcome\"), \"Age\", \"Risk\", \"Gender\", \"SC0\") \n\nnormal numbers, the more * the more signif\n\ndatac <- columnss %>% mutate(score = SC0 *10)\n\ndatacool <- datac %>% select(-Outcome1, -Outcome2, -Outcome3, -Outcome4, -Outcome5, -Outcome6, -Outcome7, -Outcome8, -Outcome9, -Outcome10, -Outcome11, -Outcome12, -Outcome13, -Outcome14, -Outcome15, -Outcome16, -Outcome17, -Outcome18, -Outcome19, -Outcome20, -SC0)\n\ndataex1<- datacool %>% select(-Risk, - Age)\n\n\noptions(scipen = 100)\n\n1. Examine the relationship between gender and the number of wins using regression analysis.\nindependent variable x = the cause explanatory, dependent variable y = outcome\nNumber of wins is referred to as the Score in our data, and it is a continuous dependent variable(=y).\nGender is a categorical variable and is the independent variable(=x).\nOn the first chunk I create one new column name ‚ÄòGendering‚Äô where I assign values 0 and 1 to female and male correspondingly.\nOn the second and third chunks, I do the linear regression between Gender(x) and Score - aka number of wins - (y).\n\ndatacool1 <- datacool %>% mutate(Gendering = case_when(Gender == \"Male\" ~ \"1\",\n                                        Gender== \"Female\" ~ \"0\"))\n\ndataex1 %>% summary(lm(score ~ factor(Gendering), data = datacool1))\n\n    Gender              score       \n Length:133         Min.   : 0.000  \n Class :character   1st Qu.: 3.000  \n Mode  :character   Median : 4.000  \n                    Mean   : 4.398  \n                    3rd Qu.: 5.000  \n                    Max.   :20.000  \n\nmodel<- lm(formula= score~Gendering, data= datacool1)\n(summary(model))\n\n\nCall:\nlm(formula = score ~ Gendering, data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7528 -1.6818 -0.7528  0.3182 16.3182 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)   4.7528     0.3088  15.389 <0.0000000000000002 ***\nGendering1   -1.0710     0.5370  -1.995              0.0482 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.914 on 131 degrees of freedom\nMultiple R-squared:  0.02947,   Adjusted R-squared:  0.02206 \nF-statistic: 3.978 on 1 and 131 DF,  p-value: 0.04817\n\n\nLooking at the results from the intercept and the Gendering column (1 = given that is a male), we can see that the regression equation is the following:\ny=-1.07x+4.75\na) What is the effect (the slope) of gender?\nTo see the effect we look to the slope of x. It indicates that on average males win one time less than females.\nb) How strong is the predictive power of gender?\nLooking at the multiple R square we can see that the effect of gender explains 2,947% of the variability of the score.\nc) What are the predicted outcomes for men and women?\nFor this I substitute the values for male and female on the regression equation. For male x=1, for female x=0. After doing that, we will have a result that demonstrates the predicted score for females and males given that our initial assumption holds true.\n\n#for male\nmalepred <- -1.07*1+4.75\n\n#for female\nfemalepred <- -1.07*0+4.75\n\nThe predicted score for male on average is 3.68, while the equivalent predicted score for female is 4.75.\n2. Examine the relationship between age and the number of wins using regression analysis.\n\nmodel1<- lm(formula = score~Age, data= datacool1)\nsummary(model1)\n\n\nCall:\nlm(formula = score ~ Age, data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4383 -1.4232 -0.4080  0.6524 15.4559 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  4.01507    1.36266   2.946  0.00381 **\nAge          0.01511    0.05276   0.287  0.77495   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.957 on 131 degrees of freedom\nMultiple R-squared:  0.0006262, Adjusted R-squared:  -0.007003 \nF-statistic: 0.08208 on 1 and 131 DF,  p-value: 0.7749\n\n\nAfter following the same procedure as we followed in exercise 1, we have the equivalent regression results for Age as independent variable x, and score as dependent variable y. The equation is as follows:\ny=0.015x+4.01\nmultiple R-squared, how much of the score variability is explained assuming that there is an effect of age.\na) What is the effect of age?\nThe effect (aka slope) of age is 0.015. This implies that there is an observed higher score of 0.015 with people that are older. Put differerently, the age factor has a negligible result to the score as is almost zero.\nb) How strong is the predictive power of age?\nLooking at the multiple R square we can see that the effect of age has almost zero explanation for the variability of the score. In other words, the predictive power of age on the time of victories is very weak.\nc) What's the predicted outcome for a 20 year old person? For a 40 year old person?\n\n#for 20 year old\ntwentypred <- 0.015*20+4.01\n\n#for 40 year old\nfourtypred <- 0.015*40+4.01\n\nFor this I substitute the values for 20year old and 40year old on the regression equation. For 20year x=20, for 40year old x=40. After doing that, we will have a result that demonstrates the predicted score for 20year old and 40year old given that our initial assumption holds true.\nThe predicted score for 20year old on average is 4.31, while the equivalent predicted score for 40year old is 4.61.\n3. Examine the relationship between age and gender combined.\nwhy the numbers are changing slightly (=men and women have different ages. when u plug both coefficients together it tries to)\na) What are the effects now? How do they compare with the results in 1 and 2? Explain the differences\n\nmodel1<- lm(formula = score~Age+factor(Gendering), data= datacool1)\nsummary(model1)\n\n\nCall:\nlm(formula = score ~ Age + factor(Gendering), data = datacool1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.859 -1.673 -0.673  0.401 16.082 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)   \n(Intercept)         4.08770    1.34714   3.034  0.00291 **\nAge                 0.02660    0.05244   0.507  0.61280   \nfactor(Gendering)1 -1.10062    0.54165  -2.032  0.04419 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.922 on 130 degrees of freedom\nMultiple R-squared:  0.03139,   Adjusted R-squared:  0.01649 \nF-statistic: 2.107 on 2 and 130 DF,  p-value: 0.1258\n\n\nNow the equation becomes:\ny=0.02x1-1.1x2+4.08\nwith x1 being the age, and x2 being the gender given is a male (if that one is zero then is a female)\nAs we can see here, while the age effect is still close to zero, now it is a bit higher (0.026 as opposed to 0.015); however it still remains a weak factor. With respect to Gender, now is also higher ton the other end than its corresponding previous result (-1.10 as opposed to -1.07).\nConsequently, there is a very slight inter correlation between the two factors as the model tends to become slightly stronger. How ever the differences are minimal. This is indicated also in the adjusted R squared (=0.016) which is even lower than R squared. This means that at least one of the two variables does not explain the dependent variable score. As we can see from the exercises 1 and 2, this variable is Age because its adjusted R squared is negative when we have Age as the only independent variable (see exercise 2).\nThen we proceed to an interaction coefficient just to observe the cross sections between our independent variables. Despite that the numbers are changing a bit though interaction coefficient is not necessary for the purpose of this exercise.\n\ncool3 <- lm(score~ Age * factor(Gendering), data = datacool1)\nsummary(cool3)\n\n\nCall:\nlm(formula = score ~ Age * factor(Gendering), data = datacool1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8912 -1.6606 -0.6606  0.6527 15.5740 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)   \n(Intercept)             5.90581    1.99661   2.958  0.00369 **\nAge                    -0.04612    0.07890  -0.585  0.55989   \nfactor(Gendering)1     -4.41097    2.74144  -1.609  0.11006   \nAge:factor(Gendering)1  0.12987    0.10544   1.232  0.22030   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.916 on 129 degrees of freedom\nMultiple R-squared:  0.04265,   Adjusted R-squared:  0.02039 \nF-statistic: 1.916 on 3 and 129 DF,  p-value: 0.1303\n\n\nWe observe that the estimations between age and gender are not significant.\nb) How strong is the predictive power of this model?\nLooking at the adjusted R square we can see that the effect of age combined with the effect of gender explains 1,6% the variability of the score - when doing multiple regression, adjusted R squared is a better indication to look at for the predictive power of the models. And in conclusion, this one does not have a strong predictive power.\nc) What is the predicted outcome for a 20 year old man? 20 year old woman? 40 year old man? 40 year old woman?\nFollowing similar procedure as the exercise 1 and 2 c, we substitute the age and gender factors in the equation for the equivalent results we need in the four different instances:\n\n#for 20year old man\ntwentyman <- 0.02*20-1.1*1+4.08\n\n#for 20year old woman\ntwentywoman <- 0.02*20-1.1*0+4.08\n\n#for 40year old man\nfourtyman <- 0.02*40-1.1*1+4.08\n\n#for 40year old woman\nfourtywoman <- 0.02*40-1.1*0+4.08\n\nAs we can see, the predicted scores are the following:\nfor 20year old man = 3.38\nfor 20year old woman = 4.48\nfor 40year old man = 3.78\nfor 40year old woman = 4.88"
  },
  {
    "objectID": "software/sitcoms.html",
    "href": "software/sitcoms.html",
    "title": "Sitcoms",
    "section": "",
    "text": "Let's continue working with the Google Trends data you obtained for Homework 02. Homework 3 starts from the line 180 and onward.\n\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.1 ‚îÄ‚îÄ\n\n\n‚úî ggplot2 3.3.5      ‚úî purrr   0.3.4 \n‚úî tibble  3.1.6      ‚úî dplyr   1.0.10\n‚úî tidyr   1.2.1      ‚úî stringr 1.4.0 \n‚úî readr   2.0.1      ‚úî forcats 0.5.1 \n\n\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(broom)\nlibrary(ggthemes)\nlibrary(ggsci)\n\n\nCode to load the data into R and prepare it for the analysis. You need to correctly specify data types and choose concise variable names.\n\n#Import data for the US\nuscoms <- read_csv(\"uscoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (United States), South Park: (United States), The Simps...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for the world\nworldcoms <- read_csv(\"worldcoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Worldwide), South Park: (Worldwide), The Simpsons: (Wo...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Import data for Cyprus\ncycoms <- read_csv(\"cycoms.csv\", skip = 1)\n\nRows: 217 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): Month\ndbl (3): Family Guy: (Cyprus), South Park: (Cyprus), The Simpsons: (Cyprus)\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nprepare the data for analysis\n\n#NOW WE CLEAN AND PREPARE THE DATA FOR ANALYSIS\n#Clean for the US.\nuscoms_clean <- uscoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (United States)`,\n         southpark = `South Park: (United States)`,\n         simpsons = `The Simpsons: (United States)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n  mutate(day = 15, .after = month) %>%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Worldwide\nworldcoms_clean <- worldcoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (Worldwide)`,\n         southpark = `South Park: (Worldwide)`,\n         simpsons = `The Simpsons: (Worldwide)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n  mutate(day = 15, .after = month) %>%\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\n#Clean for Cyprus.\ncycoms_clean <- cycoms %>%\n  rename(month = Month,\n         famguy = `Family Guy: (Cyprus)`,\n         southpark = `South Park: (Cyprus)`,\n         simpsons = `The Simpsons: (Cyprus)`) %>% \n  mutate_if(is.character, str_replace, pattern = \"<\", replacement = \"\") %>% \n  mutate_at(c(\"famguy\", \"southpark\", \"simpsons\"), as.numeric) %>% \n\n  #I separate the month and  year into two columns. Then I convert the character column to a number.\n  separate(month, into = c(\"year\", \"month\"), sep = \"-\", convert = TRUE) %>%\n\n  #I create a new column that is called day, and I use 15 as it is the middle of the month.\n  mutate(day = 15, .after = month) %>%\n\n  #Here I create a date column using the ymd (=year month day)function.\n  mutate(date = ymd(paste(year, month, day, sep=\"-\")))\n\nCode that will calculate the average popularity of the terms by year for each of the search terms in each of the geographies.\n\n#HERE WE SUMMARIZE THE AVERAGE POPULARITY OF THE TERMS BY YEAR OF EACH SEARCH\n#Summarize for the US\nuscoms_summary <- uscoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Worldwide\nworldcoms_summary <- worldcoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n#Summarize for Cyprus\ncycoms_summary <- cycoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\n\nBefore we analyse the data, we tidy them. This way, we have three rows that correspond for each month from 2004 until 2021. This makes easier for us to specify what goes where.\n\nuscoms_tidy <- uscoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworldcoms_tidy <- worldcoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\ncycoms_tidy <- cycoms_clean %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\n4. Analyze the data to answer the following questions:\n\nIn what year was each term most popular? In which geography is each of the terms most popular in the year you found in Part A?\n\nuscoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\nworldcoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line()+ scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\n\ncycoms_tidy %>% \n  ggplot(aes(x = date, y = score, color = name)) +\n  geom_line() + scale_x_date(date_breaks = \"2 year\", date_labels = \"%Y\")\n\n\n\n\nBased on the graphs the most popular year for\n‚ÄìFamily guy was: 2nd half of 2008 (USA)/ 1st half of 2009(Worldwide)/ 1st, 2nd half of 2007 (Cyprus)\n‚ÄìSimpsons was: mid-2007 (USA,Worldwide)/ 2nd half of 2008 (Cyprus)\n‚ÄìSouth Park was: 1st half of 2010 (USA, Worldwide) / first half of 2004 (Cyprus)\nCalculate the ratio of the most popular term to the least popular term. Describe how this ratio changed over time in each of the geographies by relying on yearly data.\n\n\n#I hereby summarize in three different ways the mean search score of each series.\nworldcoms_clean %>% summarize(mean(southpark), mean(simpsons), mean(famguy))\n\n# A tibble: 1 √ó 3\n  `mean(southpark)` `mean(simpsons)` `mean(famguy)`\n              <dbl>            <dbl>          <dbl>\n1              22.2             36.2           20.3\n\nworldcoms_clean %>% summarize_at(vars(southpark, simpsons, famguy), mean)\n\n# A tibble: 1 √ó 3\n  southpark simpsons famguy\n      <dbl>    <dbl>  <dbl>\n1      22.2     36.2   20.3\n\nsummary(worldcoms_clean)\n\n      year          month             day         famguy        southpark    \n Min.   :2004   Min.   : 1.000   Min.   :15   Min.   : 5.00   Min.   : 7.00  \n 1st Qu.:2008   1st Qu.: 3.000   1st Qu.:15   1st Qu.:12.00   1st Qu.:11.00  \n Median :2013   Median : 6.000   Median :15   Median :19.00   Median :21.00  \n Mean   :2013   Mean   : 6.475   Mean   :15   Mean   :20.29   Mean   :22.22  \n 3rd Qu.:2017   3rd Qu.: 9.000   3rd Qu.:15   3rd Qu.:29.00   3rd Qu.:29.00  \n Max.   :2022   Max.   :12.000   Max.   :15   Max.   :45.00   Max.   :69.00  \n    simpsons           date           \n Min.   : 13.00   Min.   :2004-01-15  \n 1st Qu.: 23.00   1st Qu.:2008-07-15  \n Median : 37.00   Median :2013-01-15  \n Mean   : 36.18   Mean   :2013-01-13  \n 3rd Qu.: 47.00   3rd Qu.:2017-07-15  \n Max.   :100.00   Max.   :2022-01-15  \n\n#Looking at the results we conclude that the simpsons on average was the most popular by far with 36.18 score. Then southpark follows with 22.22 and then family guy follows with 20.29.\n\nThen we group the averages by year so we can compare the yearly differences.\n\nworld_summary <- worldcoms_clean %>% \n  group_by(year) %>% \n  summarize_at(c(\"famguy\", \"southpark\", \"simpsons\"), mean)\n\nworld_tidy <- world_summary %>%\n  pivot_longer(cols = c(\"famguy\", \"southpark\", \"simpsons\"),\n               names_to = \"name\",\n               values_to = \"score\")\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() \n\n\n\n\n\nHOMEWORK 3 STARTS HERE\n\nPrepare the graph showing the trends in popularity of the search terms over time. Make sure to add a descriptive title, label the axes, and modify the look of the graph to be presentable. You can choose your own colors or use one of the palettes we talked about in class. Add a one-sentence explanation for the geometry you selected for the graph.\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\") +\n  scale_color_discrete(labels = c(\"Family Guy\", \"The Simpsons\", \"South Park\")) #rename titles\n\n\n\n\nThe same code as before is used to make the graph. The only change here is that we rename the names of the shows in the graph by using the command ‚Äúscale_color_discrete(labels = c(‚Ä¶)‚Äù.\n\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  geom_line() +\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"Popularity graph\",\n       subtitle = \"Top 3 American sitcoms\"\n       ) +\n  scale_color_discrete(name = \"Shows:\",\n                       labels = c(\"South Park\", \"The Simpsons\", \"Family Guy\")) +\n  theme_wsj()  #this theme makes the graph more presentable\n\n\n\n\nThis is a more presentable version of the graph above.\n\nSmooth the data (using a graph) to eliminate random noise. Explain which smoothing method you chose and why.\n\n\nworld_tidy %>% \n  ggplot(aes(x = year, y = score, color = name)) +\n  #geom_line() + #I remove this so we can see the variability of each sitcom.\n  geom_smooth(method = \"loess\") + #This command creates a moving average that smoothes out all the fluctuations.\n  labs(x = \"Year\",\n       y = \"Popularity\",\n       title = \"World Popularity of the main American cartoon sitcoms - Google Trends data\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFor this code we use LOESS command to smooth the fluctuations. LOESS (aka locally weighted smoothing), helps us see the relationship between the three variables and foresee trends. A LOESS smoother takes the data, fits a regression with the subset of data, and uses that linear regression model to get a point for the smoothed curve. The points closer to the fitted line are more impact-full.\n\nCreate a chart to show seasonality by month in your data. What does the chart tell you about the seasonality of your chosen search terms? Some examples of seasonality charts can be found here: https://www.r-graph-gallery.com/142-basic-radar-chart.html (Links to an external site.)\n\n\nworldcoms_tidy %>% \n  group_by(month, name) %>% #I group by month to see which season are popular. By name to see the shows.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score, fill = name)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  coord_polar()\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n\n\n\nAs we can see, the Simpsons seasonality graph looks the same. That is probably we have no limits to the popularity score of the variable and every score is close to each other. i.e.¬†popularity does not vary by a lot.\n\nworldcoms_tidy %>% \n  filter(name == \"simpsons\") %>% #I filter by name \"simpsons\" so I can examine the Simpsons show by itself.\n  group_by(month) %>% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(30, 40)) + #I include score limits from 30 to 40 in order to see the exact seasons where \"The simpsons\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col).\n\n\n\n\n\n\nworldcoms_tidy %>% \n  filter(name == \"famguy\") %>% #I filter by name \"famguy\" so I can examine the \"Family Guy\" show by itself.\n  group_by(month) %>% #I group by month again to see which month is more popular.\n  summarize(mean_score = mean(score)) %>% \n  ggplot(aes(x = month, y = mean_score)) +\n  geom_col(position = \"dodge\", alpha = 0) +\n  geom_point() +\n  geom_polygon(alpha = 0.2) + \n  scale_x_continuous(breaks = 1:12, labels =  month.abb[1:12]) + \n  scale_y_continuous(limits = c(15, 25)) + #I include score limits from 15 to 25 in order to see the exact seasons where \"The Family Guy\" peaked and scored low in popularity.\n  coord_polar()\n\nWarning: Removed 12 rows containing missing values (geom_col)."
  },
  {
    "objectID": "software/presidents_analysis.html",
    "href": "software/presidents_analysis.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Refer to the state of the union addresses made by US presidents since WWII. To simplify the task, only looks at the first address for each president. (You can find the archive here: https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union (Links to an external site.))\nWe ask BeautifulSoup to locate a table on the page, scan through the rows of this table, and then get the first link on each of the rows."
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-of-all-presidents-words-since-truman",
    "href": "software/presidents_analysis.html#interpretation-of-all-presidents-words-since-truman",
    "title": "Loizos Konstantinou",
    "section": "Interpretation of all president‚Äôs words since Truman",
    "text": "Interpretation of all president‚Äôs words since Truman\nAmerican priorities shifted over time. As we can see, From Hoover (1929) until Nixon (1974) issues and words related to ‚Äúfreedom‚Äù, and ‚Äúpeace‚Äù were emphasized. This makes sense since during that time, WW2 and the Vietnam War were fought.\n‚ÄúEconomy‚Äù seems like a topic that is popular in almost every presidency.\nDuring Ford‚Äôs and Reagan‚Äôs presidency, ‚Äútax‚Äù, and ‚Äúgrowth‚Äù became really hot buzz words. Especially during Reagan administration, big tax reforms were introduced which they have significantly reduce taxes for businesses.\n‚Äúinflation‚Äù and ‚Äúenergy‚Äù were also popular during Nixon, Ford, and Reagan. It is important because especially during Nixon, the US economy after 14 years of economic development got in a stagflationary state; oil and gas crisis at the 70s was also a part of that.\nDuring George W Bush (2005), ‚Äúsecurity‚Äù was the most popular word used. This is because especially after 911, security became the main focus of his presidency.\nHealthcare gained importance during the Clinton Administration, and two administrations later, the Obama Administration expanded Medicaid. With the Covid-19 pandemic, healthcare again dominated the policy priorities in Biden‚Äôs 2022 address.\nAll the presidents, irrespective of their political affiliation (Democrats vs Republicans), mentioned about strengthening / growing the economy. Only presidents affiliated with the Democratic party seemed to emphasize on ‚Äúhealthcare‚Äù, whereas a common theme among the Republican presidents‚Äô addresses was ‚Äúwar / military spending / terrorism‚Äù.\nTop ten words of all Democrat Presidents\n\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\ndemocrat_speeches = pd.read_excel('democrat_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in democrat_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\ndemocrat_speeches['top ten'] = top_list\n\ndisplay(democrat_speeches)\n\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('industrial', 9),('work', 8),('recovery', 7)...\n    \n    \n      1\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('prosperity', 12),('production', 12),('power...\n    \n    \n      2\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('economic', 16),('development', 10),('peace'...\n    \n    \n      3\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('freedom', 12),('life', 9),('progress', 8),(...\n    \n    \n      4\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      [('inflation', 17),('economic', 14),('tax', 13...\n    \n    \n      5\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('children', 24),('work', 21),('budget', 17),...\n    \n    \n      6\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('jobs', 32),('work', 20),('energy', 18),('fa...\n    \n    \n      7\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('folks', 19),('see', 15),('families', 15),('...\n    \n  \n\n\n\n\nTop ten words of all Republican Presidents\n\n#Previous speech links were not scraped so we import the excel that includes them here for the analysis\nrepublican_speeches = pd.read_excel('republican_speeches.xlsx')\n\n#I use a function and include the code that want to apply for every president.\ndef my_function(x):\n    r = requests.get(x['html'])\n    r.encoding = 'utf-8' \n    html = r.text \n    #print(html[0:2000]) \n\n    soup = BeautifulSoup(html, \"html.parser\")\n    president_text = soup.get_text()\n    #print(president_text[0:2000])\n\n    #THE DOCUMENT IS TOKENIZED -- SPLITTING TEXT INTO INDIVUDUAL WORDS\n    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n    tokens = tokenizer.tokenize(president_text) \n    #print(tokens[0:10])\n\n    #Defining stopwords and adding more to the list. This list is same across all 14 presidents' speeches. \n    sw = nltk.corpus.stopwords.words('english') \n    newsw = ['annual', 'number', 'help', 'thank', 'get',  'going', 'think', 'look', 'said', \n             'create', 'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', \n             'long', 'better', 'plan', 'national', 'ask' '10', 'much', 'good', 'great', \n             'best', 'cannot', 'still', 'know', 'years', '1', 'major', 'want', 'able', 'put', \n             'capacity', 'programs', 'per', 'percent', 'million', 'act', 'provide', 'afford', \n             'needed', 'may', 'possible', 'full', '2', 'effort', 'meeting', 'address', 'ever', \n             'measures', 'ago', 'delivered', '5', 'program', 'past', 'future', 'need', 'needs', \n             'house', 'also', 'tonight', 'propose', 'toward', 'continue', 'society','country', \n             'seek', 'period', 'year', 'man', 'men', 'one', 'areas', 'begin', 'live', 'make', \n             'let', 'upon', 'well', 'office', 'meet', 'make' 'citizens', 'human', 'self', 'among', \n             'peoples', 'affairs', 'would', 'field', 'first', 'interest', 'today', 'recommendations', \n             'recomenndation', 'within', 'shall', 'administration', 'nation', 'nations', 'us', 'we', \n             'policy', 'legislation', 'time', 'new', 'many', 'several', 'few', 'government', 'world', \n             'people', 'united', 'states', 'system', 'every', 'people', 'must', '626','give', \n             'categories', '226762', '17608', '24532', '430', '38','statistics', 'analyses', \n             'miscellaneous', 'congressional', 'skip', 'content', 'documents', 'attributes', 'media', 'message', \n             'congress', 'state', 'union', 'america', 'american', 'americans', 'presidency', 'president', \n             'project', 'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', \n             'main', 'take','like','yet','j','000', 'ask', '1974', 'federal', 'http', 'www', 'usb',\n             'edu', 'the', 'the', 'before', 'joint', 'session', 'the','american', 'america',\n            'year','congress','let','time', 'nation', 'new', 'people']\n    sw.extend(newsw)\n    #print(sw[0:30])\n\n    president_words = [token.lower() for token in tokens] \n    words_ns = [word for word in president_words if word not in sw] \n    #print(words_ns[:20])\n    president_ns = \" \".join(words_ns)\n\n    #Determining the most commoon words \n    count = Counter(words_ns)\n    top_ten_president = count.most_common(10)\n    top_10_string = ','.join([str(x) for x in top_ten_president])\n    print_list = [top_10_string]\n    return print_list\n\n#for i in range(13):\n# #    fun(df2)\ntop_list = []\nfor index, row in republican_speeches.iterrows():\n    top = my_function(row)\n    top_list.append(top)\n\nrepublican_speeches['top ten'] = top_list\n\ndisplay(republican_speeches)\n\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('public', 33),('law', 25),('service', 23),('...\n    \n    \n      1\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('free', 16),('security', 16),('economy', 12)...\n    \n    \n      2\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('peace', 27),('energy', 17),('war', 8),('pro...\n    \n    \n      3\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('energy', 25),('oil', 20),('tax', 17),('econ...\n    \n    \n      4\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('freedom', 20),('tax', 16),('growth', 14),('...\n    \n    \n      5\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('budget', 17),('work', 12),('hope', 10),('dr...\n    \n    \n      6\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('security', 29),('freedom', 20),('social', 1...\n    \n    \n      7\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('tax', 15),('last', 13),('together', 13),('w...\n    \n  \n\n\n\n\n\nCan you conduct topic analysis of LDA using the speeches to determine what things presidents talk about in state of the union speeches?\n\nLinear Discriminant Analysis (LDA) is like PCA, but it focuses on maximizing the seperatibility among known categories\n\ntype(df2.iloc[:,0])\n\npandas.core.series.Series\n\n\n\ndataset = pd.read_excel('speeches53463.xlsx')\n\ngrouped = dataset.groupby('name')\ndataset['date'] = pd.to_datetime(dataset['date'])\n\n#print(dataset.head())\ngrouped = dataset.groupby('name')\n\ndataset2 = dataset.loc[dataset.groupby('name').date.idxmin()]\n#.filter(lambda x: x['date'] == min(x['date']))\n\n\nimport re\nimport numpy as np\n    \n# Print the titles of the first rows \nprint(df2[[0]].head())\n\n# Remove punctuation\ndataset['title_processed'] = dataset['speech'].map(lambda x: re.sub('[,\\.!?]', '', x))\n\n# Convert the titles to lowercase\ndataset['title_processed'] = dataset['title_processed'].map(lambda x: x.lower())\n\n# Print the processed titles of the first rows \ndataset['title_processed'].head()\n\n                                                   0\n0  Address Before a Joint Session of the Congress...\n1  Address Before a Joint Session of Congress on ...\n2  Address Before a Joint Session of Congress on ...\n3  Annual Message to the Congress on the State of...\n4  Radio Address Summarizing the State of the Uni...\n\n\n0    the presidentthank you thank you thank you goo...\n1    the presidentthank you all very very much than...\n2    thank you very much mr speaker mr vice preside...\n3    the presidentmr speaker mr vice president memb...\n4    the presidentmadam speaker mr vice president m...\nName: title_processed, dtype: object\n\n\n\n# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\n# Helper function\ndef plot_10_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n\n    plt.bar(x_pos, counts,align='center')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.title('10 most common words')\n    plt.show()\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(dataset['title_processed'])\n\n# Visualise the 10 most common words\nplot_10_most_common_words(count_data, count_vectorizer)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation",
    "href": "software/presidents_analysis.html#interpretation",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nWhile the most common words do not seem to reveal something particular, when those generic words are cleaned as we saw above, we get specific topics and buzz words from each president.\n\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)\n\n# Load the LDA model from sk-learn\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below (use int values below 15)\nnumber_topics = 5\nnumber_words = 25\n\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics)\nlda.fit(count_data)\n\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)\n\nTopics found via LDA:\n\nTopic #0:\nworld war nations peace people united nation free economic great shall congress year military security men national forces new freedom defense time power american government\n\nTopic #1:\nstates government united congress country public great citizens people year time state war treaty foreign shall present subject american general power peace mexico law relations\n\nTopic #2:\ngovernment congress law public federal business national great people country present labor power legislation action conditions shall men year world necessary work nation make time\n\nTopic #3:\nnew america people year years congress american world government make americans help work time federal nation tax economy jobs let programs budget need health program\n\nTopic #4:\nstates united government great congress treaty act year commerce spain public session present state nations france citizens war duties vessels relations minister effect subject commercial"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-1",
    "href": "software/presidents_analysis.html#interpretation-1",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nTopic 0: aligns with internal state laws and political stability.\nTopic 1: seems to align mostly with internal US issues. It is targeted to citizens stability (jobs) and needs.\nTopic 2: Seems to be more related with tax policy and approach to the US economy.\nTopic 3: It has to do with public policy and foreign affairs.\nTopic 4: Mainly targeted to defence department and peace status maintenance while protecting the interests of the US.\n\nCan you determine the sentiment of each state of the union using nltk‚Äôs Vader module?\n\n\nanalyzer=SentimentIntensityAnalyzer()\ndef polarity_score(text):\n    if len(text)>0:\n        score=analyzer.polarity_scores(text)['compound']\n        return score\n    else:\n        return 0\ndataset['polarityscore'] = dataset['speech'].apply(lambda text : polarity_score(text))\ndataset['polarityscore']\n\n0      0.9999\n1      0.9999\n2      1.0000\n3      0.9999\n4      0.9999\n        ...  \n254    0.9998\n255    0.9994\n256    0.9995\n257    0.9991\n258   -0.9997\nName: polarityscore, Length: 259, dtype: float64\n\n\n\ndef sentianamolybarplot(df):\n    polarity_scale=[0.9991,0.9992,0.9993,0.9994,0.9995,0.9996,0.9997,0.9998,0.9999,1]\n    #'Review_polarity' is column name of sentiment score calculated for whole review.\n    df3=df[(df['polarityscore']>0)]\n    out = pd.cut(df3['polarityscore'],polarity_scale)\n    ax = out.value_counts(sort=False).plot.bar(rot=0, color=\"b\", figsize=(12,8))\n    for p in ax.patches:\n        ax.annotate(str(p.get_height()), (p.get_x() * 1.040, p.get_height() * 1.015))\n    plt.show()\nsentianamolybarplot(dataset)\n\n\n\n\nInterpretation\nIn the State of Union Speeches, the presidents talk about important issues facing Americans and offers their ideas on solving the nation‚Äôs problems, including suggestions for new laws and policies. As displayed in the plot, the polarity scores for 13 speeches (barring W. Bush) is positive. This is understandable as the State of Union speeches are a PR vehicle, leveraged to display the President‚Äôs power and positive influence.\n\nDo speeches of different presidents cluster in any way that can allow you to determine their political party? How different are Biden and Trump according to this clustering?\n\n\ndef remove_noise(text, stop_words = nltk.corpus.stopwords.words('english')):\n    newsw = ['annual', 'number', 'help', 'thank', 'get', 'going', 'think', 'look', 'said', 'create',\n             'citizens', 'citizen', 'across', 'since', 'go', 'believe', 'say', 'long', 'better', \n             'plan', 'national', 'ask' '10', 'much', 'good', 'great', 'best', 'cannot', 'still', \n             'know', 'years', '1', 'major', 'want', 'able', 'put', 'capacity', 'programs', 'per', \n             'percent', 'million', 'act', 'provide', 'afford', 'needed', 'may', 'possible', 'full',\n             '2', 'effort', 'meeting', 'address', 'ever', 'measures', 'ago', 'delivered', '5', \n             'program', 'past', 'future', 'need', 'needs', 'house', 'also', 'tonight', 'propose', \n             'toward', 'continue', 'society','country', 'seek', 'period', 'year', 'man', 'men', \n             'one', 'areas', 'begin', 'live', 'make', 'let', 'upon', 'well', 'office', 'meet', \n             'make' 'citizens', 'human', 'self', 'among', 'peoples', 'affairs', 'would', 'field', \n             'first', 'interest', 'today', 'recommendations', 'recomenndation', 'within', 'shall', \n             'administration', 'nation', 'nations', 'us', 'we', 'policy', 'legislation', 'time', \n             'new', 'many', 'several', 'few', 'government', 'world', 'people', 'united', 'states', \n             'system', 'every', 'people', 'must', '626','give', 'categories', '226762', '17608', \n             '24532', '430', '38','statistics', 'analyses', 'miscellaneous', 'congressional', \n             'skip', 'content', 'documents', 'attributes', 'media', 'message', 'congress', 'state',\n             'union', 'america', 'american', 'americans', 'presidency', 'president', 'project', \n             'search', 'toggle', 'navigation', 'search', 'guidebook', 'archive', 'category', 'main', \n             'take','like','yet','j','000']\n    stop_words = stop_words + newsw\n    tokens = word_tokenize(text)\n    cleaned_tokens = []\n    for token in tokens:\n        token = re.sub('[^A-Za-z0-9]+', '', token)\n        if len(token) > 1 and token.lower() not in stop_words:\n            # Get lowercase\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\n\n# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.8,\n                                   max_features = 50,\n                                   min_df = 0.1,\n                                   tokenizer = remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(dataset['speech'].values)\n\n\nnum_clusters = 2\n\n# Generate cluster centers through the kmeans function\ncluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)\n# display(cluster_centers)\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(num_clusters):\n    print('Cluster: {}'.format(i+1))\n    # Sort the terms and print top 3 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms [:15])\n\nCluster: 1\n['federal', 'economic', 'budget', 'work', 'tax', 'economy', 'security', 'jobs', 'nt', 'freedom', 'health', 'free', 'life', 'together', 'defense']\nCluster: 2\n['treaty', 'subject', 'commerce', 'treasury', 'general', 'relations', 'powers', 'laws', 'duty', 'session', 'interests', 'rights', 'however', 'service', 'trade']"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-2",
    "href": "software/presidents_analysis.html#interpretation-2",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nOn clustering the popular words in the speech, it seems like Cluster 1 aligns with speeches by Republican presidents and Cluster 2 with that of speeches by Democartic presidents.\n\n#Converting the datafram into list for further analysis.\nspeech_list = []\nfor i in range(len(dataset2)):\n    speech_list.append(dataset2.iloc[[i]]['speech'].item())\n    \ntitles = []\nfor i in range(len(dataset2)):\n    titles.append(dataset2.iloc[[i]]['name'].item())\n    \ntexts = [txt.split() for txt in speech_list]\n\n# Create an instance of a PorterStemmer object\nporter = PorterStemmer()\n\n# For each token of each text, we generated its stem \ntexts_stem = [[porter.stem(token) for token in text] for text in texts]\n\n# Create a dictionary from the stemmed tokens\ndictionary = corpora.Dictionary(texts_stem)\n\n# Create a bag-of-words model for each speech, using the previously generated dictionary\nbows = [dictionary.doc2bow(text) for text in texts_stem]\n\n# Generate the tf-idf model\nmodel = TfidfModel(bows)\n\n# Compute the similarity matrix (pairwise distance between all speeches)\nsims = similarities.MatrixSimilarity(model[bows])\n\n# Transform the resulting list into a DataFrame\nsim_df = pd.DataFrame(list(sims))\n\n# Add the name of the presidents as columns and index of the DataFrame\nsim_df.columns = titles\nsim_df.index = titles\n\n# Print the resulting matrix\nsim_df\n\n# creat function that adds two numbers\n\n\n\n\n\n  \n    \n      \n      Abraham Lincoln\n      Andrew Jackson\n      Andrew Johnson\n      Barack Obama\n      Benjamin Harrison\n      Calvin Coolidge\n      Chester A. Arthur\n      Donald J. Trump\n      Dwight D. Eisenhower\n      Franklin D. Roosevelt\n      ...\n      Rutherford B. Hayes\n      Theodore Roosevelt\n      Thomas Jefferson\n      Ulysses S. Grant\n      Warren G. Harding\n      William Howard Taft\n      William J. Clinton\n      William McKinley\n      Woodrow Wilson\n      Zachary Taylor\n    \n  \n  \n    \n      Abraham Lincoln\n      1.000000\n      0.121246\n      0.151205\n      0.047722\n      0.110114\n      0.118578\n      0.114825\n      0.047754\n      0.075706\n      0.047841\n      ...\n      0.092012\n      0.056717\n      0.110742\n      0.124756\n      0.082261\n      0.092794\n      0.046491\n      0.113427\n      0.077418\n      0.145153\n    \n    \n      Andrew Jackson\n      0.121246\n      1.000000\n      0.121070\n      0.053532\n      0.131924\n      0.086656\n      0.106385\n      0.054705\n      0.077823\n      0.047385\n      ...\n      0.112656\n      0.070145\n      0.117074\n      0.146622\n      0.092593\n      0.113945\n      0.050358\n      0.102445\n      0.086139\n      0.155541\n    \n    \n      Andrew Johnson\n      0.151205\n      0.121070\n      1.000000\n      0.047996\n      0.097058\n      0.094947\n      0.079968\n      0.049593\n      0.073742\n      0.053389\n      ...\n      0.082619\n      0.077785\n      0.093668\n      0.127001\n      0.090346\n      0.077516\n      0.049155\n      0.080710\n      0.082443\n      0.092401\n    \n    \n      Barack Obama\n      0.047722\n      0.053532\n      0.047996\n      1.000000\n      0.046357\n      0.069860\n      0.037358\n      0.213163\n      0.103428\n      0.096561\n      ...\n      0.035344\n      0.069914\n      0.054143\n      0.047495\n      0.073191\n      0.049368\n      0.317297\n      0.050867\n      0.064416\n      0.039508\n    \n    \n      Benjamin Harrison\n      0.110114\n      0.131924\n      0.097058\n      0.046357\n      1.000000\n      0.095429\n      0.198889\n      0.043727\n      0.082837\n      0.068579\n      ...\n      0.269502\n      0.057292\n      0.080109\n      0.146124\n      0.080189\n      0.125491\n      0.049069\n      0.142834\n      0.076850\n      0.134368\n    \n    \n      Calvin Coolidge\n      0.118578\n      0.086656\n      0.094947\n      0.069860\n      0.095429\n      1.000000\n      0.085849\n      0.072170\n      0.118543\n      0.078246\n      ...\n      0.081089\n      0.087503\n      0.077970\n      0.098388\n      0.128878\n      0.092294\n      0.076209\n      0.074285\n      0.086982\n      0.092193\n    \n    \n      Chester A. Arthur\n      0.114825\n      0.106385\n      0.079968\n      0.037358\n      0.198889\n      0.085849\n      1.000000\n      0.042525\n      0.058818\n      0.042672\n      ...\n      0.144648\n      0.045146\n      0.063310\n      0.179516\n      0.066345\n      0.137828\n      0.045936\n      0.123470\n      0.059776\n      0.132799\n    \n    \n      Donald J. Trump\n      0.047754\n      0.054705\n      0.049593\n      0.213163\n      0.043727\n      0.072170\n      0.042525\n      1.000000\n      0.077175\n      0.062748\n      ...\n      0.040511\n      0.077699\n      0.046509\n      0.056097\n      0.065570\n      0.040417\n      0.183823\n      0.042885\n      0.056082\n      0.040505\n    \n    \n      Dwight D. Eisenhower\n      0.075706\n      0.077823\n      0.073742\n      0.103428\n      0.082837\n      0.118543\n      0.058818\n      0.077175\n      1.000000\n      0.108428\n      ...\n      0.059357\n      0.074973\n      0.060950\n      0.085503\n      0.107023\n      0.072974\n      0.128444\n      0.065945\n      0.104088\n      0.069061\n    \n    \n      Franklin D. Roosevelt\n      0.047841\n      0.047385\n      0.053389\n      0.096561\n      0.068579\n      0.078246\n      0.042672\n      0.062748\n      0.108428\n      1.000000\n      ...\n      0.064508\n      0.072999\n      0.052945\n      0.059127\n      0.094722\n      0.054787\n      0.086663\n      0.063699\n      0.077462\n      0.050414\n    \n    \n      Franklin Pierce\n      0.138249\n      0.149229\n      0.113608\n      0.039742\n      0.135522\n      0.096017\n      0.129588\n      0.034980\n      0.083958\n      0.055784\n      ...\n      0.096431\n      0.059912\n      0.091416\n      0.149410\n      0.087211\n      0.114362\n      0.038283\n      0.098263\n      0.081036\n      0.181408\n    \n    \n      George Bush\n      0.039750\n      0.110718\n      0.045218\n      0.223350\n      0.039387\n      0.067593\n      0.030361\n      0.166730\n      0.101130\n      0.063581\n      ...\n      0.029770\n      0.064276\n      0.043393\n      0.048644\n      0.064347\n      0.045680\n      0.236761\n      0.042725\n      0.064541\n      0.038623\n    \n    \n      George W. Bush\n      0.045682\n      0.045953\n      0.042901\n      0.244457\n      0.043902\n      0.076660\n      0.044497\n      0.170425\n      0.109930\n      0.069602\n      ...\n      0.038201\n      0.047932\n      0.043533\n      0.048213\n      0.069961\n      0.033635\n      0.258270\n      0.042687\n      0.054859\n      0.033777\n    \n    \n      George Washington\n      0.075427\n      0.080150\n      0.054498\n      0.022271\n      0.046189\n      0.048199\n      0.043466\n      0.021537\n      0.033813\n      0.023598\n      ...\n      0.056167\n      0.028022\n      0.075892\n      0.047378\n      0.038203\n      0.039933\n      0.022466\n      0.030561\n      0.040550\n      0.058378\n    \n    \n      Gerald R. Ford\n      0.040651\n      0.041807\n      0.043266\n      0.133494\n      0.045676\n      0.079921\n      0.053701\n      0.082173\n      0.135036\n      0.062518\n      ...\n      0.035063\n      0.051462\n      0.039757\n      0.051069\n      0.071961\n      0.044571\n      0.183497\n      0.041057\n      0.048935\n      0.039580\n    \n    \n      Grover Cleveland\n      0.117943\n      0.128614\n      0.102102\n      0.036857\n      0.129553\n      0.088140\n      0.141185\n      0.043138\n      0.070560\n      0.049183\n      ...\n      0.098994\n      0.069918\n      0.072758\n      0.129015\n      0.083239\n      0.154388\n      0.035827\n      0.105645\n      0.084410\n      0.169106\n    \n    \n      Harry S. Truman\n      0.060982\n      0.071213\n      0.065671\n      0.087225\n      0.070839\n      0.099180\n      0.058541\n      0.080372\n      0.165834\n      0.092160\n      ...\n      0.055451\n      0.064122\n      0.054537\n      0.079809\n      0.106270\n      0.081560\n      0.105554\n      0.059129\n      0.081950\n      0.079267\n    \n    \n      Herbert Hoover\n      0.103231\n      0.079289\n      0.081658\n      0.070459\n      0.111400\n      0.127897\n      0.128176\n      0.049934\n      0.146132\n      0.097870\n      ...\n      0.078349\n      0.062296\n      0.072092\n      0.110752\n      0.135878\n      0.098500\n      0.090548\n      0.076114\n      0.096796\n      0.091948\n    \n    \n      James Buchanan\n      0.074219\n      0.117172\n      0.085650\n      0.053138\n      0.213879\n      0.067012\n      0.108787\n      0.033511\n      0.050480\n      0.062365\n      ...\n      0.181783\n      0.050247\n      0.071056\n      0.138804\n      0.063212\n      0.083642\n      0.036889\n      0.143667\n      0.058985\n      0.117347\n    \n    \n      James K. Polk\n      0.108668\n      0.127383\n      0.080034\n      0.034267\n      0.083551\n      0.063491\n      0.086467\n      0.037409\n      0.050591\n      0.038777\n      ...\n      0.081692\n      0.041648\n      0.059918\n      0.124555\n      0.054302\n      0.098106\n      0.031113\n      0.069200\n      0.073058\n      0.204857\n    \n    \n      James Madison\n      0.071391\n      0.094473\n      0.062079\n      0.024049\n      0.066594\n      0.058868\n      0.067699\n      0.027369\n      0.047359\n      0.028658\n      ...\n      0.071821\n      0.037817\n      0.098455\n      0.088595\n      0.049361\n      0.056839\n      0.027100\n      0.063285\n      0.035300\n      0.113097\n    \n    \n      James Monroe\n      0.121900\n      0.130480\n      0.097276\n      0.034281\n      0.096656\n      0.073366\n      0.089977\n      0.034260\n      0.062603\n      0.045482\n      ...\n      0.076209\n      0.049221\n      0.112049\n      0.125313\n      0.073308\n      0.077521\n      0.033153\n      0.085300\n      0.060619\n      0.129747\n    \n    \n      Jimmy Carter\n      0.038933\n      0.046552\n      0.046946\n      0.240644\n      0.048041\n      0.088707\n      0.047153\n      0.162571\n      0.159608\n      0.088317\n      ...\n      0.040429\n      0.069212\n      0.045362\n      0.048941\n      0.091093\n      0.049687\n      0.229270\n      0.049615\n      0.075476\n      0.042085\n    \n    \n      John Adams\n      0.085961\n      0.099759\n      0.060527\n      0.025965\n      0.059714\n      0.045513\n      0.077210\n      0.026649\n      0.044619\n      0.024816\n      ...\n      0.053008\n      0.042880\n      0.083986\n      0.104180\n      0.036770\n      0.063388\n      0.029237\n      0.067327\n      0.038350\n      0.110925\n    \n    \n      John F. Kennedy\n      0.054274\n      0.064872\n      0.067020\n      0.145161\n      0.064716\n      0.087427\n      0.052211\n      0.091397\n      0.166903\n      0.082637\n      ...\n      0.060584\n      0.066535\n      0.050081\n      0.067712\n      0.091555\n      0.065329\n      0.145564\n      0.086820\n      0.069619\n      0.051925\n    \n    \n      John Quincy Adams\n      0.134975\n      0.163353\n      0.091024\n      0.045042\n      0.101502\n      0.086493\n      0.104866\n      0.043737\n      0.062438\n      0.040328\n      ...\n      0.084253\n      0.063036\n      0.126550\n      0.123556\n      0.071098\n      0.098951\n      0.040523\n      0.082057\n      0.072336\n      0.149257\n    \n    \n      John Tyler\n      0.131702\n      0.152333\n      0.117307\n      0.045652\n      0.123190\n      0.103976\n      0.117884\n      0.047375\n      0.062916\n      0.067584\n      ...\n      0.141787\n      0.081356\n      0.119774\n      0.153659\n      0.101667\n      0.120820\n      0.052103\n      0.113313\n      0.072481\n      0.170983\n    \n    \n      Joseph R. Biden\n      0.030122\n      0.035307\n      0.032910\n      0.345184\n      0.028887\n      0.054890\n      0.031472\n      0.220390\n      0.068947\n      0.053099\n      ...\n      0.021369\n      0.050805\n      0.034194\n      0.039431\n      0.052046\n      0.028445\n      0.306361\n      0.027510\n      0.054481\n      0.033634\n    \n    \n      Lyndon B. Johnson\n      0.045479\n      0.043621\n      0.046318\n      0.128715\n      0.035904\n      0.065005\n      0.038097\n      0.106125\n      0.100885\n      0.056544\n      ...\n      0.033196\n      0.058649\n      0.040729\n      0.053333\n      0.065493\n      0.042870\n      0.168042\n      0.054452\n      0.047279\n      0.036023\n    \n    \n      Martin van Buren\n      0.123016\n      0.189609\n      0.095427\n      0.050744\n      0.148065\n      0.078894\n      0.110916\n      0.030465\n      0.075507\n      0.051770\n      ...\n      0.114139\n      0.069005\n      0.121863\n      0.127558\n      0.084042\n      0.114678\n      0.043701\n      0.112850\n      0.086940\n      0.182542\n    \n    \n      Millard Fillmore\n      0.128602\n      0.168584\n      0.111184\n      0.040514\n      0.116195\n      0.083215\n      0.112446\n      0.046339\n      0.078221\n      0.067761\n      ...\n      0.102227\n      0.080306\n      0.119655\n      0.143147\n      0.097297\n      0.102903\n      0.044584\n      0.100449\n      0.100466\n      0.200081\n    \n    \n      Richard Nixon\n      0.039255\n      0.049510\n      0.062639\n      0.139949\n      0.042007\n      0.068414\n      0.038798\n      0.105487\n      0.136241\n      0.066019\n      ...\n      0.036937\n      0.061997\n      0.043634\n      0.054097\n      0.077172\n      0.050246\n      0.174663\n      0.044486\n      0.073935\n      0.037279\n    \n    \n      Ronald Reagan\n      0.042896\n      0.039901\n      0.044701\n      0.231009\n      0.048165\n      0.076235\n      0.040150\n      0.140081\n      0.140061\n      0.081909\n      ...\n      0.034992\n      0.052896\n      0.045076\n      0.049203\n      0.080074\n      0.039373\n      0.300796\n      0.046391\n      0.057221\n      0.038241\n    \n    \n      Rutherford B. Hayes\n      0.092012\n      0.112656\n      0.082619\n      0.035344\n      0.269502\n      0.081089\n      0.144648\n      0.040511\n      0.059357\n      0.064508\n      ...\n      1.000000\n      0.058833\n      0.074037\n      0.131257\n      0.078165\n      0.077687\n      0.046635\n      0.139910\n      0.066988\n      0.097658\n    \n    \n      Theodore Roosevelt\n      0.056717\n      0.070145\n      0.077785\n      0.069914\n      0.057292\n      0.087503\n      0.045146\n      0.077699\n      0.074973\n      0.072999\n      ...\n      0.058833\n      1.000000\n      0.050832\n      0.071917\n      0.103435\n      0.060783\n      0.066718\n      0.069393\n      0.067994\n      0.059469\n    \n    \n      Thomas Jefferson\n      0.110742\n      0.117074\n      0.093668\n      0.054143\n      0.080109\n      0.077970\n      0.063310\n      0.046509\n      0.060950\n      0.052945\n      ...\n      0.074037\n      0.050832\n      1.000000\n      0.093839\n      0.069619\n      0.051927\n      0.047307\n      0.067414\n      0.075135\n      0.109134\n    \n    \n      Ulysses S. Grant\n      0.124756\n      0.146622\n      0.127001\n      0.047495\n      0.146124\n      0.098388\n      0.179516\n      0.056097\n      0.085503\n      0.059127\n      ...\n      0.131257\n      0.071917\n      0.093839\n      1.000000\n      0.091914\n      0.121181\n      0.060122\n      0.167308\n      0.080233\n      0.137702\n    \n    \n      Warren G. Harding\n      0.082261\n      0.092593\n      0.090346\n      0.073191\n      0.080189\n      0.128878\n      0.066345\n      0.065570\n      0.107023\n      0.094722\n      ...\n      0.078165\n      0.103435\n      0.069619\n      0.091914\n      1.000000\n      0.081010\n      0.092189\n      0.096931\n      0.096031\n      0.070389\n    \n    \n      William Howard Taft\n      0.092794\n      0.113945\n      0.077516\n      0.049368\n      0.125491\n      0.092294\n      0.137828\n      0.040417\n      0.072974\n      0.054787\n      ...\n      0.077687\n      0.060783\n      0.051927\n      0.121181\n      0.081010\n      1.000000\n      0.045340\n      0.082594\n      0.087100\n      0.124097\n    \n    \n      William J. Clinton\n      0.046491\n      0.050358\n      0.049155\n      0.317297\n      0.049069\n      0.076209\n      0.045936\n      0.183823\n      0.128444\n      0.086663\n      ...\n      0.046635\n      0.066718\n      0.047307\n      0.060122\n      0.092189\n      0.045340\n      1.000000\n      0.051195\n      0.075808\n      0.044786\n    \n    \n      William McKinley\n      0.113427\n      0.102445\n      0.080710\n      0.050867\n      0.142834\n      0.074285\n      0.123470\n      0.042885\n      0.065945\n      0.063699\n      ...\n      0.139910\n      0.069393\n      0.067414\n      0.167308\n      0.096931\n      0.082594\n      0.051195\n      1.000000\n      0.067011\n      0.103954\n    \n    \n      Woodrow Wilson\n      0.077418\n      0.086139\n      0.082443\n      0.064416\n      0.076850\n      0.086982\n      0.059776\n      0.056082\n      0.104088\n      0.077462\n      ...\n      0.066988\n      0.067994\n      0.075135\n      0.080233\n      0.096031\n      0.087100\n      0.075808\n      0.067011\n      1.000000\n      0.081701\n    \n    \n      Zachary Taylor\n      0.145153\n      0.155541\n      0.092401\n      0.039508\n      0.134368\n      0.092193\n      0.132799\n      0.040505\n      0.069061\n      0.050414\n      ...\n      0.097658\n      0.059469\n      0.109134\n      0.137702\n      0.070389\n      0.124097\n      0.044786\n      0.103954\n      0.081701\n      1.000000\n    \n  \n\n43 rows √ó 43 columns\n\n\n\nHere we can see the degree of similarity of each president‚Äôs speech with each other. There is no speech with similarity more than 40%.\n\n# Compute the clusters from the similarity matrix,\n# using the Ward variance minimization algorithm\nZ = hierarchy.linkage(sim_df, 'ward')\nplt.rcParams['figure.figsize'] = [10,20]\n# Display this result as a horizontal dendrogram\na = hierarchy.dendrogram(Z,  leaf_font_size=20, labels=sim_df.index,  orientation=\"left\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-3",
    "href": "software/presidents_analysis.html#interpretation-3",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nIn the dendrogram above, the presidents seem to be clustered based on the era that they served. Two big clusters are distinct ‚Äì the green one which includes mostly recent presidents 20th and 21st century, and ‚Äì the orange one which includes presidents of the 18th and 19th century.\n\nWho was the president whose speech was the most similar to the speech of Biden in 2022?\n\n\nv = sim_df[['Joseph R. Biden']]\nv\n\n\n\n\n\n  \n    \n      \n      Joseph R. Biden\n    \n  \n  \n    \n      Abraham Lincoln\n      0.030122\n    \n    \n      Andrew Jackson\n      0.035307\n    \n    \n      Andrew Johnson\n      0.032910\n    \n    \n      Barack Obama\n      0.345184\n    \n    \n      Benjamin Harrison\n      0.028887\n    \n    \n      Calvin Coolidge\n      0.054890\n    \n    \n      Chester A. Arthur\n      0.031472\n    \n    \n      Donald J. Trump\n      0.220390\n    \n    \n      Dwight D. Eisenhower\n      0.068947\n    \n    \n      Franklin D. Roosevelt\n      0.053099\n    \n    \n      Franklin Pierce\n      0.030974\n    \n    \n      George Bush\n      0.227037\n    \n    \n      George W. Bush\n      0.205001\n    \n    \n      George Washington\n      0.012446\n    \n    \n      Gerald R. Ford\n      0.094785\n    \n    \n      Grover Cleveland\n      0.025062\n    \n    \n      Harry S. Truman\n      0.052615\n    \n    \n      Herbert Hoover\n      0.045206\n    \n    \n      James Buchanan\n      0.024993\n    \n    \n      James K. Polk\n      0.038190\n    \n    \n      James Madison\n      0.016895\n    \n    \n      James Monroe\n      0.019880\n    \n    \n      Jimmy Carter\n      0.198534\n    \n    \n      John Adams\n      0.016608\n    \n    \n      John F. Kennedy\n      0.094466\n    \n    \n      John Quincy Adams\n      0.024704\n    \n    \n      John Tyler\n      0.035437\n    \n    \n      Joseph R. Biden\n      1.000000\n    \n    \n      Lyndon B. Johnson\n      0.107517\n    \n    \n      Martin van Buren\n      0.030007\n    \n    \n      Millard Fillmore\n      0.024063\n    \n    \n      Richard Nixon\n      0.113265\n    \n    \n      Ronald Reagan\n      0.223184\n    \n    \n      Rutherford B. Hayes\n      0.021369\n    \n    \n      Theodore Roosevelt\n      0.050805\n    \n    \n      Thomas Jefferson\n      0.034194\n    \n    \n      Ulysses S. Grant\n      0.039431\n    \n    \n      Warren G. Harding\n      0.052046\n    \n    \n      William Howard Taft\n      0.028445\n    \n    \n      William J. Clinton\n      0.306361\n    \n    \n      William McKinley\n      0.027510\n    \n    \n      Woodrow Wilson\n      0.054481\n    \n    \n      Zachary Taylor\n      0.033634\n    \n  \n\n\n\n\n\n# This is needed to display plots in a notebook\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [10,10]\n\n# Select the column corresponding to Biden's address and \nv = sim_df['Joseph R. Biden']\n\n# Sort by ascending scores\nv_sorted = v.sort_values(ascending=True)\n\n# Plot this data has a horizontal bar plot\nv_sorted.plot.barh(x='lab', y='val', rot=0).plot()\n\n# Modify the axes labels and plot title for better readability\nplt.xlabel(\"Cosine distance\")\nplt.ylabel(\"\")\nplt.title(\"Most similar to Biden's\")\n\nText(0.5, 1.0, \"Most similar to Biden's\")"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-4",
    "href": "software/presidents_analysis.html#interpretation-4",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nBiden‚Äôs address is most similar to that of Obama‚Äôs, follwed by Clinton and George Bush. As we can see, George Washington, John Adams and James Madison are the least similar to Biden. This can be explained by the different eras that each president lived. Speeches in 18th century are different than speeches of today. Biden‚Äôs speech similarity with Obama and Clinton makes sense also because they are all recently elected democrats.\n\nBonus points: (5 points): Develop and algorithm that can allow you to determine if the speech was given by a Democrat or by a republican.\n\nPS2: I will go over this homework on Thursday to help you think through how to solve it. You will be able to recycle a lot of code discussed.\n\nspeeches\n\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      top ten\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('public', 33),('law', 25),('service', 23),('...\n    \n    \n      1\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('industrial', 9),('work', 8),('recovery', 7)...\n    \n    \n      2\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('prosperity', 12),('production', 12),('power...\n    \n    \n      3\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('free', 16),('security', 16),('economy', 12)...\n    \n    \n      4\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('economic', 16),('development', 10),('peace'...\n    \n    \n      5\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      [('freedom', 12),('life', 9),('progress', 8),(...\n    \n    \n      6\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('peace', 27),('energy', 17),('war', 8),('pro...\n    \n    \n      7\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('energy', 25),('oil', 20),('tax', 17),('econ...\n    \n    \n      8\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      [('inflation', 17),('economic', 14),('tax', 13...\n    \n    \n      9\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('freedom', 20),('tax', 16),('growth', 14),('...\n    \n    \n      10\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('budget', 17),('work', 12),('hope', 10),('dr...\n    \n    \n      11\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('children', 24),('work', 21),('budget', 17),...\n    \n    \n      12\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('security', 29),('freedom', 20),('social', 1...\n    \n    \n      13\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('jobs', 32),('work', 20),('energy', 18),('fa...\n    \n    \n      14\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('tax', 15),('last', 13),('together', 13),('w...\n    \n    \n      15\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      [('folks', 19),('see', 15),('families', 15),('...\n    \n  \n\n\n\n\n\ndf2[[0]]\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      Address Before a Joint Session of the Congress...\n    \n    \n      1\n      Address Before a Joint Session of Congress on ...\n    \n    \n      2\n      Address Before a Joint Session of Congress on ...\n    \n    \n      3\n      Annual Message to the Congress on the State of...\n    \n    \n      4\n      Radio Address Summarizing the State of the Uni...\n    \n    \n      5\n      Fifth Annual Message | The American Presidency...\n    \n    \n      6\n      First Annual Message | The American Presidency...\n    \n    \n      7\n      First Annual Message | The American Presidency...\n    \n    \n      8\n      First Annual Message | The American Presidency...\n    \n    \n      9\n      Fifth Annual Message | The American Presidency...\n    \n    \n      10\n      Fifth Annual Message | The American Presidency...\n    \n    \n      11\n      State of the Union Message to the Congress: Ov...\n    \n    \n      12\n      State of the Union Message to the Congress on ...\n    \n    \n      13\n      Address Before a Joint Session of the Congress...\n    \n  \n\n\n\n\n\nstate_speeches = pd.read_excel('state_speeches.xlsx')\nstate_speeches\n\n\n\n\n\n  \n    \n      \n      president\n      year\n      party\n      html\n      address\n    \n  \n  \n    \n      0\n      Hoover\n      1929\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      The Constitution requires that the President \"...\n    \n    \n      1\n      FD_Roosvelt\n      1934\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      I COME before you at the opening of the Regula...\n    \n    \n      2\n      Truman\n      1949\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      I am happy to report to this 81st Congress tha...\n    \n    \n      3\n      Eisenhower\n      1957\n      republican\n      https://www.presidency.ucsb.edu/documents/annu...\n      I appear before the Congress today to report o...\n    \n    \n      4\n      Kennedy\n      1961\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      It is a pleasure to return from whence I came....\n    \n    \n      5\n      Lyndon\n      1965\n      democrat\n      https://www.presidency.ucsb.edu/documents/annu...\n      On this Hill which was my home, I am stirred b...\n    \n    \n      6\n      Nixon\n      1974\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      We meet here tonight at a time of great challe...\n    \n    \n      7\n      Ford\n      1975\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      Twenty-six years ago, a freshman Congressman, ...\n    \n    \n      8\n      Carter\n      1978\n      democrat\n      https://www.presidency.ucsb.edu/documents/the-...\n      Two years ago today we had the first caucus in...\n    \n    \n      9\n      Reagan\n      1985\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      I come before you to report on the state of ou...\n    \n    \n      10\n      Bush\n      1989\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      Mr. Speaker, Mr. President, and distinguished ...\n    \n    \n      11\n      Clinton\n      1997\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      Mr. Speaker, Mr. Vice President, Members of th...\n    \n    \n      12\n      Bush\n      2005\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      As a new Congress gathers, all of us in the el...\n    \n    \n      13\n      Obama\n      2013\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      Please, everybody, have a seat. Mr. Speaker, M...\n    \n    \n      14\n      Trump\n      2018\n      republican\n      https://www.presidency.ucsb.edu/documents/addr...\n      The President. Mr. Speaker, Mr. Vice President...\n    \n    \n      15\n      Joe\n      2022\n      democrat\n      https://www.presidency.ucsb.edu/documents/addr...\n      The President. Thank you all very, very much. ...\n    \n  \n\n\n\n\n\n\n# Split training and testing data\nX_train, X_test, y_train, y_test = train_test_split(state_speeches['address'], state_speeches['party'], test_size=0.3, \n                 random_state=53)\n\n# Initialize count vectorizer\ncount_vectorizer = CountVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n# Create count train and test variables\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\n\n# Initialize tfidf vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', \n                                   min_df=0.05, max_df=0.9)\n\n\n# Create tfidf train and test variables\ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ntfidf_nb = MultinomialNB()\ntfidf_nb.fit(tfidf_train, y_train)\ntfidf_nb_pred = tfidf_nb.predict(tfidf_test)\ntfidf_nb_score = metrics.accuracy_score(y_test, tfidf_nb_pred)\n\ncount_nb = MultinomialNB()\ncount_nb.fit(count_train, y_train)\ncount_nb_pred = count_nb.predict(count_test)\ncount_nb_score = metrics.accuracy_score(y_test, count_nb_pred)\n\nprint('NaiveBayes Tfidf Score: ', tfidf_nb_score)\nprint('NaiveBayes Count Score: ', count_nb_score)\n\nNaiveBayes Tfidf Score:  0.2\nNaiveBayes Count Score:  0.2\n\n\n\n%matplotlib inline\nfrom sklearn.metrics import plot_confusion_matrix\n\n\ntfidf_nb_cm = metrics.confusion_matrix(y_test, tfidf_nb_pred, labels=['republican', 'democrat'])\ncount_nb_cm = metrics.confusion_matrix(y_test, count_nb_pred, labels=['republican', 'democrat'])\n\n# plot_confusion_matrix(tfidf_nb_cm, classes=['republican', 'democrat'], title=\"TF-IDF NB Confusion Matrix\")\n\n# plot_confusion_matrix(count_nb_cm, classes=['republican', 'democrat'], title=\"Count NB Confusion Matrix\", figure=1)"
  },
  {
    "objectID": "software/presidents_analysis.html#interpretation-5",
    "href": "software/presidents_analysis.html#interpretation-5",
    "title": "Loizos Konstantinou",
    "section": "Interpretation",
    "text": "Interpretation\nIt looks like that algorithm‚Äôs power to identify whether the speech comes from a democrat or a republican is only 20%. In this case, for the algorithm to get stronger, more speeches are necessary from both sides, and maybe more text cleaning.\n\npip install jupyterthemes\n\nRequirement already satisfied: jupyterthemes in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (0.20.0)\nRequirement already satisfied: matplotlib>=1.4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (3.4.3)\nRequirement already satisfied: lesscpy>=0.11.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (0.15.0)\nRequirement already satisfied: notebook>=5.6.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (6.4.5)\nRequirement already satisfied: ipython>=5.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (7.29.0)\nRequirement already satisfied: jupyter-core in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jupyterthemes) (4.8.1)\nRequirement already satisfied: setuptools>=18.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (58.0.4)\nRequirement already satisfied: appnope in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\nRequirement already satisfied: traitlets>=4.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (3.0.20)\nRequirement already satisfied: pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (2.10.0)\nRequirement already satisfied: backcall in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.2.0)\nRequirement already satisfied: pexpect>4.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (4.8.0)\nRequirement already satisfied: matplotlib-inline in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.1.2)\nRequirement already satisfied: decorator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (5.1.0)\nRequirement already satisfied: pickleshare in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.7.5)\nRequirement already satisfied: jedi>=0.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipython>=5.4.1->jupyterthemes) (0.18.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython>=5.4.1->jupyterthemes) (0.8.2)\nRequirement already satisfied: six in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy>=0.11.2->jupyterthemes) (1.16.0)\nRequirement already satisfied: ply in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from lesscpy>=0.11.2->jupyterthemes) (3.11)\nRequirement already satisfied: python-dateutil>=2.7 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.3.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (3.0.4)\nRequirement already satisfied: pillow>=6.2.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (8.4.0)\nRequirement already satisfied: numpy>=1.16 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.3->jupyterthemes) (1.20.3)\nRequirement already satisfied: jinja2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (2.11.3)\nRequirement already satisfied: pyzmq>=17 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (22.2.1)\nRequirement already satisfied: argon2-cffi in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (20.1.0)\nRequirement already satisfied: jupyter-client>=5.3.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.12)\nRequirement already satisfied: Send2Trash>=1.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (1.8.0)\nRequirement already satisfied: prometheus-client in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.11.0)\nRequirement already satisfied: nbformat in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (5.1.3)\nRequirement already satisfied: ipykernel in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.4.1)\nRequirement already satisfied: ipython-genutils in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.2.0)\nRequirement already satisfied: tornado>=6.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1)\nRequirement already satisfied: terminado>=0.8.3 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (0.9.4)\nRequirement already satisfied: nbconvert in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from notebook>=5.6.0->jupyterthemes) (6.1.0)\nRequirement already satisfied: ptyprocess>=0.5 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from pexpect>4.3->ipython>=5.4.1->jupyterthemes) (0.7.0)\nRequirement already satisfied: wcwidth in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.4.1->jupyterthemes) (0.2.5)\nRequirement already satisfied: cffi>=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from argon2-cffi->notebook>=5.6.0->jupyterthemes) (1.14.6)\nRequirement already satisfied: pycparser in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=5.6.0->jupyterthemes) (2.20)\nRequirement already satisfied: debugpy<2.0,>=1.0.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from ipykernel->notebook>=5.6.0->jupyterthemes) (1.4.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jinja2->notebook>=5.6.0->jupyterthemes) (1.1.1)\nRequirement already satisfied: entrypoints>=0.2.2 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.3)\nRequirement already satisfied: jupyterlab-pygments in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.1.2)\nRequirement already satisfied: pandocfilters>=1.4.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (1.4.3)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.3)\nRequirement already satisfied: bleach in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (4.0.0)\nRequirement already satisfied: testpath in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.0)\nRequirement already satisfied: defusedxml in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.7.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbconvert->notebook>=5.6.0->jupyterthemes) (0.8.4)\nRequirement already satisfied: async-generator in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.10)\nRequirement already satisfied: nest-asyncio in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=5.6.0->jupyterthemes) (1.5.1)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from nbformat->notebook>=5.6.0->jupyterthemes) (3.2.0)\nRequirement already satisfied: attrs>=17.4.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (21.2.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.6.0->jupyterthemes) (0.18.0)\n\n\nRequirement already satisfied: packaging in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (21.0)\nRequirement already satisfied: webencodings in /Users/loizoskon/opt/anaconda3/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=5.6.0->jupyterthemes) (0.5.1)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n!jt -l\n\nAvailable Themes: \n   chesterish\n   grade3\n   gruvboxd\n   gruvboxl\n   monokai\n   oceans16\n   onedork\n   solarizedd\n   solarizedl"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Resume",
    "section": "",
    "text": "Here is my up-to-date Resume\nI have a blog that is occasionally updated üìù\nSee my projects for some things I‚Äôve been working on üõ†\nCurrently learning  and  on : LoizosKons\nFind me on chess.com here ‚ôû"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Loizos Konstantinou",
    "section": "",
    "text": "Some words I wrote\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\n\n\n\nNepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp\n\n\nMar 7, 2023\n\n\n\n\n\n\n\nExperience Cyprus Like a Local!\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\nSUSI Entrepreneurship Program: A Free Ticket to an American Dream\n\n\nJan 5, 2023\n\n\n\n\n\n\n\nMaking the Most of Your University Experience at EUC: A Student‚Äôs Perspective\n\n\nOct 2, 2022\n\n\n\n\n\n\n\nCopenhagen: A Great Travel Guide for Your Next Visit\n\n\nJun 9, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html",
    "href": "posts/2023-03-10-susi/index.html",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "",
    "text": "SUSI group at the University of Tennesee at Chattanooga\nStudy of the US Institutes for Student Leaders is an intensive academic program for undergraduate students from Europe. I was a part of the Entrepreneurship and Economic Development 2019 cohort which took place in Tennessee, Chattanooga. Twenty (20) Students from seventeen (17) different European countries were selected for this program. We spent five weeks in the United States getting a deeper understanding of American culture, business environment in social and corporate settings, and enhancing our leadership skills. New friends, entrepreneurial exposure, business visits, US cultural learning, and travel experiences, and the opportunity to get funding for future projects on the benefits side, PLUS all of these for $0 dollars! On the cost side, only opportunity cost."
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#about-the-program",
    "href": "posts/2023-03-10-susi/index.html#about-the-program",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "About the program",
    "text": "About the program\nWhen I first came across the program, I read about the opportunity and I got excited about the potential learnings. I would have the opportunity to visit new places, spend my summer in a purposeful way learn about economic development, AND make new friends and experiences. Then, I also realized that the program would pay for ALL my travel and living expenses for five weeks in the U.S. It was an opportunity I couldn‚Äôt pass up!\nIntroductory SUSI video before arrival üëà"
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#selection-process",
    "href": "posts/2023-03-10-susi/index.html#selection-process",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "Selection process",
    "text": "Selection process\nPut in effort in the application because the return is too good to be true, but it is! First, you need to submit your application through the link from the US Embassy‚Äôs post. For this step, you need to make a compelling case for why you are the right fit for the program. Focus on the application description criteria. In the motivation section, try not to recycle your resume (they already have it), but focus on your motivation. The selection committee wants to nominate someone that is very active within and outside the university, has leadership skills and potential, and is curious to learn about the US culture.\nIf you are selected for the interview congrats! Now you are being tested in two things: 1) your ability to communicate in english, 2) your stories that show you are the right fit ‚Äî leadership skills, activity outside school, curiosity, and interest in the program. An important thing to note here, no matter how well you do, the selection committee will be the one that gets to choose and you will be compared with other candidates. In other words, not everything is in your control. So my advice here is to try to focus on how you can perform your best, as it is the only thing that is in your hands and can maximize your likelihood of getting nominated."
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#experience",
    "href": "posts/2023-03-10-susi/index.html#experience",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "Experience",
    "text": "Experience\nSUSI experience in a nutshell üëà"
  },
  {
    "objectID": "posts/2023-03-10-susi/index.html#a-typical-weekday",
    "href": "posts/2023-03-10-susi/index.html#a-typical-weekday",
    "title": "SUSI Entrepreneurship Program: A Free Ticket to an American Dream",
    "section": "A typical weekday",
    "text": "A typical weekday\nEarly in the mornings, from 8 to 11 am, we had classes about management, entrepreneurship, and inclusion and diversity where before each class each one of us had to present something in the class. Then we were having lunch in the university‚Äôs dining hall. Every day we had an abundant buffet of every food you can imagine; from burgers, chicken nuggets, and pizzas, to salmon, salads, fruits, waffles, and frozen yogurt.\nAfter lunch, we were had visits in companies and NGOs near the area. The company visit could have been anything from a small social enterprise, to a large multinational conglomerate. We were meeting chief executives and people from the team that were presenting us the ways that their company runs and its culture. Our schedule continued with activities which were either another visit excursion to a company or a cultural/bonding activity such as bowling, going sightseeing in the town, volunteering at an NGO, etc.\nIn the evening we had the option to eat dinner wherever we preferred. The program included allowance every week for us to spend which was more than enough to eat at a mid-range restaurant every day or cook at home and save more money for shopping and doing other activities.\n You can hereby see the schedule of SUSI 2018 ‚Äî one year before my cohort ‚Äî each year is different, and better in my opinion, but the core concept stays the same.\nBesides Chattanooga, we also visited other cities such as Nashville, and Atlanta. During the last week of our program, we spent 3 days in New York City, visiting companies and exploring the city. The last three days, we spent in Washington DC, where we presented our business idea to FHI360 headquarters (the NGO under the US State Department program was organized).\n\n\n\nMini SUSI reunion in Vienna, January 2020\n\n\nLiving 5 weeks in the US with the same people made everyone to come closer to each other. During SUSI I have made some really good friends in Europe whom I am glad to host anytime in Cyprus and I know that every time I will be in their country I will have someone to hang out with.\nPlease clap üëè on this on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html",
    "href": "posts/2023-03-15-cyprus/index.html",
    "title": "Experience Cyprus Like a Local!",
    "section": "",
    "text": "Most viewed video about Cyprus üëà\nAre you planning a trip to the island of goddess Aphrodite? From beautiful beaches to majestic mountains, Cyprus is a place where you can do a wide variety of activities. In this article, you will find details such as currency, transportation, restaurant recommendations, cultural visits, and cool activities you can do. Cyprus is sunny more than 300 days a year, however, remember to check the weather before arriving."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#currency",
    "href": "posts/2023-03-15-cyprus/index.html#currency",
    "title": "Experience Cyprus Like a Local!",
    "section": "CURRENCY",
    "text": "CURRENCY\nThe main currency is Euro. Live currency converter. They accept card payments everywhere. If you have a foreign currency, with Revolut you should be fine. Keep in mind that the north of the island (37%) is occupied by Turkey since 1974, therefore, Turkish Lira is what was adopted and is still in use in that part of the island."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#transportation",
    "href": "posts/2023-03-15-cyprus/index.html#transportation",
    "title": "Experience Cyprus Like a Local!",
    "section": "TRANSPORTATION",
    "text": "TRANSPORTATION\nThe most efficient way to transport in Cyprus as a tourist would be to rent a car. This offers the flexibility to explore the island at your own pace, and reach remote destinations that may not be accessible by public transportation. Remember, people left-hand side of the road, like in the UK. It is recommended to book your rental car in advance, especially during peak tourist season, to ensure availability and secure a better rate.\nTaxis and buses are also options; While you might not be as flexible as with a car, buses at least can be cheaper ‚Äî routes and schedules."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#beach",
    "href": "posts/2023-03-15-cyprus/index.html#beach",
    "title": "Experience Cyprus Like a Local!",
    "section": "BEACH",
    "text": "BEACH\nThere are different kinds of beaches in Cyprus. The most popular among Cypriots are in Protaras and Ayia Napa area. Here are some options for you:\nMarcelo (wavy with some rocks. More popular for students).\nFig tree bay (sand only, with a more shallow surface. More popular for families).\nYianna Marie Beach (This is something hybrid between the two prementioned beaches and one of the top preferences of locals. Here there is sand, not wavy with occasional rocks.)\nFor nice scenery, you can visit Cape Greco, where you can see sea caves. There are also people that are swimming in the crystal clear waters there, and some risk-loving people that dive from the Cape Greco cliff in the sea (not recommended).\n\n\n\nCape Greco"
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#nature",
    "href": "posts/2023-03-15-cyprus/index.html#nature",
    "title": "Experience Cyprus Like a Local!",
    "section": "NATURE",
    "text": "NATURE\nFor greenery and nature, you can go to Troodos National Forest Park around Mount Olympus. It is an area of outstanding natural beauty, suitable for activities such as hiking, winter skiing, biking, nature study, camping, and picnics. The highest point is Chionistra (1,952 m) and the lowest is Moni forest (700m).\n\n\n\nXyliatos Dam\n\n\nBeautiful Villages to visit:\nKalopanagiotis (It is known as the ornament of Marathasa Valley, a village that kept its old character while it evolved. With an old Unesco heritage church from the Venetian era, walking trails in the forest, and restaurants around, it attracts thousands of visitors every year. A lot of locals go there to relax, especially during summer.) Omodos (Beautiful village with traditional scenery and a war museum) Agros (A cultural village. It is reknown for its Rose Factory, its infamous traditional sweets store ‚Äúta Glyka tis Nikis‚Äù, and the traditional Cyprus sausage that is produced there.)\n\n\n\nKalopanagiotis"
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#activities",
    "href": "posts/2023-03-15-cyprus/index.html#activities",
    "title": "Experience Cyprus Like a Local!",
    "section": "ACTIVITIES",
    "text": "ACTIVITIES\nüö≤ Biking Nicosia‚Äôs Cycling route park, Pedieos linear park. It goes from Anthoupoli (Nicosia suburb) to downtown. Arguably, one of the most underrated experiences, it is a 14km(8.6 miles) ride with a lot of greenery, city nature, runners, and cats. You can also walk here on its pedestrian side.\nüëà\nüé≥ Bowling at Kykko Bowling. It is the only bowling place in Nicosia. With $6/game you can have fun while showing off your talent.\n‚öΩ Football. If you are five people, you can call in advance one of the following football futsal fields and they will find you opponents to play against; Paeeek, THOI (those are two good places I know; located in Nicosia).\nüî´ Paintball. While it is more expensive than the rest of the activities ($25/person) is a fun group activity ‚Äî min 8 people.\nüíª Remote working / Reading. Yfantourgeio is a great place to work remotely from. It is a quiet co-working space and in the heart of Nicosia. Another quiet place you can go is the library of the University of Cyprus. Moreover, you can go to one of the multiple coffee shops in Cyprus Nero, Costa Coffee, and Gloria Jeans (less quiet but more social)."
  },
  {
    "objectID": "posts/2023-03-15-cyprus/index.html#food-min-max",
    "href": "posts/2023-03-15-cyprus/index.html#food-min-max",
    "title": "Experience Cyprus Like a Local!",
    "section": "FOOD min $ /max $$$$",
    "text": "FOOD min $ /max $$$$\nü•ñ Zorbas $ (The most famous bakery in Cyprus where any local goes to. They are even operating in New York. For a quick breakfast, a sweet, or a traditional cooked meal for lunch or dinner, this is your go-to place. They have many locations in different cities in Cyprus.)\n\n\n\nZorbas bakeries\n\n\nüçñ ETHA Egkomis $$ (Traditional Cypriot food)\nüçñ Zannetos $$ (Traditional Cypriot food. Best place for meze.)\nüçñ Piatsa Gourounaki $ (Traditonal Cypriot food; fast food that it is convenientily located downtown.)\nüçî The Garrison Bar $$ (Burger bar with a Peaky Blinders theme and menu.)\nüçî Babylon Bar $$ (A local bar with a billiard and nice atmosphere.)\nüçî Moondogs Bar $$ (A local sports bar with billiard and a nice atmosphere.)\nüç± China Spice $$$ (Chinese restaurant. Great place for dinner, good value for money for its fanciness, with a variety of dishes. My favorite Chinese in Cyprus, especially on Tuesdays, there is an all-you-can-eat buffet for $25/person.)\nüçï Alfa Pizza $ (Cypriot fast food pizza chain)\nü•ô Avo $ (If you are a backpacker this is your place. It is the cheapest place you can get food in Nicosia. While you can get all sorts of food from souvlaki to pizza, Armenian food and lahmacun is their specialty.)\nü•û Edem‚Äôs Yard $$ (Breakfast/brunch in Larnaca‚Äôs palm trees area)\nüç¶ Papafilippou Ice-Cream $ (The most famous local ice-cream. Enjoy your ice cream while playing arcade games. If you want to try this but cannot go to their main store, papafilippou ice-cream is being sold in any kiosk.)\nüç¶ Heraclis Ice-Cream $ (The oldest ice-cream shop in Cyprus. It opened in 1939, from a local man named Heraclis during the British occupation before Cyprus become indipendent.)\nHopefully, this article helps you make a memorable and satisfactory Cyprus experience. Please clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html",
    "href": "posts/2023-03-25-nepal /index.html",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "",
    "text": "Kathmandu\nIf you‚Äôre thinking of visiting Nepal, there are several things you need to know to make the most out of your trip. Nepal is a destination known for its natural beauty, cultural richness, and friendly people. Whether you‚Äôre seeking adventure or a peaceful escape, Nepal has something for everyone.\nThis article provides a valuable firsthand account of a traveler‚Äôs experiences in Nepal. Here you can find practical advice and tips on currency exchange, transportation, and sightseeing in Kathmandu and Pokhara, as well as recommendations on where to eat and what to do. The article also highlights the friendliness and safety of the Nepalese people, which is important information for anyone considering a trip to Nepal.\nWhen planning your trip, there are two options to consider:\nVisiting Nepal with a travel agency can offer many advantages such as access to local knowledge and expertise, organized activities and transportation, and peace of mind. Especially if you are going for high-altitude hikes, a guide and a porter to carry your bags can be very helpful. On the other hand, planning a trip alone gives you more flexibility, freedom, and the opportunity to design a more personalized itinerary. I would recommend that option if your main plan is to explore Nepal‚Äôs culture and interact with locals. You save a lot and you have no one to deal with. Also, it is completely safe to make the trip to Nepal alone!\nIf you are going with a tourist agency, Himalayan Wonders is extremely organized that takes care of the whole trip for you. They answer fast when you email them. If you are price sensitive, they have a sister company, Adventurehero, which does the same trips but for cheaper prices ‚Äî less fancy hotels and jeep rides instead of internal flights. When organizing your trip with an agency, it can also be stressful if you leave everything for the last moment so consider finalizing the booking some days in advance.\nWhen I visited Nepal with a friend in February 2023, we went without a guide. While doing the proper research can be time-consuming, hopefully, this article saves you some time."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#prepare-before-visiting",
    "href": "posts/2023-03-25-nepal /index.html#prepare-before-visiting",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "PREPARE BEFORE VISITING",
    "text": "PREPARE BEFORE VISITING\nüíâHealth: Consult your doctor before going. Typically, there are suggested vaccines such as rabies, typhoid, hepatitis, and tetanus, but recommendations vary depending on each traveler. Malaria is uncommon there but also depends on the cities that you are going to. Usually, malaria cases are reported in the lower west side of Nepal e.g. Ilam. It is recommended to have international health insurance that covers you before going. I got mine from Revolut by upgrading from free to premium paying $8.99/month. If you don‚Äôt have revolut, feel free to create an account through my referral.\nIf you are going for a big hike i.e.¬†Everest base camp, it is recommended to get an insurance that covers evacuation with a helicopter, because if anything happens to you while in the mountain and you are uninsured, the bill you pay will be over $2500! I did not need it, but in case you do, here is the email of the guy that I asked for information from (this advice applies to you if you are from Cyprus); giorgostheodorou4@gmail.com ; He works at Metlife, and when I shared him a trip schedule that was including everest, he drafted me a $90 insurance.\nüç£ Food: If you are considering going hiking, bring with you some protein bars that will keep you during the day. It is not certain that you will find proper restaurants in the mountain while hiking. The following comes from my doctor‚Äôs advice‚Äî In general, eat only well-cooked food to minimize the risk of food poisoning. When it comes to water, bottled drinks are your go-to. Avoid using ice cubes or drinking from glasses. When brushing your teeth use bottled water and when shower with your mouth closed.\nüõÇ Visa: You can do it when you arrive there at the airport (highly recommended) or apply for it in advance. You will need two recent passport-size photos and a fee. There are three options: Visa for 15 days ($30), for 30 days ($50), and for 90 days ($125).\nü•æ Hiking: In Nepal, you need a permit to go hiking called TIMS, which you can do there. You will need two recent passport-size photos for this. In general, nature in Nepal can be dangerous and you can be lost if you try to do big hikes without taking someone experienced with you.\n\n\n\nBackpack essentials"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#while-you-are-there",
    "href": "posts/2023-03-25-nepal /index.html#while-you-are-there",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "WHILE YOU ARE THERE",
    "text": "WHILE YOU ARE THERE\nCurrency: Nepalese rupees. Do the conversion here before you buy something.\nYou can exchange your money at the airport (don‚Äôt do it before you pass the passport check), afterwards you find better rates. You also need a Sim card. You can get that also at the airport, after you arrive, the provider Ncell will have a brunch near the exit/entrance of the airport and you can get a sim card from there. I got mine with 25GB internet data and it was around 600 Rupees."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#tips-from-8-day-trip",
    "href": "posts/2023-03-25-nepal /index.html#tips-from-8-day-trip",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "tips from 8-day trip",
    "text": "tips from 8-day trip"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-1",
    "href": "posts/2023-03-25-nepal /index.html#day-1",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 1",
    "text": "DAY 1\nArrived in Kathmandu at 6 pm in the evening, and met with my friend in the airport. It‚Äôs important to note that when it comes to transportation, you should avoid anyone that asks to give you a ride beside the taxis with green labels, which are the official ones.\nWe took a taxi (1000 rupees) from the airport to our hotel (Flock Hostel Kathmandu) and we met there our potential trip guide. We were planning to go at Pokhara and stay there for a couple of nights, do some activities and some hikes around the area. The guide was not clear about his pricings during our negotiation and he asked us to pay for his expenses as well on top of his fee, so we decided to do the trip by ourselves.\nAt night we went out to get some dinner. We ended up at Roadhouse cafe and we had some oven pizza. It was a central place with nice atmoshpere inside, mostly tourists were in it though. After we returned home. The city, despite the functionally chaotic driving (i.e.¬†there are no traffic lights) and the fact that some people will try to sell you drugs, is safe; no robberies or violence. Honestly, when it comes to security and people, I felt safer than I feel in Philadelphia.\nRoadhouse cafe seems to be a local chain as we found couple of them in Pokhara and Kathmandu."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-2",
    "href": "posts/2023-03-25-nepal /index.html#day-2",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 2",
    "text": "DAY 2\nWoke up early and went to our respective embassies; the French embassy, and the consulate of Cyprus. French embassy asked from my friend to sign a document with his address in it; they did not allow us to enter. While the consulate of Cyprus was more accessible, there are no Cypriots working there. It is operated by locals, and they mostly issue tourist and student visas for Nepalese people that visit Cyprus.\nNext destination, Gaia Restro & Coffee Shop for breakfast. It is a relaxed place with open space to sit outside and nice food.\nAfterwards, we went to buy bus tickets for Pokhara, the place we were about to go the next day. We bought the tickets from East and West International Tours and Travels Ltd (front). Normal Bus was 1400Rupees, and the Luxury/more comfortable one was 1800Rupees ‚Äî normal prices, not overcharge. Luxury bus is recommended since the road to Kathmandu is bumpy. Normally it takes around 7‚Äì8 hours the trip from Kathmandu to Pokhara. As long as you have your ticket, make sure to be at the tourist bus stop early in the morning half an hour before your bus.\nThen we started walking towards the Monkey Temple. From there that we bought the tickets it was a 35-minute walk, however it was nice as we were able to stop along the way to shops and buy clothes and souvenirs.\nThe monkey temple is awesome because there is more trees, many monkeys, and a beautiful view of the city. The ticket to get in was 200 Rupees per person. Try not to feed the monkeys and keep your stuff on you cause they are sneaky animals! My advice; enjoy the view, take some pictures, and go continue exploring the city.\nNext, on the way to Durbar Square, we found a local football court, the one that is visible from the monkey temple, where many kids were playing football. It looked like they frequently play there. Playing football with the kids was for me the best experience of the day. Football is a universal sport that makes everyone forget life problems and enjoy the game and in that moment that was the definition we lived in.\nDurbar Square was a positive surprise for us. There we found many restaurants, coffee shops, and there are no cars in that area so many people walk there. Nearby you can also find the temple of Kumari Ghar.\nKumari Ghar is a historical place of worship for Hindus and Bhuddists.\nNote that some people try to scam tourists and ask them to pay to go to the square. We walked through the square and headed towards Civil Mall, one of the country‚Äôs biggest malls. Besides many shops, cafes, and restaurants (including the KKFC local fried chicken chain), on the 6th floor, you can find all sorts of games such as billiards, ping pong, bowling, and basketball throws. Great place to hang with friends. Before going home, we stopped at pizza hut for dinner."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-3",
    "href": "posts/2023-03-25-nepal /index.html#day-3",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 3",
    "text": "DAY 3\nSince the bus was leaving at 7 am, you have to be at the Tourist bus station half an hour earlier. The bus ride is bumpy but doable. The driver makes 3 stops until Pokhara. At the stops, there are restaurants and restrooms. We ate at one of the road restaurants just rice and noodles, since we are used to different diet we did not want to risk eating meat and uncooked vegetables.\nWe arrived at Pokhara at 5 pm, and then took a taxi (we also got the number of the driver for future rides) from the tourist bus park to our hotel, Hotel Middle Path and Spa. It is a very good hotel with a lot of facilities such as a gym, heated pool, spa, and sauna. We visited in February and a twin room was $25/night. Since we liked the hotel, we extended our accommodation there for two more nights. Then for dinner we went at Rice Garden Restaurant, a central restaurant in the main street; while it was not amazing, it was good enough. We had typical noodles and rice.\nMiddle Path & Spa Hotel: The best value for money hotel I have ever been!"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-4",
    "href": "posts/2023-03-25-nepal /index.html#day-4",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 4",
    "text": "DAY 4\nOur day started early. At 5.40 am, yesterday‚Äôs taxi driver picked us up from the hotel, and drove us to Sarangkot to see the sunrise. The view from there was amazing. You could see all the big hikes of the area such as Dhaulagiri (8167m), Machhapuchhare (6993m) and, Annapurna II (8,091m), and a panoramic view of Pokhara. Then the driver took us back to the hotel where we slept for a while. After, we went for breakfast at White Rabbit cafe nearby Pokhara‚Äôs lake. It was the best breakfast we had in Nepal so far! We also had a great lake view and we saw a football tournament going on next to the lake. So after our breakfast, we wanted to get closer to watch and play with them. The tournament seemed very well organized and was taken seriously by the participants. We found out that the prize for the winning team was $2000. We watched a very entertaining game with last-minute goals and a penalty shootout. Then, we borrowed a ball from the court and we played with the kids in the field. It was a beautiful experience.\nSubsequently, we left to buy a return bus ticket from Pokhara to Kathmandu. We asked a local tourist agency which sold us the bus ticket for 1600 rupees. My friend was asking to rent a motorcycle for the next week. He negotiated with the guy there and he promised him a fee if he was going to help him find a good motorcycle at a good price. Finally, he got an offer to rent it for $35/day. We left satisfied from there.\nIf you‚Äôre planning on renting a motorcycle in Nepal, there are a few important tips to keep in mind according to my friend. First, make sure to choose a trustworthy rental company and inspect the motorcycle carefully before renting it. You can do that by negotiating with multiple rental places. When it comes to places to go, Lower Mustang is a great option with recommended stops in Kalopani, Jomson, Kagbeni, and Muktinath, each for a day. Upper Mustang is another great option with a recommended 4-day trip from Kagbeni to the Tibetan border, including a night in Ghami and the other nights in the forbidden kingdom of Lo. Remember that you will need to obtain a permit to enter these regions, which costs 500‚Ç¨. While it‚Äôs possible to complete this itinerary in 7 days, it‚Äôs recommended to allow for a total of 10 days to enjoy the trip at a more relaxing pace.\nIn the afternoon, we took a taxi for Tibetan Refugee Camp.\nThe Tibetan Refugee Camp in Pokhara, Nepal, is home to thousands of Tibetan refugees who fled from their homeland in Tibet due to political and religious persecution by the Chinese government. After the Chinese takeover of Tibet in 1959, the 14th Dalai Lama and many Tibetans fled to India and Nepal, where they established a number of refugee settlements, including the camp in Pokhara. The camp in Pokhara was established in the 1960s and has since then provided a home and a sense of community for Tibetan refugees. The camp is run by the Tibetan Refugee Welfare Office and provides basic facilities, such as housing, schools, medical clinics, and workshops, to its residents.\nDespite the challenges Tibetan refugees face, such as limited resources and difficult living conditions, the Tibetan refugees in Pokhara have maintained their rich cultural heritage and traditions. They have established monasteries, schools, and community centers, where they can practice their religion, preserve their language, and pass their traditions down to future generations."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-5",
    "href": "posts/2023-03-25-nepal /index.html#day-5",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 5",
    "text": "DAY 5\nEarly morning we packed our stuff and went for breakfast at Crown restro and lounge cafe. It did not meet our expectations. They brought us different order, then they delayed bringing us our normal one, and at the end they mistakenly charged us 600Rupees more; all of this took us an hour.\nThen we called our taxi driver to drive us from our hotel to Phedi (drive costed 1500rupees) to start our hike. We passed through Dhampus and end up in Australian Base Camp. To reach Dhampus took us 1 hour. Dhampus to Australian camp distance is 4.7 kilometers / 2.9 miles. It takes 2 to 3 hours to reach the Australian camp passing through Dhampus Jungle. On Dhampus we found two travelers, Todd and Ruth. We briefly talked, and we exchanged Instagram accounts.\nWe reached at Australian Base Camp at 4 pm. There there are two options you can stay at 1) Angels Guest house hotel, 2) Hotel Gurans. We stayed at Hotel Gurans. Even if you book one in advance, the people there are flexible with canceling last minute. So when you go there, make sure to check and negotiate prices between the two. Both are very similar; the main differences are room availability, room view, and price."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-6",
    "href": "posts/2023-03-25-nepal /index.html#day-6",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 6",
    "text": "DAY 6\nWoke up to see the sunrise at 6.30 and we had an amazing view of the Himalayas. The view was so nice, that kept us there for half of our day; we ate breakfast (omelet and pancakes), listen to music, played football, and enjoyed the view. It was the most beautiful view I have ever seen! At 2 pm we left for Kande ‚Äî 1 hour to get down there. As we reached the street, we tried to call our driver. It would have taken him an hour to come so we hitchhiked and a family in a jeep drove us to Pokhara for 1000rupees.\nWe got some rest, and for the evening we arranged dinner with the travelers we met yesterday, Todd and Ruth. We had dinner at Soul Origin cafe and restaurant, and then dessert at French creperie; both amazing places. Todd and Ruth are an amazing duo! They are from California, they traveled in 58 countries, they love to immerse themselves in new cultures and both have great stories to tell throughout their global exploration journey; Todd is a great photographer (check his IG page), and Ruth is an amazing planner, she gave us some of the most useful traveling tips (check her article here).\nHighlights from our discussion:\n\nThe most adverse trip moments are usually accompanied by strong lessons and memorable experiences.\nThe little moments we share with people are undervalued. Traveling helps us understand better how the world works.\nOur lives in Europe and USA are much easier than we frequently think they are.\nBears in Alaska don‚Äôt bother people if their bellies are full of salmon.\nWhen traveling, keep your valuables safe with a tracker."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-7",
    "href": "posts/2023-03-25-nepal /index.html#day-7",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 7",
    "text": "DAY 7\nLast day in Pokhara started with a late wake-up. We had breakfast with Todd and Ruth and then went shopping at Pokhara‚Äôs main street. There are many quality products such as cashmere clothing, and brands such as NorthFace, Colombia, Patagonia that you can find much cheaper than Europe and America. On top of that negotiating is expected so you can get an even better price. After negotiating I got the following deals: a NorthFace jacket from 8500 initial price to 7000 rupees, a NorthFace sweatpants from 2500 to 1800 rupees, gloves for snow from 1000 to 900 rupees, and a cashmere sweater from 8500 to 5000 rupees.\nThen we had lunch at the restaurant Fresh Elements. Very good restaruant with a lot of options and good prices; only downside is that their portions are relatively small. Our hotel Middle Path & Spa, is collaborating with that restaurant so everything we were ordering from our hotel was coming from there.\nMeanwhile, my friend rented a motorcycle for $35/day from Pokhara with a plan to go to a 5-day ride starting from Pokhara to Jomsom, and ending to Upper Mustang. To ride at Upper Mustang, you need to pay for a permit $500, and have two more people with you for security. However, the pictures suggest that it is a worthwhile experience! In the evening I took the overnight bus at 7.30 pm for Kathmandu."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-8",
    "href": "posts/2023-03-25-nepal /index.html#day-8",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 8",
    "text": "DAY 8\nLast day in Nepal started early in Kathmandu. I arrived at 5 am. That time everything was closed. So I took a taxi for the reception of Kathmandu Guest House hotel. There is security, resting chairs to sleep in, and good breakfast. I left my luggage there, and I walked towards Durbar square. They asked me for a ticket once more (apparently if you don‚Äôt look local they ask you to pay 150 rupees). Don‚Äôt pay for this.\nHimalayan Java is the main coffee chain in Nepal. There is one in the middle of Durbar square so I went for pancakes. It seems fancier than the average coffee shop. It is the ‚ÄòStarbucks‚Äô of Nepal.\nIn the evening, I went back to the Guest House hotel to pick up my luggage and called the morning taxi driver to take me to the airport (we agreed in the morning 800 rupees for the airport ride).\nOverall, Nepal is among the best trips I have ever made and I‚Äôd love to revisit to see more nature and different cities. I hope you find this article useful for your trip planning to Nepal or satisfactory for your curiosity about the country and its culture.\nFollow my friend‚Äôs Instagram page to see what the world looks like when you are traveling in faraway places:)\nPlease clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-1-arrival-in-kathmandu",
    "href": "posts/2023-03-25-nepal /index.html#day-1-arrival-in-kathmandu",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 1: Arrival in Kathmandu",
    "text": "DAY 1: Arrival in Kathmandu\nArrived in Kathmandu at 6 pm in the evening, and met with my friend in the airport. It‚Äôs important to note that when it comes to transportation, you should avoid anyone that asks to give you a ride beside the taxis with green labels, which are the official ones.\nWe took a taxi (1000 rupees) from the airport to our hotel (Flock Hostel Kathmandu) and we met there our potential trip guide. We were planning to go at Pokhara and stay there for a couple of nights, do some activities and some hikes around the area. The guide was not clear about his pricings during our negotiation and he asked us to pay for his expenses as well on top of his fee, so we decided to do the trip by ourselves.\nAt night we went out to get some dinner. We ended up at Roadhouse cafe and we had some oven pizza. It was a central place with nice atmoshpere inside, mostly tourists were in it though. After we returned home. The city, despite the functionally chaotic driving (i.e.¬†there are no traffic lights) and the fact that some people will try to sell you drugs, is safe; no robberies or violence. Honestly, when it comes to security and people, I felt safer than I feel in Philadelphia.\nRoadhouse cafe seems to be a local chain as we found couple of them in Pokhara and Kathmandu."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-2-exploring-kathmandu",
    "href": "posts/2023-03-25-nepal /index.html#day-2-exploring-kathmandu",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 2: Exploring Kathmandu",
    "text": "DAY 2: Exploring Kathmandu\nWoke up early and went to our respective embassies; the French embassy, and the consulate of Cyprus. French embassy asked from my friend to sign a document with his address in it; they did not allow us to enter. While the consulate of Cyprus was more accessible, there are no Cypriots working there. It is operated by locals, and they mostly issue tourist and student visas for Nepalese people that visit Cyprus.\nNext destination, Gaia Restro & Coffee Shop for breakfast. It is a relaxed place with open space to sit outside and nice food.\nAfterwards, we went to buy bus tickets for Pokhara, the place we were about to go the next day. We bought the tickets from East and West International Tours and Travels Ltd (front). Normal Bus was 1400Rupees, and the Luxury/more comfortable one was 1800Rupees ‚Äî normal prices, not overcharge. Luxury bus is recommended since the road to Kathmandu is bumpy. Normally it takes around 7‚Äì8 hours the trip from Kathmandu to Pokhara. As long as you have your ticket, make sure to be at the tourist bus stop early in the morning half an hour before your bus.\nThen we started walking towards the Monkey Temple. From there that we bought the tickets it was a 35-minute walk, however it was nice as we were able to stop along the way to shops and buy clothes and souvenirs.\nThe monkey temple is awesome because there is more trees, many monkeys, and a beautiful view of the city. The ticket to get in was 200 Rupees per person. Try not to feed the monkeys and keep your stuff on you cause they are sneaky animals! My advice; enjoy the view, take some pictures, and go continue exploring the city.\nNext, on the way to Durbar Square, we found a local football court, the one that is visible from the monkey temple, where many kids were playing football. It looked like they frequently play there. Playing football with the kids was for me the best experience of the day. Football is a universal sport that makes everyone forget life problems and enjoy the game and in that moment that was the definition we lived in.\nDurbar Square was a positive surprise for us. There we found many restaurants, coffee shops, and there are no cars in that area so many people walk there. Nearby you can also find the temple of Kumari Ghar.\nKumari Ghar is a historical place of worship for Hindus and Bhuddists.\nNote that some people try to scam tourists and ask them to pay to go to the square. We walked through the square and headed towards Civil Mall, one of the country‚Äôs biggest malls. Besides many shops, cafes, and restaurants (including the KKFC local fried chicken chain), on the 6th floor, you can find all sorts of games such as billiards, ping pong, bowling, and basketball throws. Great place to hang with friends. Before going home, we stopped at pizza hut for dinner."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-3-traveling-to-pokhara",
    "href": "posts/2023-03-25-nepal /index.html#day-3-traveling-to-pokhara",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 3: Traveling to Pokhara",
    "text": "DAY 3: Traveling to Pokhara\nSince the bus was leaving at 7 am, you have to be at the Tourist bus station half an hour earlier. The bus ride is bumpy but doable. The driver makes 3 stops until Pokhara. At the stops, there are restaurants and restrooms. We ate at one of the road restaurants just rice and noodles, since we are used to different diet we did not want to risk eating meat and uncooked vegetables.\nWe arrived at Pokhara at 5 pm, and then took a taxi (we also got the number of the driver for future rides) from the tourist bus park to our hotel, Hotel Middle Path and Spa. It is a very good hotel with a lot of facilities such as a gym, heated pool, spa, and sauna. We visited in February and a twin room was $25/night. Since we liked the hotel, we extended our accommodation there for two more nights. Then for dinner we went at Rice Garden Restaurant, a central restaurant in the main street; while it was not amazing, it was good enough. We had typical noodles and rice.\nMiddle Path & Spa Hotel: The best value for money hotel I have ever been!"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-4-sunrise-and-tibetan-refugee-camp",
    "href": "posts/2023-03-25-nepal /index.html#day-4-sunrise-and-tibetan-refugee-camp",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 4: Sunrise and Tibetan Refugee Camp",
    "text": "DAY 4: Sunrise and Tibetan Refugee Camp\nOur day started early. At 5.40 am, yesterday‚Äôs taxi driver picked us up from the hotel, and drove us to Sarangkot to see the sunrise. The view from there was amazing. You could see all the big hikes of the area such as Dhaulagiri (8167m), Machhapuchhare (6993m) and, Annapurna II (8,091m), and a panoramic view of Pokhara. Then the driver took us back to the hotel where we slept for a while. After, we went for breakfast at White Rabbit cafe nearby Pokhara‚Äôs lake. It was the best breakfast we had in Nepal so far! We also had a great lake view and we saw a football tournament going on next to the lake. So after our breakfast, we wanted to get closer to watch and play with them. The tournament seemed very well organized and was taken seriously by the participants. We found out that the prize for the winning team was $2000. We watched a very entertaining game with last-minute goals and a penalty shootout. Then, we borrowed a ball from the court and we played with the kids in the field. It was a beautiful experience.\nSubsequently, we left to buy a return bus ticket from Pokhara to Kathmandu. We asked a local tourist agency which sold us the bus ticket for 1600 rupees. My friend was asking to rent a motorcycle for the next week. He negotiated with the guy there and he promised him a fee if he was going to help him find a good motorcycle at a good price. Finally, he got an offer to rent it for $35/day. We left satisfied from there.\nIf you‚Äôre planning on renting a motorcycle in Nepal, there are a few important tips to keep in mind according to my friend. First, make sure to choose a trustworthy rental company and inspect the motorcycle carefully before renting it. You can do that by negotiating with multiple rental places. When it comes to places to go, Lower Mustang is a great option with recommended stops in Kalopani, Jomson, Kagbeni, and Muktinath, each for a day. Upper Mustang is another great option with a recommended 4-day trip from Kagbeni to the Tibetan border, including a night in Ghami and the other nights in the forbidden kingdom of Lo. Remember that you will need to obtain a permit to enter these regions, which costs 500‚Ç¨. While it‚Äôs possible to complete this itinerary in 7 days, it‚Äôs recommended to allow for a total of 10 days to enjoy the trip at a more relaxing pace.\nIn the afternoon, we took a taxi for Tibetan Refugee Camp.\nThe Tibetan Refugee Camp in Pokhara, Nepal, is home to thousands of Tibetan refugees who fled from their homeland in Tibet due to political and religious persecution by the Chinese government. After the Chinese takeover of Tibet in 1959, the 14th Dalai Lama and many Tibetans fled to India and Nepal, where they established a number of refugee settlements, including the camp in Pokhara. The camp in Pokhara was established in the 1960s and has since then provided a home and a sense of community for Tibetan refugees. The camp is run by the Tibetan Refugee Welfare Office and provides basic facilities, such as housing, schools, medical clinics, and workshops, to its residents.\nDespite the challenges Tibetan refugees face, such as limited resources and difficult living conditions, the Tibetan refugees in Pokhara have maintained their rich cultural heritage and traditions. They have established monasteries, schools, and community centers, where they can practice their religion, preserve their language, and pass their traditions down to future generations."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-5-hiking-to-australian-base-camp",
    "href": "posts/2023-03-25-nepal /index.html#day-5-hiking-to-australian-base-camp",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 5: Hiking to Australian Base Camp",
    "text": "DAY 5: Hiking to Australian Base Camp\nEarly morning we packed our stuff and went for breakfast at Crown restro and lounge cafe. It did not meet our expectations. They brought us different order, then they delayed bringing us our normal one, and at the end they mistakenly charged us 600Rupees more; all of this took us an hour.\nThen we called our taxi driver to drive us from our hotel to Phedi (drive costed 1500rupees) to start our hike. We passed through Dhampus and end up in Australian Base Camp. To reach Dhampus took us 1 hour. Dhampus to Australian camp distance is 4.7 kilometers / 2.9 miles. It takes 2 to 3 hours to reach the Australian camp passing through Dhampus Jungle. On Dhampus we found two travelers, Todd and Ruth. We briefly talked, and we exchanged Instagram accounts.\nWe reached at Australian Base Camp at 4 pm. There there are two options you can stay at 1) Angels Guest house hotel, 2) Hotel Gurans. We stayed at Hotel Gurans. Even if you book one in advance, the people there are flexible with canceling last minute. So when you go there, make sure to check and negotiate prices between the two. Both are very similar; the main differences are room availability, room view, and price."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-6-enjoying-the-himalayas-from-our-hotel",
    "href": "posts/2023-03-25-nepal /index.html#day-6-enjoying-the-himalayas-from-our-hotel",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 6: Enjoying the Himalayas from our hotel",
    "text": "DAY 6: Enjoying the Himalayas from our hotel\nWoke up to see the sunrise at 6.30 and we had an amazing view of the Himalayas. The view was so nice, that kept us there for half of our day; we ate breakfast (omelet and pancakes), listen to music, played football, and enjoyed the view. It was the most beautiful view I have ever seen! At 2 pm we left for Kande ‚Äî 1 hour to get down there. As we reached the street, we tried to call our driver. It would have taken him an hour to come so we hitchhiked and a family in a jeep drove us to Pokhara for 1000rupees.\nWe got some rest, and for the evening we arranged dinner with the travelers we met yesterday, Todd and Ruth. We had dinner at Soul Origin cafe and restaurant, and then dessert at French creperie; both amazing places. Todd and Ruth are an amazing duo! They are from California, they traveled in 58 countries, they love to immerse themselves in new cultures and both have great stories to tell throughout their global exploration journey; Todd is a great photographer (check his IG page), and Ruth is an amazing planner, she gave us some of the most useful traveling tips (check her article here).\nHighlights from our discussion:\n\nThe most adverse trip moments are usually accompanied by strong lessons and memorable experiences.\nThe little moments we share with people are undervalued. Traveling helps us understand better how the world works.\nOur lives in Europe and USA are much easier than we frequently think they are.\nBears in Alaska don‚Äôt bother people if their bellies are full of salmon.\nWhen traveling, keep your valuables safe with a tracker."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-7-shopping-in-pokhara",
    "href": "posts/2023-03-25-nepal /index.html#day-7-shopping-in-pokhara",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 7: Shopping in Pokhara",
    "text": "DAY 7: Shopping in Pokhara\nLast day in Pokhara started with a late wake-up. We had breakfast with Todd and Ruth and then went shopping at Pokhara‚Äôs main street. There are many quality products such as cashmere clothing, and brands such as NorthFace, Colombia, Patagonia that you can find much cheaper than Europe and America. On top of that negotiating is expected so you can get an even better price. After negotiating I got the following deals: a NorthFace jacket from 8500 initial price to 7000 rupees, a NorthFace sweatpants from 2500 to 1800 rupees, gloves for snow from 1000 to 900 rupees, and a cashmere sweater from 8500 to 5000 rupees.\nThen we had lunch at the restaurant Fresh Elements. Very good restaruant with a lot of options and good prices; only downside is that their portions are relatively small. Our hotel Middle Path & Spa, is collaborating with that restaurant so everything we were ordering from our hotel was coming from there.\nMeanwhile, my friend rented a motorcycle for $35/day from Pokhara with a plan to go to a 5-day ride starting from Pokhara to Jomsom, and ending to Upper Mustang. To ride at Upper Mustang, you need to pay for a permit $500, and have two more people with you for security. However, the pictures suggest that it is a worthwhile experience! In the evening I took the overnight bus at 7.30 pm for Kathmandu."
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#day-8-departure-from-nepal",
    "href": "posts/2023-03-25-nepal /index.html#day-8-departure-from-nepal",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "DAY 8: Departure from Nepal",
    "text": "DAY 8: Departure from Nepal\nLast day in Nepal started early in Kathmandu. I arrived at 5 am. That time everything was closed. So I took a taxi for the reception of Kathmandu Guest House hotel. There is security, resting chairs to sleep in, and good breakfast. I left my luggage there, and I walked towards Durbar square. They asked me for a ticket once more (apparently if you don‚Äôt look local they ask you to pay 150 rupees). Don‚Äôt pay for this.\nHimalayan Java is the main coffee chain in Nepal. There is one in the middle of Durbar square so I went for pancakes. It seems fancier than the average coffee shop. It is the ‚ÄòStarbucks‚Äô of Nepal.\nIn the evening, I went back to the Guest House hotel to pick up my luggage and called the morning taxi driver to take me to the airport (we agreed in the morning 800 rupees for the airport ride).\nOverall, Nepal is among the best trips I have ever made and I‚Äôd love to revisit to see more nature and different cities. I hope you find this article useful for your trip planning to Nepal or satisfactory for your curiosity about the country and its culture.\nFollow my friend‚Äôs Instagram page to see what the world looks like when you are traveling in faraway places:)\nPlease clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#conclusion",
    "href": "posts/2023-03-25-nepal /index.html#conclusion",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nOverall, Nepal is among the best trips I have ever made and I‚Äôd love to revisit to see more nature and different cities. I hope you find this article useful for your trip planning to Nepal or satisfactory for your curiosity about the country and its culture.\nFollow my friend‚Äôs Instagram page to see what the world looks like when you are traveling in faraway places:)\nPlease clap üëè on medium if you find this post helpful:)"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#tasks-before-visiting",
    "href": "posts/2023-03-25-nepal /index.html#tasks-before-visiting",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "TASKS BEFORE VISITING",
    "text": "TASKS BEFORE VISITING\nüíâHealth: Consult your doctor before going. Typically, there are suggested vaccines such as rabies, typhoid, hepatitis, and tetanus, but recommendations vary depending on each traveler. Malaria is uncommon there but also depends on the cities that you are going to. Usually, malaria cases are reported in the lower west side of Nepal e.g. Ilam. It is recommended to have international health insurance that covers you before going. I got mine from Revolut by upgrading from free to premium paying $8.99/month. If you don‚Äôt have revolut, feel free to create an account through my referral.\nIf you are going for a big hike i.e.¬†Everest base camp, it is recommended to get an insurance that covers evacuation with a helicopter, because if anything happens to you while in the mountain and you are uninsured, the bill you pay will be over $2500! I did not need it, but in case you do, here is the email of the guy that I asked for information from (this advice applies to you if you are from Cyprus); giorgostheodorou4@gmail.com ; He works at Metlife, and when I shared him a trip schedule that was including everest, he drafted me a $90 insurance.\nüç£ Food: If you are considering going hiking, bring with you some protein bars that will keep you during the day. It is not certain that you will find proper restaurants in the mountain while hiking. The following comes from my doctor‚Äôs advice‚Äî In general, eat only well-cooked food to minimize the risk of food poisoning. When it comes to water, bottled drinks are your go-to. Avoid using ice cubes or drinking from glasses. When brushing your teeth use bottled water and when shower with your mouth closed.\nüõÇ Visa: You can do it when you arrive there at the airport (highly recommended) or apply for it in advance. You will need two recent passport-size photos and a fee. There are three options: Visa for 15 days ($30), for 30 days ($50), and for 90 days ($125).\nü•æ Hiking: In Nepal, you need a permit to go hiking called TIMS, which you can do there. You will need two recent passport-size photos for this. In general, nature in Nepal can be dangerous and you can be lost if you try to do big hikes without taking someone experienced with you.\n\n\n\nBackpack essentials"
  },
  {
    "objectID": "posts/2023-03-25-nepal /index.html#currency-simcard",
    "href": "posts/2023-03-25-nepal /index.html#currency-simcard",
    "title": "Nepal on a Budget: Backpacking Through Kathmandu, Pokhara, and the Australian Base Camp",
    "section": "CURRENCY & SIMCARD",
    "text": "CURRENCY & SIMCARD\nCurrency: Nepalese rupees. Do the conversion here before you buy something.\nYou can exchange your money at the airport (don‚Äôt do it before you pass the passport check), afterwards you find better rates. You also need a Sim card. You can get that also at the airport, after you arrive, the provider Ncell will have a brunch near the exit/entrance of the airport and you can get a sim card from there. I got mine with 25GB internet data and it was around 600 Rupees."
  }
]